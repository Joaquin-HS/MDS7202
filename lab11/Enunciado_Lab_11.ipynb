{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut√≥nomos ü§ñ**\n",
    "\n",
    "MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol√°s Ojeda, Melanie Pe√±a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Joaqu√≠n Herrera Su√°rez\n",
    "- Nombre de alumno 2: Hecmar Taucare Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Joaquin-HS/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "84ea39c9-2ffb-4a3a-a173-f524c6e77270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.4/958.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m757.8/958.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack-v1 modela el cl√°sico juego de cartas Blackjack, donde el objetivo es vencer al dealer logrando que la suma de las cartas propias se acerque m√°s a 21, sin excederlo. El juego tiene una formulaci√≥n como un Proceso de Decisi√≥n de Markov (MDP), definido por los siguientes componentes:\n",
    "\n",
    "1. Estados: \\\\\n",
    "Representados como una tupla (player_sum, dealer_card, usable_ace):\n",
    "  - player_sum (4 - 21): La suma actual de las cartas del jugador.\n",
    "  - dealer_card (1 - 10): El valor de la carta visible del dealer.\n",
    "  - usable_ace (0 o 1): Indica si el jugador tiene un as que puede contarse como 11 sin exceder 21.\n",
    "\n",
    "2. Acciones: \\\\\n",
    "El espacio de acciones es Discrete(2) con las siguientes opciones:\n",
    "  - 0 (Stick): Detenerse y no tomar m√°s cartas.\n",
    "  - 1 (Hit): Pedir una carta adicional.\n",
    "\n",
    "3. Recompensas: \\\\\n",
    "Las recompensas est√°n definidas seg√∫n el resultado del juego:\n",
    "  - +1: Victoria del jugador.\n",
    "  - -1: Derrota del jugador.\n",
    "  - 0: Empate.\n",
    "  - +1.5: Victoria con un blackjack natural (si natural=True).\n",
    "\n",
    "4. Din√°mica de transici√≥n: \\\\\n",
    "Si el jugador pide una carta (Hit), se actualiza su estado sumando el valor de la nueva carta. Si supera 21, el juego termina con una recompensa de -1.\n",
    "Si el jugador se detiene (Stick), el dealer juega hasta alcanzar una suma m√≠nima de 17. Luego, se compara la suma final del dealer con la del jugador para determinar el resultado.\n",
    "\n",
    "5. Estado inicial: \\\\\n",
    "El juego comienza con una suma inicial para el jugador entre 4 y 21, una carta visible del dealer entre 1 y 10, y un indicador de si el jugador tiene un as utilizable.\n",
    "\n",
    "6. Fin del juego: \\\\\n",
    "El juego termina cuando, o bien el jugador supera 21 (bust), o cuando el jugador decide detenerse (Stick) y se eval√∫a el resultado contra el dealer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "c80d3d00-965b-48f4-f998-1524a862bf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.38\n",
      "Desviaci√≥n est√°ndar de recompensas: 0.90\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# N√∫mero de simulaciones\n",
    "n_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "# Se simulan 5000 episodios\n",
    "for _ in range(n_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Se selecciona una acci√≥n aleatoria\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las m√©tricas\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensas: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvApsF05c2Ry"
   },
   "source": [
    "Como era de esperar, el rendimiento de esta pol√≠tica es muy bajo, ya que no toma decisiones informadas.\n",
    "\n",
    "Las recompensas obtenidas indican que la pol√≠tica aleatoria lleva a perder m√°s partidas de las que se ganan (promedio negativo), y que se tiene una alta variabilidad en los resultados (desviaci√≥n est√°ndar alta), lo que es t√≠pico en un juego de azar como Blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "c43e24b5-be23-4a18-cf04-6e3ce6eeea09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mSe truncaron las √∫ltimas l√≠neas 5000 del resultado de transmisi√≥n.\u001b[0m\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 1940     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5100     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7872     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 1942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5104     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7876     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1943     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5108     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7883     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 1945     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5112     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7890     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.286    |\n",
      "|    n_updates        | 1947     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5116     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7895     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 1948     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5120     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7901     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 1950     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5124     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7909     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 1952     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5128     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7914     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 1953     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5132     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7918     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 1954     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5136     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7924     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 1955     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5140     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7932     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 1957     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5144     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7940     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 1959     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5148     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7946     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 1961     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5152     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7956     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 1963     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5156     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7961     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1965     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5160     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7965     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 1966     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5164     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7972     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 1967     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5168     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 1969     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5172     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7984     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 1970     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5176     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7991     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 1972     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5180     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7997     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5184     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8004     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1975     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5188     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8012     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.213    |\n",
      "|    n_updates        | 1977     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5192     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8018     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1979     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5196     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8026     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 1981     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5200     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8033     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1983     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5204     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8041     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 1985     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5208     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8046     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.288    |\n",
      "|    n_updates        | 1986     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5212     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8052     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 1987     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5216     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8059     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 1989     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5220     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8066     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 1991     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5224     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8075     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 1993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5228     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8083     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.261    |\n",
      "|    n_updates        | 1995     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5232     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8088     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5236     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8095     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.214    |\n",
      "|    n_updates        | 1998     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5240     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8100     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5244     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8105     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.211    |\n",
      "|    n_updates        | 2001     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5248     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8111     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5252     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8116     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5256     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 2005     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.11     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5260     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8131     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2007     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5264     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.25     |\n",
      "|    n_updates        | 2008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | 0.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5268     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8145     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2011     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5272     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.297    |\n",
      "|    n_updates        | 2012     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5276     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8155     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2013     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5280     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2015     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5284     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8168     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.195    |\n",
      "|    n_updates        | 2016     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5288     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8174     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2018     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5292     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8179     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2019     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5296     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8183     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5300     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8191     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2022     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5304     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8195     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2023     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5308     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8201     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.249    |\n",
      "|    n_updates        | 2025     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5312     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8209     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2027     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5316     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8215     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.226    |\n",
      "|    n_updates        | 2028     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5320     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5324     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8229     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 2032     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5328     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8234     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 2033     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5332     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8239     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.154    |\n",
      "|    n_updates        | 2034     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5336     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8244     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 2035     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5340     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8248     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.297    |\n",
      "|    n_updates        | 2036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5344     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 2038     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5348     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8264     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2040     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5352     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8271     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 2042     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5356     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8275     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 2043     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5360     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 2045     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5364     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8289     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 2047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5368     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8296     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2048     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5372     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8303     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5376     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8311     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 2052     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5380     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8318     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5384     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8325     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2056     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5388     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8330     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2057     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5392     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8340     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2059     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5396     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8348     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 2061     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5400     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8356     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 2063     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5404     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8363     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 2065     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5408     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8372     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2067     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5412     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.104    |\n",
      "|    n_updates        | 2069     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5416     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8385     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.181    |\n",
      "|    n_updates        | 2071     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5420     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8391     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2072     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5424     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8398     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 2074     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5428     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8405     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 2076     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5432     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8411     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2077     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5436     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8418     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 2079     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.77     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5440     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8425     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 2081     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.75     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5444     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8431     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0823   |\n",
      "|    n_updates        | 2082     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5448     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2084     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5452     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8441     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 2085     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5456     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8449     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5460     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8456     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5464     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8462     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2090     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5468     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8469     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.206    |\n",
      "|    n_updates        | 2092     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5472     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8473     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.231    |\n",
      "|    n_updates        | 2093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5476     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8482     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 2095     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5480     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8486     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5484     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8492     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5488     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8496     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.267    |\n",
      "|    n_updates        | 2098     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5492     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8501     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.148    |\n",
      "|    n_updates        | 2100     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5496     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8507     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 2101     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5500     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8513     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2103     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5504     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8519     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.345    |\n",
      "|    n_updates        | 2104     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5508     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8525     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 2106     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5512     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8532     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.133    |\n",
      "|    n_updates        | 2107     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5516     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8539     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2109     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5520     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8548     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2111     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5524     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8555     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 2113     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5528     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8563     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2115     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5532     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8570     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.224    |\n",
      "|    n_updates        | 2117     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5536     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8578     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2119     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5540     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8585     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2121     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5544     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8592     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 2122     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5548     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8596     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 2123     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5552     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8603     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 2125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5556     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8611     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 2127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5560     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8618     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5564     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8623     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 2130     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5568     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8631     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.149    |\n",
      "|    n_updates        | 2132     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5572     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8639     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5576     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8643     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 2135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5580     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8648     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2136     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5584     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8654     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 2138     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5588     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8660     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 2139     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5592     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8665     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2141     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5596     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8672     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 2142     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5600     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8677     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 2144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5604     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8684     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2145     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5608     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8691     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2147     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5612     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8695     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2148     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5616     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8700     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 2149     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5620     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8704     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2150     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5624     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8709     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2152     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5628     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8715     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 2153     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5632     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8719     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2154     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5636     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8725     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2156     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5640     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8729     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2157     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.42     |\n",
      "|    ep_rew_mean      | -0.24    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5644     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8734     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2158     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.28    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5648     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8740     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 2159     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5652     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8750     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 2162     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5656     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8754     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2163     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.42     |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5660     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5664     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8767     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2166     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5668     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8774     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 2168     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5672     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8782     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2170     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5676     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8789     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2172     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.38    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5680     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8798     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 2174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5684     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8804     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2175     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5688     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8810     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.168    |\n",
      "|    n_updates        | 2177     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5692     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8815     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 2178     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5696     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8823     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 2180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5700     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8829     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2182     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5704     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8834     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2183     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5708     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8839     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.133    |\n",
      "|    n_updates        | 2184     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5712     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8846     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2186     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5716     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2187     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.28    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5720     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8858     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2189     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.25    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5724     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8864     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2190     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5728     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8869     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2192     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5732     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8875     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 2193     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5736     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8881     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 2195     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5740     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8887     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 2196     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5744     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8893     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 2198     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5748     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8902     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 2200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5752     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8908     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2201     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5756     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8915     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2203     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5760     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8922     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2205     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5764     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8929     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.294    |\n",
      "|    n_updates        | 2207     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5768     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8936     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 2208     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5772     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8945     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 2211     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5776     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8952     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.276    |\n",
      "|    n_updates        | 2212     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5780     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 2214     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5784     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8966     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 2216     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5788     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8970     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.25     |\n",
      "|    n_updates        | 2217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5792     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8975     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2218     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5796     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8986     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2221     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5800     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8990     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2222     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5804     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8997     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5808     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9002     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2225     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5812     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9007     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2226     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5816     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9011     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2227     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5820     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9016     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 2228     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5824     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9022     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.3      |\n",
      "|    n_updates        | 2230     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5828     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9028     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2231     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5832     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9033     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2233     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5836     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 2234     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5840     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9045     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 2236     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5844     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9050     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2237     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5848     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9058     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2239     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5852     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9067     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2241     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5856     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9072     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.141    |\n",
      "|    n_updates        | 2242     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5860     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9076     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2243     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5864     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9081     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.237    |\n",
      "|    n_updates        | 2245     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5868     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9088     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.239    |\n",
      "|    n_updates        | 2246     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5872     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9093     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 2248     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5876     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9101     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2250     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5880     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9106     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 2251     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5884     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9112     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.207    |\n",
      "|    n_updates        | 2252     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5888     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9119     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2254     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5892     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2255     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5896     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5900     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9139     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5904     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9145     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2261     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5908     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2262     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5912     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9154     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2263     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5916     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9164     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5920     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9169     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 2267     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5924     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9176     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 2268     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5928     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9183     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.238    |\n",
      "|    n_updates        | 2270     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5932     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9187     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2271     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5936     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9191     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2272     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5940     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9198     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2274     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5944     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9204     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2275     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5948     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9214     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 2278     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5952     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 2280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5956     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9229     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 2282     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5960     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9234     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 2283     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5964     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9239     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 2284     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5968     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9245     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2286     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5972     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 2288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5976     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9264     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5980     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9272     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 2292     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5984     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9276     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 2293     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5988     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9283     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2295     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5992     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9290     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 2297     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5996     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9295     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2298     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6000     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9302     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2300     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6004     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9308     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2301     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6008     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9313     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2303     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6012     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9317     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2304     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6016     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.251    |\n",
      "|    n_updates        | 2306     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6020     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9333     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6024     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9341     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 2310     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6028     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9348     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2311     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6032     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9353     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 2313     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6036     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9358     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.213    |\n",
      "|    n_updates        | 2314     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.24    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6040     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9366     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 2316     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6044     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9374     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2318     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6048     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2319     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6052     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9386     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.207    |\n",
      "|    n_updates        | 2321     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6056     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9390     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2322     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6060     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9398     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2324     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6064     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9402     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2325     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6068     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9407     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6072     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9412     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 2327     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6076     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9418     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6080     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9424     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 2330     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6084     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9432     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 2332     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6088     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2334     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6092     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9444     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 2335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6096     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9454     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 2338     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6100     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9461     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 2340     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6104     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9468     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 2341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6108     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9473     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 2343     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6112     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9478     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2344     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6116     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9487     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 2346     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6120     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9494     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 2348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6124     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9501     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2350     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6128     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9507     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2351     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6132     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9513     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2353     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6136     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 2354     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6140     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9527     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2356     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6144     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9531     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2357     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6148     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2358     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6152     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9549     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2362     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6156     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9555     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2363     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6160     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9563     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.231    |\n",
      "|    n_updates        | 2365     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6164     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9568     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0796   |\n",
      "|    n_updates        | 2366     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6168     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9573     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2368     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6172     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9578     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.253    |\n",
      "|    n_updates        | 2369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6176     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9584     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2370     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6180     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9588     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 2371     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6184     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9594     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 2373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6188     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9599     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6192     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9604     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2375     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6196     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9612     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 2377     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6200     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9619     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.251    |\n",
      "|    n_updates        | 2379     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6204     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9624     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2380     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6208     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9631     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 2382     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6212     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9641     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2385     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6216     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9648     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 2386     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6220     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9654     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 2388     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6224     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9660     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0623   |\n",
      "|    n_updates        | 2389     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6228     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9666     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 2391     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6232     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9678     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 2394     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6236     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9685     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2396     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6240     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9689     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2397     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6244     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9698     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 2399     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6248     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9705     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2401     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6252     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9712     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2402     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6256     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9717     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6260     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9723     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 2405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6264     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.244    |\n",
      "|    n_updates        | 2406     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6268     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9734     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2408     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6272     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9738     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.175    |\n",
      "|    n_updates        | 2409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6276     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9746     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 2411     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6280     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9753     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2413     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6284     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9761     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2415     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6288     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9769     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 2417     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6292     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9775     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 2418     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6296     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9782     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 2420     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6300     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 2422     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6304     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9798     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2424     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6308     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9804     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2425     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6312     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9809     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 2427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6316     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9819     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2429     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6320     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9825     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2431     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6324     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9832     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2432     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6328     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9837     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2434     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6332     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9845     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 2436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6336     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6340     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9861     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2440     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6344     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9868     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 2441     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6348     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9873     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.262    |\n",
      "|    n_updates        | 2443     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6352     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9881     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2445     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6356     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9889     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 2447     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6360     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9894     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2448     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6364     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9901     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2450     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6368     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9907     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2451     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6372     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9912     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 2452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6376     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2454     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6380     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9924     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2455     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6384     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9933     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 2458     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6388     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9937     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 2459     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6392     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9945     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 2461     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6396     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9950     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6400     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9957     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 2464     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6404     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9963     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2465     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6408     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9969     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 2467     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6412     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9973     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 2468     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6416     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2469     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6420     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9981     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2470     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6424     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9988     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2471     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6428     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9998     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x789c8260f220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Se utilizar√° el algoritmo DQN (Deep Q-Network) por su efectividad para resolver\n",
    "problemas con espacios de acci√≥n discretos como Blacjack (se utiliza una red neuronal\n",
    "para aproximar la funci√≥n Q (que estima el valor esperado de cada acci√≥n en un\n",
    "estado dado)). Adem√°s, es menos complejo que otros algoritmos, pero lo suficientemente\n",
    "avanzado como para mostrar una mejora significativa sobre el baseline.\n",
    "'''\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Se crea el modelo DQN\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=0.001, gamma=0.99, exploration_fraction=0.1, seed=42)\n",
    "\n",
    "# Se entrena el modelo\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "ab8cee3c-c066-415e-c171-d6883acbe4d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.08 ¬± 0.96\n"
     ]
    }
   ],
   "source": [
    "# Se eval√∫a el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f} ¬± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sr1DxJhiOIe"
   },
   "source": [
    "El modelo entrenado tiene un promedio de recompensas significativamente menos negativo (-0.08) que el baseline (-0.38). Esto indica que el modelo entrenado pierde menos partidas en promedio y se acerca a un rendimiento neutral. Por su parte, la desviaci√≥n est√°ndar del modelo entrenado (0.96) es ligeramente mayor que la del baseline (0.90). Esto sugiere que hay una mayor variabilidad en las recompensas obtenidas por el modelo entrenado. Esto podr√≠a deberse a la naturaleza estoc√°stica del ambiente o a que el modelo todav√≠a no ha alcanzado una pol√≠tica completamente √≥ptima.\n",
    "\n",
    "De esta forma, el modelo entrenado tiene un desempe√±o claramente mejor que el baseline, ya que logra reducir significativamente las p√©rdidas promedio.\n",
    "Aunque no alcanza un rendimiento positivo, est√° demostrando que puede aprender estrategias m√°s efectivas que las acciones completamente aleatorias. De todas formas, el modelo podr√≠a beneficiarse de aumentar su entrenamiento (mayor `total_timesteps`); optimizar los hiperpar√°metros: `learning_rate` (tasa de aprendizaje para el optimizador), `gamma` (factor de descuento para recompensas futuras), o `exploration_fraction` (fracci√≥n de tiempo donde el agente realiza exploraci√≥n en lugar de explotaci√≥n) para que el modelo converja mejor; o directamente comparar este resultado con otros algoritmos de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAcXqiw3knfb",
    "outputId": "cf0f51c6-38c8-4921-8733-1862a05b0e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial: [12  6  1]\n",
      "Tipo de estado: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Se inspecciona el formato de un estado\n",
    "initial_state, _ = env.reset()\n",
    "print(f\"Estado inicial: {initial_state}\")\n",
    "print(f\"Tipo de estado: {type(initial_state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpeC96SjlENy"
   },
   "source": [
    "Los estados pertenecen a la clase `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J57u0bw0k57T",
    "outputId": "226f6b38-d88f-4da4-ad89-8bed860ae0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escenario 1: Estado = (6, 7, 0), Acci√≥n = Pedir carta (Hit)\n",
      "Escenario 2: Estado = (19, 3, 1), Acci√≥n = Detenerse (Stick)\n"
     ]
    }
   ],
   "source": [
    "def get_agent_action(model, state):\n",
    "    \"\"\"\n",
    "    Obtiene la acci√≥n del agente para un estado dado.\n",
    "\n",
    "    Args:\n",
    "    - model: Modelo entrenado (DQN).\n",
    "    - state: Estado del ambiente (tuple o array).\n",
    "\n",
    "    Returns:\n",
    "    - Acci√≥n seleccionada por el modelo (int).\n",
    "    \"\"\"\n",
    "    # Se convierte el estado al formato esperado por el modelo (array)\n",
    "    if isinstance(state, tuple):\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "    # Se selecciona la acci√≥n usando el modelo\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    return action\n",
    "\n",
    "# Escenarios propuestos\n",
    "scenarios = [\n",
    "    (6, 7, 0),  # Suma de cartas = 6, Dealer muestra 7, sin as\n",
    "    (19, 3, 1)  # Suma de cartas = 19, Dealer muestra 3, con as\n",
    "]\n",
    "\n",
    "# Se eval√∫an las acciones del agente en cada escenario\n",
    "for i, state in enumerate(scenarios):\n",
    "    action = get_agent_action(model, state)\n",
    "    action_str = \"Pedir carta (Hit)\" if action == 1 else \"Detenerse (Stick)\"\n",
    "    print(f\"Escenario {i+1}: Estado = {state}, Acci√≥n = {action_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jOSWgeblOoJ"
   },
   "source": [
    "Dado el estado del escenario 1, lo esperado es que el agente pida una carta (Hit), ya que tiene pocas probabilidades de superar 21. Por otro lado, para el escenario 2, lo esperado es que el agente se detenga (Stick), ya que tiene una suma segura frente a la carta del dealer y pedir una carta ser√≠a muy arriesgado. Como se puede ver en \"print\", el agente toma acciones coherentes con las reglas del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS3EILBXZ2pa"
   },
   "source": [
    "En primer lugar, se destaca que especificar el par√°metro `continuous = True` implica que el espacio de acciones del ambiente cambia de ser discreto a continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "El ambiente LunarLander consiste en un problema cl√°sico de optimizaci√≥n de trayectoria donde el objetivo es aterrizar una nave espacial en una plataforma situada en el centro de un mapa 2D.\n",
    "\n",
    "1. Estados: \\\\\n",
    "  El espacio de observaciones es un vector continuo de 8 dimensiones (Box), que describe:\n",
    "\n",
    "    - Posici√≥n de la nave: coordenadas $x$ e $y$\n",
    "    - Velocidades lineales: $v_x$ y $v_y$\n",
    "    - √Ångulo de orientaci√≥n ($\\theta$)\n",
    "    - Velocidad angular ($\\omega$)\n",
    "    - Contacto de las patas de la nave con el suelo (2 valores booleanos).\n",
    "\n",
    "  Un ejemplo de estado se puede ver a continuaci√≥n: $[x, y, v_x, v_y, Œ∏, œâ, leg1_{contact}, leg2_{contact}]$\n",
    "\n",
    "2. Acciones: \\\\\n",
    "  En modo discreto:\n",
    "    - 0: Hacer nada.\n",
    "    - 1: Encender el motor izquierdo.\n",
    "    - 2: Encender el motor principal.\n",
    "    - 3: Encender el motor derecho.\n",
    "\n",
    "  En modo continuo (el que se tiene al haber definido `continuous = True`):\n",
    "  un vector de dos elementos:\n",
    "    - $a[0]$: Potencia del motor principal (-1 a 1)\n",
    "    - $a[1]$: Potencia de los propulsores laterales (-1 a 1).\n",
    "\n",
    "3. Recompensas: \\\\\n",
    "  Las recompensas est√°n dise√±adas para incentivar un aterrizaje seguro:\n",
    "\n",
    "  - Proximidad al objetivo y velocidad reducida: recompensa positiva.\n",
    "  - Contacto de las patas con el suelo: +10 puntos por pata.\n",
    "  - Movimiento en exceso, inclinaci√≥n o uso de combustible: penalizaci√≥n.\n",
    "  - Aterrizaje seguro: +100 puntos.\n",
    "  - Colisi√≥n: -100 puntos.\n",
    "  - El episodio se considera resuelto si el puntaje total es $\\geq 200$.\n",
    "\n",
    "4. Condiciones de finalizaci√≥n \\\\\n",
    "Un episodio termina si:\n",
    "\n",
    "  - La nave colisiona con la superficie lunar.\n",
    "  - La nave sale de los l√≠mites del mapa.\n",
    "  - La nave queda inm√≥vil.\n",
    "\n",
    "Se destaca que en LunarLander con `continuous=True`, las acciones son vectores continuos que permiten mayor control con respecto al caso de Blackjack, donde las acciones son simplemente binarias: \"hit\" (pedir carta) o \"stick\" (detenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "5a6ff05b-95c4-4eb0-f8e0-e4da06ddef60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -243.93\n",
      "Desviaci√≥n est√°ndar de las recompensas: 108.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lista para almacenar las recompensas de cada episodio\n",
    "episodic_rewards = []\n",
    "\n",
    "# N√∫mero de episodios\n",
    "num_episodes = 10\n",
    "\n",
    "# Se simula el ambiente con acciones aleatorias\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Se selecciona una acci√≥n aleatoria dentro del espacio de acci√≥n continuo\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(random_action)\n",
    "        total_reward += reward\n",
    "\n",
    "    episodic_rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las m√©tricas\n",
    "mean_reward = np.mean(episodic_rewards)\n",
    "std_reward = np.std(episodic_rewards)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lbxtl5he0NN"
   },
   "source": [
    "El performance de esta pol√≠tica es bajo (`recompensa promedio: -243.93`), ya que no sigue ninguna estrategia para optimizar la posici√≥n, velocidad o el uso de motores, lo que tiende a resultar en posibles colisiones o aterrizajes incorrectos. Por su parte, por el car√°cter aleatorio de las acciones, los resultados de cada episodio pueden tener grandes variaciones, lo que se aprecia en el alto valor de desviaci√≥n est√°ndar obtenido (`108.96`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "8b1dee46-260e-438d-af67-336a1778133e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -232        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595261 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 509         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.6e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -242        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006132064 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 564         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | -243        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004289039 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 876         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | -235         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 429          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899783 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.00131     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 521          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 954          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Se utilizar√° el algoritmo PPO (Proximal Policy Optimization) por su efectividad\n",
    "y estabilidad para resolver problemas con espacios de acci√≥n continuos como el\n",
    "LunarLander en su configuraci√≥n continuous=True, al modelar distribuciones\n",
    "gaussianas para las acciones.\n",
    "'''\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Se guarda el modelo entrenado\n",
    "model.save(\"ppo_lunarlander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "87b7653d-9b5f-49ff-a536-36a71c93c295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -142.87\n",
      "Desviaci√≥n est√°ndar de las recompensas: 96.47\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander\")\n",
    "\n",
    "# Lista para almacenar las recompensas de cada episodio\n",
    "episodic_rewards = []\n",
    "\n",
    "# N√∫mero de episodios para la evaluaci√≥n\n",
    "num_episodes = 10\n",
    "\n",
    "# Se eval√∫a el modelo\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Acci√≥n del modelo entrenado\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    episodic_rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las m√©tricas\n",
    "mean_reward = np.mean(episodic_rewards)\n",
    "std_reward = np.std(episodic_rewards)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er8RpRGYj5oy"
   },
   "source": [
    "El modelo entrenado tiene un promedio de recompensa significativamente menos negativo (-142.87) que el baseline (-243.93). Esto indica que el modelo entrenado ha aprendido a realizar acciones m√°s orientadas hacia el objetivo, como acercarse al √°rea de aterrizaje y mantener la nave en buenas condiciones. Por su parte, la desviaci√≥n est√°ndar del modelo entrenado (96.47) es ligeramente menor que la del baseline (108.96), lo que sugiere un comportamiento m√°s consistente en las pol√≠ticas aprendidas, aunque todav√≠a con cierta variabilidad presente.\n",
    "\n",
    "De esta forma, el modelo entrenado tiene un desempe√±o claramente mejor que el baseline, ya que aprendi√≥ a evitar comportamientos altamente penalizados, es decir, una estrategia m√°s efectiva que efectuar acciones completamente aleatorias. Ahora bien, es importante destacar que todav√≠a no alcanza un desempe√±o que resuelva completamente el problema. De igual forma que antes, el modelo podr√≠a beneficiarse de aumentar su entrenamiento (mayor `total_timesteps`); optimizar los hiperpar√°metros: `learning_rate` (tasa de aprendizaje para el optimizador) o `gamma` (factor de descuento para recompensas futuras) para que el modelo converja mejor; o directamente comparar este resultado con otros algoritmos de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsVOtLKYnkks"
   },
   "outputs": [],
   "source": [
    "# Stable-Baselines3 espera trabajar con un entorno envuelto en un VecEnv\n",
    "# (un vector de entornos para manejar m√∫ltiples episodios en paralelo)\n",
    "\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "def evaluate_model(env, model, n_episodes=10):\n",
    "    rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    print(f\"Recompensa promedio: {avg_reward:.2f}\")\n",
    "    print(f\"Desviaci√≥n est√°ndar de las recompensas: {std_reward:.2f}\")\n",
    "    return avg_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vsvq177oKS0"
   },
   "source": [
    "##### learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYj2HvXzpIf4"
   },
   "source": [
    "Se disminuye la tasa de aprendizaje para estabilizar el entrenamiento en un ambiente complejo como el de LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SfNSbj8oTYV",
    "outputId": "61de5afb-8e1a-4288-972a-6acc46ed852e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 880      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -223         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027952273 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 669          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.78e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -213         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027620955 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 537          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00413     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.46e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -218        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 551         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001897949 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0229     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 500         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -219        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004079805 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.00881    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 635         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.44e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc502f59b70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_learning_rate = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0001, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model_learning_rate.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb7AmmMdqfxp",
    "outputId": "d2a21a7e-fec9-4321-8ab8-48a97f6a9f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -153.18\n",
      "Desviaci√≥n est√°ndar de las recompensas: 89.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-153.1816, 89.43967)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo optimizado\n",
    "evaluate_model(vec_env, model_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wScMyWZioT7M"
   },
   "source": [
    "##### batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rtodC3XpQ34"
   },
   "source": [
    "Se aumenta el tama√±o del batch dado el ambiente altamente complejo y continuo que es LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzDzVnBcoWNP",
    "outputId": "cc742f2e-0309-4d96-d5c9-153b4386b8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 916      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -229         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 763          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038873914 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 885          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.81e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 123         |\n",
      "|    ep_rew_mean          | -230        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 677         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003605734 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00959    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 602         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.5e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 124          |\n",
      "|    ep_rew_mean          | -221         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 688          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014260923 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.011       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 694          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.49e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | -218         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 690          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036488809 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0357      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 818          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00468     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.59e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc502f495d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_batch_size = PPO(\"MlpPolicy\", env, verbose=1, batch_size=256, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model_batch_size.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcYqiLN8qgTl",
    "outputId": "95e4ac18-ce40-44de-ad86-98da2b9fafb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -149.01\n",
      "Desviaci√≥n est√°ndar de las recompensas: 91.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-149.0068, 91.97724)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo optimizado\n",
    "evaluate_model(vec_env, model_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYhhMl_GoCtg"
   },
   "source": [
    "##### total_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8MK9biio8BA"
   },
   "source": [
    "Se decide aumentar el n√∫mero de pasos de entrenamiento para que PPO tenga m√°s tiempo para aprender las pol√≠ticas complejas de LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aItYF6sr6F_6",
    "outputId": "b8036702-8bd9-4e52-c8bf-23283cf2eee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 749      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -232        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595261 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 509         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.6e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -242        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006132064 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 564         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | -243        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004289039 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 876         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | -235         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899783 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.00131     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 521          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 954          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 132         |\n",
      "|    ep_rew_mean          | -215        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007246358 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.0232     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 219         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 647         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 132         |\n",
      "|    ep_rew_mean          | -199        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008402871 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.000615   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 481         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008425518 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.000404   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 284         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 140          |\n",
      "|    ep_rew_mean          | -161         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 401          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073442548 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -9.2e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 128          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00477     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 243          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003997383 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.00937     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 263         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 145          |\n",
      "|    ep_rew_mean          | -107         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 383          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045888405 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.0114       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.6         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 233          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004353313 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 130         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | -81.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 388          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030843988 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 248          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 159          |\n",
      "|    ep_rew_mean          | -69.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 383          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065037026 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.342        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 171          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 173          |\n",
      "|    ep_rew_mean          | -64.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 364          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059796213 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.5         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00682     |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 138          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 188          |\n",
      "|    ep_rew_mean          | -66          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 358          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034857607 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.1         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 198          |\n",
      "|    ep_rew_mean          | -63.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 359          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043706214 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.6         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 139          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 213          |\n",
      "|    ep_rew_mean          | -63.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 353          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067793094 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.2         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00588     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | -61.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 340          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056233574 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.643        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.3         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0061      |\n",
      "|    std                  | 0.944        |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 248         |\n",
      "|    ep_rew_mean          | -58.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005022563 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 0.934       |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 259         |\n",
      "|    ep_rew_mean          | -57.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 319         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008182257 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    std                  | 0.932       |\n",
      "|    value_loss           | 41.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 280         |\n",
      "|    ep_rew_mean          | -54.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005838968 |\n",
      "|    clip_fraction        | 0.0645      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 298          |\n",
      "|    ep_rew_mean          | -52.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074622035 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.82         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0059      |\n",
      "|    std                  | 0.915        |\n",
      "|    value_loss           | 32.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 307          |\n",
      "|    ep_rew_mean          | -51.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 294          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065849866 |\n",
      "|    clip_fraction        | 0.071        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.32         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00759     |\n",
      "|    std                  | 0.906        |\n",
      "|    value_loss           | 28.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | -48.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 291          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069328765 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.88         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00549     |\n",
      "|    std                  | 0.909        |\n",
      "|    value_loss           | 12.9         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 346       |\n",
      "|    ep_rew_mean          | -45.3     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 289       |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 183       |\n",
      "|    total_timesteps      | 53248     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0070354 |\n",
      "|    clip_fraction        | 0.0679    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.64     |\n",
      "|    explained_variance   | 0.744     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 47.4      |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.00586  |\n",
      "|    std                  | 0.905     |\n",
      "|    value_loss           | 49.6      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 363          |\n",
      "|    ep_rew_mean          | -43.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 286          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067521264 |\n",
      "|    clip_fraction        | 0.0802       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00863     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 36.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 375         |\n",
      "|    ep_rew_mean          | -43         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005757274 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.62        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    std                  | 0.88        |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 391         |\n",
      "|    ep_rew_mean          | -39.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191512 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.7        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 411          |\n",
      "|    ep_rew_mean          | -37.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075193793 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 81.2         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00501     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 427        |\n",
      "|    ep_rew_mean          | -35        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00861115 |\n",
      "|    clip_fraction        | 0.051      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.52      |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.61       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0035    |\n",
      "|    std                  | 0.844      |\n",
      "|    value_loss           | 32.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 445         |\n",
      "|    ep_rew_mean          | -32.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010349799 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.9         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 462          |\n",
      "|    ep_rew_mean          | -31          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 271          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061270446 |\n",
      "|    clip_fraction        | 0.0736       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.39         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 480          |\n",
      "|    ep_rew_mean          | -29          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049967133 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.805        |\n",
      "|    value_loss           | 7.06         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 489         |\n",
      "|    ep_rew_mean          | -28.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005673277 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.71        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 8.9         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 507          |\n",
      "|    ep_rew_mean          | -26.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 280          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067978073 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    std                  | 0.801        |\n",
      "|    value_loss           | 5.15         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 524         |\n",
      "|    ep_rew_mean          | -24.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009767488 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 538         |\n",
      "|    ep_rew_mean          | -19.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007390581 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.787       |\n",
      "|    value_loss           | 9.57        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 554          |\n",
      "|    ep_rew_mean          | -16.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 258          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045356723 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.791        |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 574          |\n",
      "|    ep_rew_mean          | -12.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 258          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 317          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057009123 |\n",
      "|    clip_fraction        | 0.0705       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.89         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    std                  | 0.778        |\n",
      "|    value_loss           | 4.34         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 591          |\n",
      "|    ep_rew_mean          | -9.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 325          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044671623 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.3         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000929    |\n",
      "|    std                  | 0.777        |\n",
      "|    value_loss           | 45.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 608          |\n",
      "|    ep_rew_mean          | -6.41        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072612492 |\n",
      "|    clip_fraction        | 0.0809       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    std                  | 0.775        |\n",
      "|    value_loss           | 4.97         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 626         |\n",
      "|    ep_rew_mean          | -3.93       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009841194 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 646         |\n",
      "|    ep_rew_mean          | 1.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 353         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005601212 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 5.81        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 663        |\n",
      "|    ep_rew_mean          | 3.06       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 253        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932139 |\n",
      "|    clip_fraction        | 0.0776     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.27      |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.77       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.00408   |\n",
      "|    std                  | 0.75       |\n",
      "|    value_loss           | 29.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 670         |\n",
      "|    ep_rew_mean          | 8.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009699607 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000668   |\n",
      "|    std                  | 0.742       |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 687         |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005176369 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 58.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 706         |\n",
      "|    ep_rew_mean          | 14          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006862046 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 5.77        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 727          |\n",
      "|    ep_rew_mean          | 19.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 400          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112000285 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.92         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00526     |\n",
      "|    std                  | 0.728        |\n",
      "|    value_loss           | 41.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc50416ef50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_timesteps100000 = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 100000 timesteps\n",
    "model_timesteps100000.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAU5BGBTqaCA",
    "outputId": "6a899529-6a27-48a6-d125-5b612005c0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: 99.12\n",
      "Desviaci√≥n est√°ndar de las recompensas: 38.09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99.12029, 38.09349)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo optimizado\n",
    "evaluate_model(vec_env, model_timesteps100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCuky96rtpCD"
   },
   "outputs": [],
   "source": [
    "export_gif(model_timesteps100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fcB7AShuR22"
   },
   "source": [
    "![Comportamiento del agente entrenado](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwHduRpBu1Zw"
   },
   "source": [
    "Como se puede observar en el gif, la nave tiene, en general, un comportamiento m√°s refinado al ajustar su orientaci√≥n y acelerar de manera controlada para aterrizar en la plataforma. Esto, adem√°s, se aprecia en el nivel de recompensa promedio obtenido, que cumple con ser mayor a 50, mostrando que el agente aprende una pol√≠tica eficiente.\n",
    "\n",
    "Se destaca la relevancia de aumentar el n√∫mero de pasos de entrenamiento para generar un modelo con mejor desempe√±o, por sobre realizar variaciones en el resto de hiperpar√°metros vistos. Sin embargo, tambi√©n se menciona el aumento en el tiempo de entrenamiento, pasando de 15-20 segundos a 6 minutos, aproximadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ud2Xm_k-hFJn",
    "outputId": "ed6ccbe4-48df-45f3-9278-203c83fa4a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google AI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Enter your Tavily API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "e9ef4e6a-4438-42d1-9a03-15fcd6fe1b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEd7CKyV2Vhp",
    "outputId": "ac663375-d2f0-44cd-863e-5e8168781562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado y renombrado: Curso-Estadistica.pdf\n",
      "Descargado y renombrado: Curso-Aprendizaje-de-Maquinas.pdf\n",
      "Total de p√°ginas: 274\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "# Links a los documentos en GitHub\n",
    "github_links = [\n",
    "    \"https://github.com/GAMES-UChile/Curso-Estadistica/raw/master/notas_de_clase.pdf\",\n",
    "    \"https://github.com/GAMES-UChile/Curso-Aprendizaje-de-Maquinas/raw/master/notas_de_clase.pdf\"\n",
    "]\n",
    "\n",
    "# Paths locales como alternativa\n",
    "local_doc_paths = [\n",
    "    \"Notas_de_clase_Aprendizaje_de_Maquinas.pdf\",\n",
    "    \"Notas_de_clase_Estadistica.pdf\"\n",
    "]\n",
    "\n",
    "# Lista donde se almacenar√°n los paths a los documentos accesibles\n",
    "doc_paths = []\n",
    "\n",
    "# Se intentan descargar los archivos desde GitHub en primer lugar\n",
    "for url in github_links:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Se extrae el identificador √∫nico del URL\n",
    "            identifier = url.split(\"/\")[-4]  # Toma el segmento del curso (ejemplo: 'Curso-Estadistica') para diferenciar pdf's\n",
    "            filename = f\"{identifier}.pdf\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            doc_paths.append(filename)\n",
    "            print(f\"Descargado y renombrado: {filename}\")\n",
    "        else:\n",
    "            print(f\"No se pudo descargar desde GitHub: {url} (Status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar {url}: {e}\")\n",
    "\n",
    "# Si no se pudieron descargar desde GitHub, se usan los archivos locales\n",
    "# (se adjuntar√°n en la tarea entregada por UCursos, mas no en el repositorio).\n",
    "if len(doc_paths) < 2:\n",
    "    print(\"Intentando usar archivos locales...\")\n",
    "    for local_path in local_doc_paths:\n",
    "        if os.path.exists(local_path):\n",
    "            doc_paths.append(local_path)\n",
    "            print(f\"Archivo local encontrado: {local_path}\")\n",
    "        else:\n",
    "            print(f\"Archivo local no encontrado: {local_path}\")\n",
    "\n",
    "# Se verifica que se tienen al menos 2 documentos\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "# Se verifica que el total de p√°ginas es al menos 50\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\"\n",
    "\n",
    "print(f\"Total de p√°ginas: {total_paginas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iI5kYfBSGkzy",
    "outputId": "5526561e-6814-452a-d96d-8d729a61d2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8kZ5h3FBPxL",
    "outputId": "b1d04dda-6f3c-4bce-803b-789e59244c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-yXAdCSn4JM",
    "outputId": "920656a3-a60f-4a3d-8bf4-3109395c1a51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/html/_background_server.py:103: DeprecationWarning: make_current is deprecated; start the event loop first\n",
      "  ioloop.make_current()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrae texto de todas las p√°ginas de un archivo PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def create_documents_from_pdfs(doc_paths):\n",
    "    \"\"\"Convierte PDFs a objetos Document para procesar con LangChain.\"\"\"\n",
    "    documents = []\n",
    "    for doc_path in doc_paths:\n",
    "        text = extract_text_from_pdf(doc_path)\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": doc_path}))\n",
    "    return documents\n",
    "\n",
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorizaci√≥n y almacenamiento\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNDKi098BcqL",
    "outputId": "f2d4439f-5c8c-429e-d302-7fa152fe3f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 1362\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generaci√≥n de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # N√∫mero m√°ximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_UiEn1hoZYR",
    "outputId": "6bd6a7f1-eb72-4382-ba91-8679f09fb9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øQu√© es un estimador de m√°xima verosimilitud?\n",
      "Respuesta correcta: Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Seg√∫n el texto proporcionado, el estimador de m√°xima verosimilitud (EMV) es una funci√≥n de los datos que busca entregar un valor cercano a un par√°metro desconocido.  Se basa en la funci√≥n de verosimilitud, que representa la probabilidad de observar los datos dados un valor espec√≠fico del par√°metro.  El EMV se define como el valor del par√°metro que maximiza la funci√≥n de verosimilitud (o una funci√≥n mon√≥tonamente creciente de esta).  En otras palabras, el EMV es el valor del par√°metro que hace m√°s probable la observaci√≥n de los datos.\n",
      "\n",
      "Pregunta: ¬øQu√© es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) considera datos en forma de pares (dato, etiqueta) y el objetivo es estimar una funci√≥n f(x) tal que se tiene la siguiente igualdad.\n",
      "\n",
      "Pregunta: ¬øQu√© es un modelo de regresi√≥n lineal?\n",
      "Respuesta correcta: Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: En el contexto de un conjunto de entrenamiento  D que contiene N observaciones de entrada y salida,  fx·µ¢g·¥∫·µ¢‚Çå‚ÇÅ y fy·µ¢g·¥∫·µ¢‚Çå‚ÇÅ,  la regresi√≥n lineal busca encontrar un modelo lineal, es decir, una funci√≥n f definida por f:‚Ñù·µê‚Üí‚Ñù, x‚Ü¶f(x) = a·µÄx + b; a‚àà‚Ñù·µê; b‚àà‚Ñù.  En otras palabras, busca una relaci√≥n lineal entre las variables dependientes e independientes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¬øQu√© es un estimador de m√°xima verosimilitud?\", \"Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\"),\n",
    "    (\"¬øQu√© es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¬øQu√© es un modelo de regresi√≥n lineal?\", \"Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la soluci√≥n RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPb91-ksN7HT"
   },
   "source": [
    "En general, las respuestas entregadas por la soluci√≥n RAG son buenas. En todas ellas se explaya m√°s de lo que se aprecia en la tupla correcta, lo que tiene sentido ya que est√° sacando la informaci√≥n de notas de clase de un curso con alto enfoque matem√°tico.\n",
    "\n",
    "Ahora bien, se destaca que la segunda respuesta pareciera no conclu√≠r del todo, lo que puede deberse a varias razones: por un lado, el alto volumen de datos y/o complejidad de la pregunta podr√≠a estar generando un corte prematuro debido a limitaciones de tiempo; por otro, el valor de `temperature` definido es 0, lo que significa que el modelo est√° generando respuestas determin√≠sticas, es decir, m√°s directas y menos variadas, lo que implica que quiz√°s no est√° explorando de manera apropiada para completar las respuestas de mejor forma; y por √∫ltimo, que el modelo est√© tratando de generar una respuesta a partir de fragmentos que no contienen toda la informaci√≥n necesaria, o directamente presentan problemas al hacer la recuperaci√≥n de los documentos, generando un atasque en el modelo y, as√≠, una respuesta incompleta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Fx_1Y2Z0vV"
   },
   "source": [
    "Para comparar desempe√±os se obtuvieron los par√°metros por defecto (ejecutados en la iteraci√≥n anterior) del siguiente [link](https://python.langchain.com/api_reference/_modules/langchain_core/vectorstores/base.html#VectorStore.as_retriever).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrDKSEYkTRZM"
   },
   "source": [
    "##### Cantidad de chunks recuperados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li7XOksKTYtU"
   },
   "source": [
    "El n√∫mero de chunks que recupera el modelo depende de la cantidad de fragmentos que el recuperador de documentos (FAISS) retorna durante la b√∫squeda (por defecto son 4). El recuperar demasiados chunks puede generar que el modelo se sobrecargue y no logre identificar los fragmentos m√°s relevantes. Por otra parte, si se recuperan pocos chunks, puede que no tenga suficiente informaci√≥n para responder adecuadamente a la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1cDHHvDTToW",
    "outputId": "b54ba808-bdba-48b9-dd0a-dbdedf9f7043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øQu√© es un estimador de m√°xima verosimilitud?\n",
      "Respuesta correcta: Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un estimador de m√°xima verosimilitud (EMV) es una funci√≥n de los datos que busca encontrar el valor de un par√°metro desconocido que maximiza la probabilidad de haber observado los datos.  En otras palabras, se busca el valor del par√°metro que hace m√°s probable la muestra obtenida.  Se puede definir con respecto a la funci√≥n de verosimilitud o a cualquier funci√≥n no decreciente de esta.  A menudo, se maximiza la log-verosimilitud en lugar de la verosimilitud para facilitar los c√°lculos.\n",
      "\n",
      "Pregunta: ¬øQu√© es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) es un tipo de aprendizaje autom√°tico que utiliza datos etiquetados, es decir, pares (dato, etiqueta), para entrenar un modelo que pueda predecir la etiqueta de nuevos datos.  El objetivo es estimar una funci√≥n f(x) tal que etiqueta = f(dato).  Ejemplos incluyen la identificaci√≥n de spam en correos electr√≥nicos (clasificaci√≥n) o la estimaci√≥n del precio de una propiedad (regresi√≥n).\n",
      "\n",
      "Pregunta: ¬øQu√© es un modelo de regresi√≥n lineal?\n",
      "Respuesta correcta: Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un modelo de regresi√≥n lineal busca encontrar una relaci√≥n lineal entre una variable dependiente (salida, respuesta o etiqueta; usualmente denotada y) y una o m√°s variables independientes (est√≠mulo o caracter√≠stica; usualmente denotada x).  En esencia, intenta modelar c√≥mo cambia la variable dependiente cuando se modifica la variable(s) independiente(s) mediante una funci√≥n lineal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generaci√≥n de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # N√∫mero m√°ximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 16}),  # Se recuperan 16 chunks\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¬øQu√© es un estimador de m√°xima verosimilitud?\", \"Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\"),\n",
    "    (\"¬øQu√© es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¬øQu√© es un modelo de regresi√≥n lineal?\", \"Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la soluci√≥n RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR4jX58UdRTM"
   },
   "source": [
    "Como se puede ver, en general las respuestas son m√°s completas y precisas con respecto a las que se ten√≠an en la secci√≥n anterior. Adem√°s, ahora la segunda respuesta si logr√≥ terminar de generarse, lo que permite comprobar, al menos en parte, una de las razones dadas previamente de porqu√© no hab√≠a conclu√≠do. Se destaca que el modelo no se sobrecarg√≥ en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVIk5t3wTT2x"
   },
   "source": [
    "##### Tipo de b√∫squeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrqcO7_qT_qT"
   },
   "source": [
    "El tipo de b√∫squeda se refiere al algoritmo utilizado para recuperar los documentos m√°s relevantes. Por un lado, el m√©todo `similarity` (el que est√° definido por defecto) busca y devuelve los k documentos m√°s similares en funci√≥n de la distancia en el espacio de embeddings (como la distancia coseno o euclidiana). Por otro, el m√©todo `mmr` (Maximal Marginal Relevance) equilibra la relevancia y la diversidad de los documentos recuperados, lo que es √∫til si se quiere evitar que se devuelvan m√∫ltiples chunks con contenido muy similar. Por √∫ltimo, el m√©todo `similarity_score_threshold` solo devuelve documentos cuya puntuaci√≥n de similitud supere un umbral definido por el par√°metro score_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdSjDdgETWwU",
    "outputId": "3a7ce802-3c11-4285-93d4-2f2525859d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øQu√© es un estimador de m√°xima verosimilitud?\n",
      "Respuesta correcta: Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un estimador de m√°xima verosimilitud (EMV) es un m√©todo para estimar los par√°metros de una distribuci√≥n de probabilidad dada una muestra de datos.  En esencia, busca los valores de los par√°metros que maximizan la probabilidad de observar los datos que se han obtenido.  Es decir, encuentra los par√°metros que hacen que los datos observados sean lo m√°s probables posible.\n",
      "\n",
      "Pregunta: ¬øQu√© es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde un algoritmo aprende de un conjunto de datos etiquetados.  Esto significa que cada dato en el conjunto de entrenamiento est√° asociado con una etiqueta o salida correcta.  El algoritmo aprende a mapear las entradas a las salidas correctas, y luego puede usar este conocimiento para predecir las salidas de nuevas entradas que no ha visto antes.\n",
      "\n",
      "Pregunta: ¬øQu√© es un modelo de regresi√≥n lineal?\n",
      "Respuesta correcta: Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un modelo de regresi√≥n lineal es una herramienta estad√≠stica que se utiliza para modelar la relaci√≥n entre una variable dependiente (la que se quiere predecir) y una o m√°s variables independientes (las que se utilizan para predecir).  El modelo asume que esta relaci√≥n es lineal, es decir, que se puede representar mediante una l√≠nea recta (o un hiperplano en el caso de m√∫ltiples variables independientes).  La ecuaci√≥n general de un modelo de regresi√≥n lineal simple (con una sola variable independiente) es:\n",
      "\n",
      "Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ\n",
      "\n",
      "donde:\n",
      "\n",
      "* Y es la variable dependiente.\n",
      "* X es la variable independiente.\n",
      "* Œ≤‚ÇÄ es la intersecci√≥n (el valor de Y cuando X es 0).\n",
      "* Œ≤‚ÇÅ es la pendiente (el cambio en Y por cada unidad de cambio en X).\n",
      "* Œµ es el t√©rmino de error, que representa la variabilidad no explicada por el modelo.\n",
      "\n",
      "En el caso de una regresi√≥n lineal m√∫ltiple (con m√∫ltiples variables independientes), la ecuaci√≥n se extiende para incluir m√°s t√©rminos Œ≤·µ¢X·µ¢, uno para cada variable independiente.  El objetivo del modelo es estimar los valores de Œ≤‚ÇÄ y Œ≤‚ÇÅ, (y los Œ≤·µ¢ en el caso m√∫ltiple) que mejor ajusten la l√≠nea a los datos observados, minimizando el error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generaci√≥n de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # N√∫mero m√°ximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8}),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¬øQu√© es un estimador de m√°xima verosimilitud?\", \"Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\"),\n",
    "    (\"¬øQu√© es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¬øQu√© es un modelo de regresi√≥n lineal?\", \"Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la soluci√≥n RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545U9equb8Ul"
   },
   "source": [
    "De las respuestas generadas se destaca la gran relevancia que tiene el nivel de exigencia con respecto al umbral de similitud permitido, sobre todo para preguntas que no son tan directas de responder al requerir un conocimiento m√°s amplio. Adem√°s, se evidencia como ahora la √∫ltima pregunta tuvo una respuesta mucho m√°s completa (en un sentido matem√°tico) y apropiada/entendible (la respuesta a esta pregunta en la secci√≥n anterior ten√≠a una parte matem√°tica dif√≠cil de entender en un primer y r√°pido an√°lisis), lo que se entiende por el alto requisito m√≠nimo de similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtx17d_jS25Q"
   },
   "source": [
    "##### Tama√±o del chunk (chunk_size y chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeYeZgclTAdo"
   },
   "source": [
    "El tama√±o del chunk es el n√∫mero de tokens que contiene cada fragmento de texto extra√≠do de los documentos. Por una parte, el hecho de que los chunks sean m√°s peque√±os puede permitir que el modelo enfoque su atenci√≥n en partes m√°s espec√≠ficas del texto, pero pueden perder contexto si el fragmento no cubre informaci√≥n clave. Por otro lado, los chunks m√°s grandes pueden permitir una mejor cobertura de contexto, pero tambi√©n pueden hacer que el modelo se enfoque en demasiada informaci√≥n a la vez, lo que podr√≠a resultar en respuestas m√°s difusas o imprecisas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": [
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)  # Se disminuye chunk_size y chunk_overlap\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorizaci√≥n y almacenamiento\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSPjGZ0HjvxH",
    "outputId": "e0e0ab2b-aa5b-40c0-eb6c-dac2993cf948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 4127\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BorltMG8j3kh",
    "outputId": "76ff84ef-027c-4af8-f151-96c7e721d53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øQu√© es un estimador de m√°xima verosimilitud?\n",
      "Respuesta correcta: Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Based on the provided text, a maximum likelihood estimator (EMV) is an estimator that finds an estimator based on how probable it is that the estimator generated the data.  The text also mentions that the maximum likelihood estimator is Œ∏MV=max{xi}n, but doesn't fully explain what that means in a broader context.\n",
      "\n",
      "Pregunta: ¬øQu√© es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) utiliza datos en forma de pares (dato, etiqueta) con el objetivo de estimar una funci√≥n f(x) tal que se cumple una igualdad.\n",
      "\n",
      "Pregunta: ¬øQu√© es un modelo de regresi√≥n lineal?\n",
      "Respuesta correcta: Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Based on the provided text, a simple linear regression model is defined as:  Yi = Œ≤0 + Œ≤1Xi + Œµi, where E(Œµi) = 0, and V(Œµi) = œÉ2.  The goal is to estimate Œ≤0 and Œ≤1 to find the best linear approximation.  A linear regression model seeks a linear function,  f(x) = a<sup>T</sup>x + b, where 'a' is a vector and 'b' is a scalar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generaci√≥n de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # N√∫mero m√°ximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¬øQu√© es un estimador de m√°xima verosimilitud?\", \"Un estimador de m√°xima verosimilitud es un m√©todo para estimar los par√°metros de un modelo probabil√≠stico, buscando los valores que hacen que los datos observados sean m√°s probables bajo ese modelo.\"),\n",
    "    (\"¬øQu√© es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¬øQu√© es un modelo de regresi√≥n lineal?\", \"Un modelo de regresi√≥n lineal es un tipo de modelo estad√≠stico que busca establecer una relaci√≥n lineal entre una variable dependiente y una o m√°s variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la soluci√≥n RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj1hHx1zkhzV"
   },
   "source": [
    "Las respuestas generadas coinciden con lo mencionado sobre si se reduc√≠a el tama√±o del chunk: en general, se pierde contexto cuando el fragmento no cuenta con informaci√≥n clave. El modelo no logra generar respuestas del todo satisfactorias, ya sea porque menciona que el texto no explica de manera completa ciertos aspectos; no entrega la informaci√≥n de manera completa o bien explicada; o directamente experimenta un cambio de idioma, probablemente porque no ten√≠a en su contexto el que la pregunta era en espa√±ol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Se configura la tool Tavily para devolver un m√°ximo de 3 resultados por b√∫squeda\n",
    "tavily_tool = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlmreyrVrTwv",
    "outputId": "6546d92c-4259-4217-a47a-ccc6e7774537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=f29e7761509f0fc7b21d0b9656936848b7925a20ff553c7f5804c4200ab30b6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Se inicializa el wrapper de la API de Wikipedia\n",
    "wikipedia_api_wrapper = WikipediaAPIWrapper()\n",
    "\n",
    "# Se crea la tool\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pD1_n0wrsDI5",
    "outputId": "86371a09-fdbe-49e6-9f93-65c1a74c5445"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "# Prompt ReAct predefinido de LangChain\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools = [tavily_tool, wikipedia_tool]\n",
    "\n",
    "# Agente ReAct\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "# Se convierte el agente en un executor para usarlo\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pqo2dsxvywW_",
    "outputId": "f1ff4d31-d3f0-430f-c7c4-149184947318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: This question asks who wrote Don Quixote.  I should consult Wikipedia, as it's a well-known work of literature and Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Don Quixote\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Don Quixote\n",
      "Summary: Don Quixote, the full title being The Ingenious Gentleman Don Quixote of La Mancha, is a Spanish novel by Miguel de Cervantes. It was originally published in two parts, in 1605 and 1615. Considered a founding work of Western literature, it is often said to be the first modern novel. Don Quixote is also one of the most-translated books in the world and one of the best-selling novels of all time. \n",
      "The plot revolves around the adventures of a member of the lowest nobility, an hidalgo from La Mancha named Alonso Quijano, who reads so many chivalric romances that he loses his mind and decides to become a knight-errant (caballero andante) to revive chivalry and serve his nation, under the name Don Quixote de la Mancha. He recruits as his squire a simple farm labourer, Sancho Panza, who brings a unique, earthy wit to Don Quixote's lofty rhetoric. In the first part of the book, Don Quixote does not see the world for what it is and prefers to imagine that he is living out a knightly story meant for the annals of all time. However, as Salvador de Madariaga pointed out in his Gu√≠a del lector del Quijote (1972 [1926]), referring to \"the Sanchification of Don Quixote and the Quixotization of Sancho\", as \"Sancho's spirit ascends from reality to illusion, Don Quixote's declines from illusion to reality\".\n",
      "The book had a major influence on the literary community, as evidenced by direct references in Alexandre Dumas's The Three Musketeers (1844), and Edmond Rostand's Cyrano de Bergerac (1897) as well as the word quixotic. Mark Twain referred to the book as having \"swept the world's admiration for the mediaeval chivalry-silliness out of existence\". It has been described by some as the greatest work ever written.\n",
      "\n",
      "Page: Don Quixote (ballet)\n",
      "Summary: Don Quixote is a ballet in three acts, based on episodes taken from the famous novel Don Quixote de la Mancha by Miguel de Cervantes. It was originally choreographed by Marius Petipa to the music of Ludwig Minkus and first presented by Moscow's Bolshoi Ballet on 26 December [O.S. 14 December] 1869. Petipa and Minkus revised the ballet into a more elaborate and expansive version in five acts and eleven scenes for the Mariinsky Ballet, first presented on 21 November [O.S. 9 November] 1871 at the Imperial Bolshoi Kamenny Theatre of St. Petersburg.\n",
      "All modern productions of the Petipa/Minkus ballet are derived from the version staged by Alexander Gorsky for the Bolshoi Theatre of Moscow in 1900, a production the ballet master staged for the Imperial Ballet of St. Petersburg in 1902.\n",
      "\n",
      "Page: List of Don Quixote characters\n",
      "Summary: The following is a partial list of characters in the novel Don Quixote de la Mancha by Miguel de Cervantes Saavedra.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Miguel de Cervantes\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Miguel de Cervantes\n"
     ]
    }
   ],
   "source": [
    "# Pregunta que deber√≠a usar Wikipedia\n",
    "response = agent_executor.invoke({\"input\": \"¬øQui√©n escribi√≥ Don Quijote de la Mancha?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfgXiQkosbmg",
    "outputId": "57620984-e62a-4a32-825b-f97f2ffb1e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer the question about the latest news on artificial intelligence, I need to use a search engine that provides current and reliable information.  Tavily_search_results_json seems best suited for this.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"latest news artificial intelligence\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://techcrunch.com/category/artificial-intelligence/', 'content': 'AI News & Artificial Intelligence | TechCrunch AI News & Artificial Intelligence | TechCrunch AI Site Search Toggle AI AI News coverage on artificial intelligence and machine learning tech, the companies building them, and the ethical issues AI raises today. AI AI AI AI AI AI AI Juna.ai wants to use AI agents to make factories more energy-efficient Norwegian startup Factiverse wants to fight disinformation with AI AI A popular technique to make AI more efficient has drawbacks AI AI AI AI Here‚Äôs the full list of 44 US AI startups that have raised $100M or more in 2024 OpenAI at one point considered acquiring AI chip startup Cerebras AI AI AI AI AI A popular technique to make AI more efficient has drawbacks'}, {'url': 'https://apnews.com/hub/artificial-intelligence', 'content': \"This week‚Äôs turmoil with ChatGPT-maker OpenAI has heightened trust concerns in the AI world\\nWhat you need to know about Emmett Shear, OpenAI‚Äôs new interim CEO\\nInsider Q&A: Pentagon AI chief on network-centric warfare, generative AI challenges\\n‚ÄòPlease regulate AI:' Artists push for U.S. copyright reforms but tech industry says not so fast\\nBusiness Highlights: Advertisers flee X as Musk endorses antisemitic conspiracy; Open AI ditches CEO\\nChatGPT-maker OpenAI fires CEO Sam Altman, the face of the AI boom, for lack of candor with company\\nAmazon lays off hundreds in its Alexa division as it plows resources into AI\\nYouTube creators will soon have to disclose use of generative AI in videos or AWS chief Adam Selipsky talks generative AI, Amazon‚Äôs investment in Anthropic and cloud cost cutting\\nEurope reaches a deal on the world‚Äôs first comprehensive AI rules\\nNvidia CEO suggests Malaysia could be AI ‚Äòmanufacturing‚Äô hub as Southeast Asia expands data centers\\nEurope‚Äôs talks on world-leading AI rules paused after 22 hours and will start again Friday\\nGoogle launches Gemini, upping the stakes in the global AI race\\nBank of England will review the risks that AI poses to UK financial stability\\nEurope was set to lead the world on AI regulation. Artificial intelligence\\nChatGPT-maker braces for fight with New York Times and authors on ‚Äòfair use‚Äô of copyrighted works\\nCES 2024 updates: The most interesting news and gadgets from tech‚Äôs big show\\nMaryland governor signs executive order guiding AI use\\nJudges in England and Wales are given cautious approval to use AI in writing legal opinions\\nMicrosoft‚Äôs new AI key is first big change to keyboards in decades\\n Tech giants are divided as they lobby regulators\\nEurope‚Äôs world-leading artificial intelligence rules are facing a do-or-die moment\\nSports Illustrated is the latest media company damaged by an AI experiment gone wrong\\nPentagon‚Äôs AI initiatives accelerate hard decisions on lethal autonomous weapons\\nPutin to boost AI work in Russia to fight a Western monopoly he says is ‚Äòunacceptable and dangerous‚Äô\\n Now he wants a treaty regulating AI\\nCongressional candidate‚Äôs voter outreach tool is latest AI experiment ahead of 2024 elections\\nEurope agreed on world-leading AI rules.\"}, {'url': 'https://news.mit.edu/topic/artificial-intelligence2', 'content': 'October 30, 2023\\nRead full story ‚Üí\\nAccelerating AI tasks while preserving data security\\nThe SecureLoop search tool efficiently identifies secure designs for hardware that can boost the performance of complex AI tasks, while requiring less energy.\\nOctober 30, 2023\\nRead full story ‚Üí\\nThe brain may learn about the world the same way some computational models do\\nTwo studies find ‚Äúself-supervised‚Äù models, which learn about their environment from unlabeled data, can show activity patterns similar to those of the mammalian brain.\\nOctober 30, 2023\\nRead full story ‚Üí\\nCelebrating Kendall Square‚Äôs past and shaping its future\\nThe 15th Kendall Square Association annual meeting explored new and old aspects of the neighborhood.\\nOctober 23, 2023\\nRead full story ‚Üí\\nTo excel at engineering design, generative AI must learn to innovate, study finds\\nAI models that prioritize similarity falter when asked to design something completely new.\\n Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nTopic\\nArtificial intelligence\\nDownload RSS feed: News Articles / In the Media / Audio\\nSearch algorithm reveals nearly 200 new kinds of CRISPR systems\\nBy analyzing bacterial data, researchers have discovered thousands of rare new CRISPR systems that have a range of functions and could enable gene editing, diagnostics, and more.\\n November 15, 2023\\nRead full story ‚Üí\\nExplained: Generative AI\\nHow do powerful generative AI systems like ChatGPT work, and what makes them different from other types of artificial intelligence?\\nNovember 9, 2023\\nRead full story ‚Üí\\nUsing AI to optimize for rapid neural imaging\\nMIT CSAIL researchers combine AI and electron microscopy to expedite detailed brain network mapping, aiming to enhance connectomics research and clinical pathology.\\n November 2, 2023\\nRead full story ‚Üí\\nNew techniques efficiently accelerate sparse tensors for massive AI models\\nComplimentary approaches ‚Äî ‚ÄúHighLight‚Äù and ‚ÄúTailors and Swiftiles‚Äù ‚Äî could boost the performance of demanding machine-learning tasks.\\n November 21, 2023\\nRead full story ‚Üí\\nSynthetic imagery sets new bar in AI training efficiency\\nMIT CSAIL researchers innovate with synthetic imagery to train AI, paving the way for more efficient and bias-reduced machine learning.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Tavily search results provide a good overview of recent AI news from various sources.  I can synthesize this information to answer the question.\n",
      "\n",
      "Thought:I now know the final answer.\n",
      "\n",
      "Final Answer: Las √∫ltimas noticias sobre inteligencia artificial incluyen el despido del CEO de OpenAI, Sam Altman;  el desarrollo de nuevas regulaciones de IA en Europa;  avances en modelos de IA generativa como Gemini de Google;  preocupaciones sobre el uso √©tico de la IA y su impacto en diversas industrias (incluyendo medios de comunicaci√≥n y arte); y  el aumento de la inversi√≥n y desarrollo en IA por parte de grandes empresas tecnol√≥gicas.  Tambi√©n hay noticias sobre el uso de la IA en sectores como la manufactura, la defensa y la atenci√≥n m√©dica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Las √∫ltimas noticias sobre inteligencia artificial incluyen el despido del CEO de OpenAI, Sam Altman;  el desarrollo de nuevas regulaciones de IA en Europa;  avances en modelos de IA generativa como Gemini de Google;  preocupaciones sobre el uso √©tico de la IA y su impacto en diversas industrias (incluyendo medios de comunicaci√≥n y arte); y  el aumento de la inversi√≥n y desarrollo en IA por parte de grandes empresas tecnol√≥gicas.  Tambi√©n hay noticias sobre el uso de la IA en sectores como la manufactura, la defensa y la atenci√≥n m√©dica.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta que deber√≠a usar Tavily\n",
    "response = agent_executor.invoke({\"input\": \"¬øCu√°les son las √∫ltimas noticias sobre inteligencia artificial?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRZu29jGsyN0"
   },
   "source": [
    "El agente deber√≠a ocupar Tavily para preguntas sobre temas actuales o informaci√≥n din√°mica que no suele estar en una enciclopedia, como noticias o eventos recientes. Por otra parte, deber√≠a ocupar Wikipedia para preguntas hist√≥ricas, acad√©micas o generales que tienen respuestas bien documentadas en su base de conocimiento.\n",
    "\n",
    "De esta forma, seg√∫n la verificaci√≥n de respuestas realizadas se aprecia que el agente est√° usando correctamente las tools disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm3hMsbMuGRb"
   },
   "source": [
    "Se ejecutar√° de nuevo la soluci√≥n RAG, utilizando la versi√≥n previa a la prueba de hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCurn5CKuWvU",
    "outputId": "ec7cbfa3-dbc4-4b14-e1f3-9354d433e9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 1362\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrae texto de todas las p√°ginas de un archivo PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def create_documents_from_pdfs(doc_paths):\n",
    "    \"\"\"Convierte PDFs a objetos Document para procesar con LangChain.\"\"\"\n",
    "    documents = []\n",
    "    for doc_path in doc_paths:\n",
    "        text = extract_text_from_pdf(doc_path)\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": doc_path}))\n",
    "    return documents\n",
    "\n",
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorizaci√≥n y almacenamiento\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "CkSfbC7EueeL"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generaci√≥n de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # N√∫mero m√°ximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-5EczyVuTGn"
   },
   "source": [
    "Ahora s√≠, se generan las tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def rag_tool(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa la soluci√≥n RAG para responder preguntas basadas en el contenido de los PDFs.\n",
    "    \"\"\"\n",
    "    response = qa_chain.run(question)\n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def react_agent_tool(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa el agente ReAct para responder preguntas usando Tavily y Wikipedia.\n",
    "    \"\"\"\n",
    "    response = agent_executor.invoke({\"input\": input_text})\n",
    "    return response[\"output\"]\n",
    "\n",
    "# Lista de herramientas transformadas\n",
    "tools = [rag_tool, react_agent_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "# Prompt para el agente supervisor\n",
    "supervisor_prompt = \"\"\"\n",
    "Eres un agente supervisor con acceso a herramientas especializadas:\n",
    "1. Usa `rag_tool` para responder preguntas relacionadas con informaci√≥n extra√≠da de PDFs.\n",
    "2. Usa `react_agent_tool` para responder preguntas que requieren b√∫squedas en la web.\n",
    "\n",
    "Pregunta: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "# Agente supervisor\n",
    "supervisor_agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,  # Lista de herramientas\n",
    "    prompt=PromptTemplate.from_template(supervisor_prompt)\n",
    ")\n",
    "\n",
    "# Se convierte el agente en ejecutor\n",
    "supervisor = AgentExecutor(agent=supervisor_agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_1t0zkgv1qW",
    "outputId": "747bc9ab-1887-40fd-f37a-958e302acbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¬øQu√© es un estimador de m√°xima verosimilitud?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¬øQu√© es un estimador de m√°xima verosimilitud?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to understand what a maximum likelihood estimator is.  A Wikipedia search should provide a good definition and explanation.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Maximum likelihood estimation\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Maximum likelihood estimation\n",
      "Summary: In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.\n",
      "If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when the random errors are assumed to have normal distributions with the same variance.\n",
      "From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with a prior distribution that is uniform in the region of interest. In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.\n",
      "\n",
      "Page: Maximum a posteriori estimation\n",
      "Summary: In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate.  MAP estimation can therefore be seen as a regularization of maximum likelihood estimation.\n",
      "\n",
      "Page: Likelihood function\n",
      "Summary: A likelihood function (often simply called the likelihood) measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. It is constructed from the joint probability distribution of the random variable that (presumably) generated the observations. When evaluated on the actual data points, it becomes a function solely of the model parameters.\n",
      "In maximum likelihood estimation, the argument that maximizes the likelihood function serves as a point estimate for the unknown parameter, while the Fisher information (often approximated by the likelihood's Hessian matrix at the maximum) gives an indication of the estimate's precision.\n",
      "In contrast, in Bayesian statistics, the estimate of interest is the converse of the likelihood, the so-called posterior probability of the parameter given the observed data, which is calculated via Bayes' rule.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The Wikipedia articles provide a good explanation.\n",
      "\n",
      "Final Answer: Un estimador de m√°xima verosimilitud (MLE) es un m√©todo para estimar los par√°metros de una distribuci√≥n de probabilidad asumida, dados algunos datos observados.  Se logra maximizando una funci√≥n de verosimilitud para que, bajo el modelo estad√≠stico asumido, los datos observados sean los m√°s probables. El punto en el espacio de par√°metros que maximiza la funci√≥n de verosimilitud se llama estimaci√≥n de m√°xima verosimilitud.  La l√≥gica de la m√°xima verosimilitud es intuitiva y flexible, y como tal, el m√©todo se ha convertido en un medio dominante de inferencia estad√≠stica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mUn estimador de m√°xima verosimilitud (MLE) es un m√©todo para estimar los par√°metros de una distribuci√≥n de probabilidad asumida, dados algunos datos observados.  Se logra maximizando una funci√≥n de verosimilitud para que, bajo el modelo estad√≠stico asumido, los datos observados sean los m√°s probables. El punto en el espacio de par√°metros que maximiza la funci√≥n de verosimilitud se llama estimaci√≥n de m√°xima verosimilitud.  La l√≥gica de la m√°xima verosimilitud es intuitiva y flexible, y como tal, el m√©todo se ha convertido en un medio dominante de inferencia estad√≠stica.\u001b[0m\u001b[32;1m\u001b[1;3mUn estimador de m√°xima verosimilitud (MLE) es un m√©todo para estimar los par√°metros de una distribuci√≥n de probabilidad asumida, dados algunos datos observados. Se logra maximizando una funci√≥n de verosimilitud para que, bajo el modelo estad√≠stico asumido, los datos observados sean los m√°s probables. El punto en el espacio de par√°metros que maximiza la funci√≥n de verosimilitud se llama estimaci√≥n de m√°xima verosimilitud. La l√≥gica de la m√°xima verosimilitud es intuitiva y flexible, y como tal, el m√©todo se ha convertido en un medio dominante de inferencia estad√≠stica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: Un estimador de m√°xima verosimilitud (MLE) es un m√©todo para estimar los par√°metros de una distribuci√≥n de probabilidad asumida, dados algunos datos observados. Se logra maximizando una funci√≥n de verosimilitud para que, bajo el modelo estad√≠stico asumido, los datos observados sean los m√°s probables. El punto en el espacio de par√°metros que maximiza la funci√≥n de verosimilitud se llama estimaci√≥n de m√°xima verosimilitud. La l√≥gica de la m√°xima verosimilitud es intuitiva y flexible, y como tal, el m√©todo se ha convertido en un medio dominante de inferencia estad√≠stica.\n",
      "\n",
      "Pregunta: ¬øQu√© es el aprendizaje supervisado?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¬øQu√© es el aprendizaje supervisado?'}`\n",
      "responded: Para responder a la pregunta \"¬øQu√© es el aprendizaje supervisado?\", usar√© la herramienta `react_agent_tool` ya que requiere informaci√≥n general que probablemente se encuentre en la web.  `rag_tool` se enfoca en PDFs espec√≠ficos.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about supervised learning, I need a definition.  Wikipedia is a good resource for this type of general definition.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Supervised learning\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Supervised learning\n",
      "Summary: Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n",
      "\n",
      "Page: Self-supervised learning\n",
      "Summary: Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects. \n",
      "During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.\n",
      "Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition. \n",
      "\n",
      "\n",
      "\n",
      "Page: Weak supervision\n",
      "Summary: Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer. Wikipedia provided a good definition of supervised learning.\n",
      "\n",
      "Final Answer: El aprendizaje supervisado (SL) es un paradigma en el aprendizaje autom√°tico donde los objetos de entrada (por ejemplo, un vector de variables predictoras) y un valor de salida deseado (tambi√©n conocido como se√±al supervisora etiquetada por humanos) entrenan un modelo. Los datos de entrenamiento se procesan, creando una funci√≥n que asigna nuevos datos a los valores de salida esperados. Un escenario √≥ptimo permitir√° que el algoritmo determine correctamente los valores de salida para instancias no vistas. Esto requiere que el algoritmo de aprendizaje generalice a partir de los datos de entrenamiento a situaciones no vistas de una manera \"razonable\" (ver sesgo inductivo). Esta cualidad estad√≠stica de un algoritmo se mide a trav√©s del llamado error de generalizaci√≥n.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mEl aprendizaje supervisado (SL) es un paradigma en el aprendizaje autom√°tico donde los objetos de entrada (por ejemplo, un vector de variables predictoras) y un valor de salida deseado (tambi√©n conocido como se√±al supervisora etiquetada por humanos) entrenan un modelo. Los datos de entrenamiento se procesan, creando una funci√≥n que asigna nuevos datos a los valores de salida esperados. Un escenario √≥ptimo permitir√° que el algoritmo determine correctamente los valores de salida para instancias no vistas. Esto requiere que el algoritmo de aprendizaje generalice a partir de los datos de entrenamiento a situaciones no vistas de una manera \"razonable\" (ver sesgo inductivo). Esta cualidad estad√≠stica de un algoritmo se mide a trav√©s del llamado error de generalizaci√≥n.\u001b[0m\u001b[32;1m\u001b[1;3mEl aprendizaje supervisado es un paradigma de aprendizaje autom√°tico donde se utiliza un conjunto de datos etiquetados para entrenar un modelo.  Estos datos consisten en pares de entrada-salida, donde la entrada es el dato que se introduce al modelo y la salida es el resultado esperado. El modelo aprende a mapear las entradas a las salidas correspondientes a trav√©s de un proceso de entrenamiento.  Una vez entrenado, el modelo puede predecir la salida para nuevas entradas que no ha visto antes.  Requiere datos etiquetados previamente por humanos, lo que lo diferencia de otros tipos de aprendizaje autom√°tico como el no supervisado.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: El aprendizaje supervisado es un paradigma de aprendizaje autom√°tico donde se utiliza un conjunto de datos etiquetados para entrenar un modelo.  Estos datos consisten en pares de entrada-salida, donde la entrada es el dato que se introduce al modelo y la salida es el resultado esperado. El modelo aprende a mapear las entradas a las salidas correspondientes a trav√©s de un proceso de entrenamiento.  Una vez entrenado, el modelo puede predecir la salida para nuevas entradas que no ha visto antes.  Requiere datos etiquetados previamente por humanos, lo que lo diferencia de otros tipos de aprendizaje autom√°tico como el no supervisado.\n",
      "\n",
      "Pregunta: ¬øQu√© equipo gan√≥ la √∫ltima Eurocopa?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¬øQu√© equipo gan√≥ la √∫ltima Eurocopa?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out which team won the most recent UEFA European Championship.  Wikipedia should have this information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: UEFA European Championship\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: UEFA European Championship\n",
      "Summary: The UEFA European Football Championship, less formally the European Championship and informally the Euro or Euros, is the primary association football tournament organised by the Union of European Football Associations (UEFA). The competition is contested by UEFA members' senior men's national teams, determining the continental champion of Europe. It is the second-most watched football tournament in the world after the FIFA World Cup; the Euro 2016 final was watched by a global audience of around 600 million. The competition has been held every four years since 1960, except for 2020, when it was postponed until 2021 due to the COVID-19 pandemic in Europe, but kept the name Euro 2020. Scheduled to be in the even-numbered year between FIFA World Cup tournaments, it was originally called the European Nations' Cup before changing to its current name in 1968. Since 1996, the individual events have been branded as \"UEFA Euro [year]\".\n",
      "Before entering the tournament, all teams other than the host nations (which qualify automatically) compete in a qualifying process. Until 2016, the championship winners could compete in the following year's FIFA Confederations Cup, but were not obliged to do so. From the 2020 edition onwards, the winner competes in the CONMEBOL‚ÄìUEFA Cup of Champions.\n",
      "The seventeen European Championship tournaments have been won by ten national teams: Spain have won four titles, Germany have won three titles, Italy and France have won two titles, and the Soviet Union, Czechoslovakia, the Netherlands, Denmark, Greece and Portugal have won one title each. To date, Spain is the only team to have won consecutive titles, doing so in 2008 and 2012.\n",
      "The most recent championship, held in Germany in 2024, was won by Spain, who lifted a record fourth European title after beating England 2‚Äì1 in the final at the Olympiastadion in Berlin.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA European Under-21 Championship\n",
      "Summary: The UEFA European Under-21 Championship, the UEFA Under-21 Championship or simply the Euro Under-21, is a biennial football competition contested by the European men's under-21 national teams of the UEFA member associations. Since 1992, the competition also serves as the UEFA qualification tournament for the Summer Olympics.\n",
      "Italy and Spain are the most successful teams in the tournament's history, having won five titles each. England are the current champions, having beaten Spain 1‚Äì0 in the 2023 final.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA European Under-19 Championship\n",
      "Summary: The UEFA European Under-19 Championship, or simply the Euro Under-19, is an annual football competition, contested by the European men's under-19 national teams of the UEFA member associations.\n",
      "Spain is the most successful team in this competition, having won twelve titles. Spain are also the current champions.\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia article mentions the most recent Euro (Euro 2020, actually held in 2021) but doesn't state the winner.  It also mentions a future Euro 2024, but that hasn't happened yet. I need to refine my search.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Euro 2020 winner\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: UEFA Euro 2020 final\n",
      "Summary: The UEFA Euro 2020 final was an association football match that took place at Wembley Stadium in London, England, on 11 July 2021, to determine the winners of UEFA Euro 2020. It was the sixteenth final of the UEFA European Championship, a quadrennial tournament contested by the senior men's national teams of the member associations of UEFA to decide the champions of Europe. Originally scheduled for 12 July 2020, the match had been postponed along with the rest of the tournament due to the COVID-19 pandemic in Europe. The match was contested between Italy, in their fourth Euro final, and England, in their first ever Euro final, and just their second final at any major tournament, after the 1966 FIFA World Cup final.\n",
      "In front of a crowd of 67,173, limited by COVID-19 restrictions, with an estimated global audience of 328 million, England's Luke Shaw opened the scoring in the second minute of the match, the fastest goal ever scored in a European Championship final. Leonardo Bonucci ‚Äì who was later named the man of the match ‚Äì scored an equaliser midway through the second half. With the score 1‚Äì1 after extra time, England gained a 2‚Äì1 advantage in the penalty shoot-out after two kicks each, but their last three takers all missed, and Italy won 3‚Äì2.\n",
      "This was Italy's first major title since the 2006 FIFA World Cup, and their first European Championship since winning it on home soil in 1968; in terms of European Championship titles, it put Italy level with France on two titles, and one title behind Spain and Germany. England became the third nation in the 21st century to lose the European Championship final on home soil, following Portugal in 2004 and France in 2016. After the match, England's unsuccessful penalty takers (Marcus Rashford, Jadon Sancho and Bukayo Saka) were subjected to racial abuse on social media, which was investigated by the Metropolitan Police. The event was also marred by crowd disorder as roughly six thousand ticketless England supporters fought police and security in attempts to breach the stadium.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA Euro 2020\n",
      "Summary: The 2020 UEFA European Football Championship, commonly referred to as UEFA Euro 2020 or simply Euro 2020, was the 16th UEFA European Championship, the quadrennial international men's football championship of Europe organised by the Union of European Football Associations (UEFA). To celebrate the diamond jubilee of the European Championship competition, UEFA president Michel Platini declared that the tournament would be hosted in several nations as a \"romantic\" one-off event, with 11 cities in 11 UEFA countries each providing venues for the tournament, making it the second senior international tournament in history after the 2007 AFC Asian Cup to have more than two nations co-hosting it.\n",
      "Portugal were the defending champions, but were eliminated in the round of 16 by Belgium. Italy won their second European Championship title by beating England on penalties in the final following a 1‚Äì1 draw after extra time. The win came exactly on the 39th anniversary of Italy's 1982 FIFA World Cup final victory over West Germany.\n",
      "The tournament was originally intended to be played between 12 June and 12 July 2020. Due to COVID-19 restrictions during that year, the tournament was postponed to June and July 2021, while retaining the name UEFA Euro 2020 and host venues. Alongside special rules regarding COVID-19 restrictions, UEFA also allowed two extra substitutions and implemented video assistant referee (VAR) for the first time. Initially, there were 13 venues chosen for the tournament but two were later dropped. Brussels was dropped in December 2017 after the city's Eurostadium was abandoned, while Dublin was dropped in April 2021 because there was no guarantee that spectators could attend. Spain originally intended to use Bilbao as a host venue but later changed it to Seville to allow for spectators at matches. UEFA chose Stadio Olimpico in Rome to host the open\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Italy won the last Eurocopa (Euro 2020, held in 2021).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mItaly won the last Eurocopa (Euro 2020, held in 2021).\u001b[0m\u001b[32;1m\u001b[1;3mItalia gan√≥ la √∫ltima Eurocopa (Eurocopa 2020, celebrada en 2021).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: Italia gan√≥ la √∫ltima Eurocopa (Eurocopa 2020, celebrada en 2021).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preguntas de prueba\n",
    "questions = [\n",
    "    \"¬øQu√© es un estimador de m√°xima verosimilitud?\",\n",
    "    \"¬øQu√© es el aprendizaje supervisado?\",\n",
    "    \"¬øQu√© equipo gan√≥ la √∫ltima Eurocopa?\"\n",
    "]\n",
    "\n",
    "# Se obtienen respuestas del supervisor\n",
    "for question in questions:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    response = supervisor.invoke({\"input\": question})\n",
    "    print(f\"Respuesta del supervisor: {response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Zbntu8xDG5"
   },
   "source": [
    "El agente supervisor decide qu√© herramienta utilizar en funci√≥n de la pregunta. Seg√∫n lo esperado, para preguntas relacionadas con los PDFs entregados, deber√≠a usar `rag_tool`, y para preguntas que requieran buscar en la web, usar√° `react_agent_tool`. Ahora bien, como se puede ver, el agente siempre busca por Wikipedia, inclusive para aquellas preguntas que est√°n muy relacionadas a los archivos PDF. Esto se debe principalmente a la generalidad de las preguntas realizadas, ya que basta con una b√∫squeda r√°pida en Wikipedia para responderlas. Ahora bien, si se hicieran preguntas mucho m√°s espec√≠ficas que su respuesta probablemente no est√© en Wikipedia, como por ejemplo algunas relacionadas a un paper espec√≠fico de la materia de estad√≠stica o aprendizaje de m√°quinas, probablemente s√≠ vaya a buscar la respuesta utilizando `rag_tool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "Un Router es un modelo expl√≠citamente programado para clasificar preguntas y dirigirlas hacia la herramienta adecuada bas√°ndose en reglas predefinidas (como un clasificador de texto). En contraste, el enfoque con el agente supervisor utiliza un LLM para razonar din√°micamente sobre qu√© herramienta utilizar.\n",
    "\n",
    "El enfoque del agente supervisor cuenta con la ventaja de que puede razonar sobre preguntas m√°s complejas, mientras que un Router es m√°s r√≠gido al depender de reglas espec√≠ficas. Adem√°s, se destaca que no requiere entrenamiento adicional, ya que el razonamiento pasa por el LLM. Por otro lado, el agente supervisor cuenta con la desventaja asociada al costo computacional, ya que usar un LLM como razonador es m√°s costoso que utilizar una soluci√≥n simplemente basada en reglas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8TsvnCPbkIA",
    "outputId": "11d72e1b-7c40-4b0a-b91b-e9e6875575e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "Z3KedQSvg1-n",
    "outputId": "a4157ff5-75d0-43b6-b651-fb82218ba763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://93ee8203a294149d4c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93ee8203a294149d4c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\",\n",
    "    description=\"Hola! Soy un chatbot de aprendizaje de m√°quinas y estad√≠stica c:\",\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qBPet_Mq8dX9",
    "ZJ6J1_-Y9nHO",
    "pmcX6bRC9agQ",
    "LEO_dY4x_SJu",
    "E-bpdb8wZID1",
    "RO-EsAaPAYEm",
    "YChodtNQwzG2",
    "hQrZVQflX_5f",
    "3z-oIUSrlAsY",
    "x6Xw4YHT3P5d",
    "_vsvq177oKS0",
    "wScMyWZioT7M",
    "SYhhMl_GoCtg",
    "mQ4fPRRihGLe",
    "ZrxOQroVnaZ5",
    "Qtx17d_jS25Q",
    "V47l7Mjfrk0N",
    "SonB1A-9rtRq",
    "CvUIMdX6r0ne",
    "dKV0JxK3r-XG",
    "4JWVSuWiZ8Mj",
    "vFc3jBT5g0kT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

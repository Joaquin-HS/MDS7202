{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Joaquín Herrera Suárez\n",
    "- Nombre de alumno 2: Hecmar Taucare Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Joaquin-HS/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "84ea39c9-2ffb-4a3a-a173-f524c6e77270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/958.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m757.8/958.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiente Blackjack-v1 modela el clásico juego de cartas Blackjack, donde el objetivo es vencer al dealer logrando que la suma de las cartas propias se acerque más a 21, sin excederlo. El juego tiene una formulación como un Proceso de Decisión de Markov (MDP), definido por los siguientes componentes:\n",
    "\n",
    "1. Estados: \\\\\n",
    "Representados como una tupla (player_sum, dealer_card, usable_ace):\n",
    "  - player_sum (4 - 21): La suma actual de las cartas del jugador.\n",
    "  - dealer_card (1 - 10): El valor de la carta visible del dealer.\n",
    "  - usable_ace (0 o 1): Indica si el jugador tiene un as que puede contarse como 11 sin exceder 21.\n",
    "\n",
    "2. Acciones: \\\\\n",
    "El espacio de acciones es Discrete(2) con las siguientes opciones:\n",
    "  - 0 (Stick): Detenerse y no tomar más cartas.\n",
    "  - 1 (Hit): Pedir una carta adicional.\n",
    "\n",
    "3. Recompensas: \\\\\n",
    "Las recompensas están definidas según el resultado del juego:\n",
    "  - +1: Victoria del jugador.\n",
    "  - -1: Derrota del jugador.\n",
    "  - 0: Empate.\n",
    "  - +1.5: Victoria con un blackjack natural (si natural=True).\n",
    "\n",
    "4. Dinámica de transición: \\\\\n",
    "Si el jugador pide una carta (Hit), se actualiza su estado sumando el valor de la nueva carta. Si supera 21, el juego termina con una recompensa de -1.\n",
    "Si el jugador se detiene (Stick), el dealer juega hasta alcanzar una suma mínima de 17. Luego, se compara la suma final del dealer con la del jugador para determinar el resultado.\n",
    "\n",
    "5. Estado inicial: \\\\\n",
    "El juego comienza con una suma inicial para el jugador entre 4 y 21, una carta visible del dealer entre 1 y 10, y un indicador de si el jugador tiene un as utilizable.\n",
    "\n",
    "6. Fin del juego: \\\\\n",
    "El juego termina cuando, o bien el jugador supera 21 (bust), o cuando el jugador decide detenerse (Stick) y se evalúa el resultado contra el dealer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "c80d3d00-965b-48f4-f998-1524a862bf9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -0.38\n",
      "Desviación estándar de recompensas: 0.90\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Número de simulaciones\n",
    "n_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "# Se simulan 5000 episodios\n",
    "for _ in range(n_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Se selecciona una acción aleatoria\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las métricas\n",
    "mean_reward = np.mean(rewards)\n",
    "std_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensas: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar de recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvApsF05c2Ry"
   },
   "source": [
    "Como era de esperar, el rendimiento de esta política es muy bajo, ya que no toma decisiones informadas.\n",
    "\n",
    "Las recompensas obtenidas indican que la política aleatoria lleva a perder más partidas de las que se ganan (promedio negativo), y que se tiene una alta variabilidad en los resultados (desviación estándar alta), lo que es típico en un juego de azar como Blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "c43e24b5-be23-4a18-cf04-6e3ce6eeea09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mSe truncaron las últimas líneas 5000 del resultado de transmisión.\u001b[0m\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.252    |\n",
      "|    n_updates        | 1940     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5100     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7872     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 1942     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5104     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7876     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1943     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5108     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7883     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 1945     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5112     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7890     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.286    |\n",
      "|    n_updates        | 1947     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5116     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7895     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 1948     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5120     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7901     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 1950     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5124     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7909     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 1952     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5128     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7914     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 1953     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5132     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7918     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 1954     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5136     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 7924     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 1955     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5140     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7932     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 1957     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5144     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7940     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 1959     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5148     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7946     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 1961     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5152     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7956     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 1963     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5156     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7961     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1965     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5160     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7965     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 1966     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5164     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7972     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 1967     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5168     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 1969     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5172     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7984     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 1970     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5176     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7991     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 1972     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5180     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 7997     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5184     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8004     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1975     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5188     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8012     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.213    |\n",
      "|    n_updates        | 1977     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5192     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8018     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1979     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5196     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8026     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 1981     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5200     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8033     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 1983     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5204     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8041     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 1985     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5208     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8046     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.288    |\n",
      "|    n_updates        | 1986     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5212     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8052     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 1987     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5216     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8059     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 1989     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5220     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8066     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 1991     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5224     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8075     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 1993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5228     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8083     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.261    |\n",
      "|    n_updates        | 1995     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5232     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8088     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 1996     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5236     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8095     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.214    |\n",
      "|    n_updates        | 1998     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5240     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8100     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5244     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8105     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.211    |\n",
      "|    n_updates        | 2001     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5248     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8111     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5252     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8116     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5256     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 2005     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.11     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5260     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8131     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2007     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5264     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8136     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.25     |\n",
      "|    n_updates        | 2008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | 0.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5268     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8145     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2011     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5272     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.297    |\n",
      "|    n_updates        | 2012     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5276     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8155     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2013     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5280     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8161     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2015     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5284     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8168     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.195    |\n",
      "|    n_updates        | 2016     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5288     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8174     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2018     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5292     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8179     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2019     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5296     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8183     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5300     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8191     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2022     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5304     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8195     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2023     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5308     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8201     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.249    |\n",
      "|    n_updates        | 2025     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5312     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8209     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2027     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5316     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8215     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.226    |\n",
      "|    n_updates        | 2028     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5320     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5324     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8229     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.227    |\n",
      "|    n_updates        | 2032     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5328     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8234     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 2033     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5332     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8239     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.154    |\n",
      "|    n_updates        | 2034     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5336     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8244     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 2035     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5340     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8248     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.297    |\n",
      "|    n_updates        | 2036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5344     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 2038     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5348     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8264     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2040     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5352     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8271     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 2042     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5356     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 8275     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 2043     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5360     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 2045     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5364     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8289     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 2047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5368     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8296     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2048     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5372     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8303     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2050     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5376     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8311     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 2052     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5380     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8318     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5384     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8325     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2056     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5388     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8330     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2057     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5392     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8340     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2059     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5396     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8348     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 2061     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5400     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8356     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 2063     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5404     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8363     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 2065     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5408     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8372     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2067     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5412     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.104    |\n",
      "|    n_updates        | 2069     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5416     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8385     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.181    |\n",
      "|    n_updates        | 2071     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5420     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8391     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2072     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5424     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8398     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 2074     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5428     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8405     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.201    |\n",
      "|    n_updates        | 2076     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5432     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8411     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2077     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5436     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8418     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 2079     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.77     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5440     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8425     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 2081     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.75     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5444     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8431     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0823   |\n",
      "|    n_updates        | 2082     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5448     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2084     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5452     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8441     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.119    |\n",
      "|    n_updates        | 2085     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5456     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8449     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2087     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5460     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8456     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2088     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5464     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8462     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2090     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5468     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8469     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.206    |\n",
      "|    n_updates        | 2092     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5472     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8473     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.231    |\n",
      "|    n_updates        | 2093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5476     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8482     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.275    |\n",
      "|    n_updates        | 2095     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5480     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8486     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5484     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8492     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5488     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8496     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.267    |\n",
      "|    n_updates        | 2098     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5492     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8501     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.148    |\n",
      "|    n_updates        | 2100     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5496     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8507     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 2101     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | 0.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5500     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8513     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2103     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5504     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8519     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.345    |\n",
      "|    n_updates        | 2104     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5508     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8525     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 2106     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5512     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8532     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.133    |\n",
      "|    n_updates        | 2107     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5516     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8539     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2109     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5520     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8548     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2111     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5524     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8555     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 2113     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5528     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8563     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2115     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5532     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8570     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.224    |\n",
      "|    n_updates        | 2117     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5536     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8578     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2119     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5540     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8585     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2121     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5544     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8592     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 2122     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5548     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8596     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 2123     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5552     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8603     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 2125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5556     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8611     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.233    |\n",
      "|    n_updates        | 2127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5560     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8618     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5564     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8623     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 2130     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5568     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 8631     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.149    |\n",
      "|    n_updates        | 2132     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5572     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8639     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5576     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8643     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 2135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5580     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8648     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2136     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5584     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8654     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.161    |\n",
      "|    n_updates        | 2138     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5588     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8660     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 2139     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5592     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8665     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2141     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5596     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8672     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 2142     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5600     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8677     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 2144     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5604     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8684     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2145     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5608     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8691     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2147     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5612     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8695     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2148     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5616     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8700     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.349    |\n",
      "|    n_updates        | 2149     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5620     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8704     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2150     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5624     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8709     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2152     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5628     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8715     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 2153     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5632     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8719     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2154     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5636     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8725     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2156     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5640     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8729     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2157     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.42     |\n",
      "|    ep_rew_mean      | -0.24    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5644     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8734     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2158     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.28    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5648     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8740     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 2159     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5652     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8750     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 2162     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5656     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8754     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2163     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.42     |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5660     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8760     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2164     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.44     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5664     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8767     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2166     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5668     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8774     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.222    |\n",
      "|    n_updates        | 2168     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.43     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5672     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8782     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2170     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5676     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8789     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2172     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.38    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5680     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8798     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 2174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5684     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8804     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2175     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5688     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8810     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.168    |\n",
      "|    n_updates        | 2177     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5692     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8815     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 2178     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5696     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8823     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 2180     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5700     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8829     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2182     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5      |\n",
      "|    ep_rew_mean      | -0.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5704     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8834     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.194    |\n",
      "|    n_updates        | 2183     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.37    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5708     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8839     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.133    |\n",
      "|    n_updates        | 2184     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5712     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8846     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.193    |\n",
      "|    n_updates        | 2186     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5716     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2187     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.28    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5720     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8858     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2189     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.25    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5724     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8864     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2190     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5728     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8869     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2192     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5732     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8875     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 2193     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5736     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8881     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 2195     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5740     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8887     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 2196     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5744     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8893     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 2198     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5748     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8902     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.26     |\n",
      "|    n_updates        | 2200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5752     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8908     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2201     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5756     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8915     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2203     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5760     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8922     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2205     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5764     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8929     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.294    |\n",
      "|    n_updates        | 2207     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5768     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8936     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 2208     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5772     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8945     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 2211     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5776     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8952     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.276    |\n",
      "|    n_updates        | 2212     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5780     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8960     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 2214     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5784     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8966     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 2216     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5788     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8970     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.25     |\n",
      "|    n_updates        | 2217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5792     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 8975     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2218     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5796     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8986     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2221     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5800     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8990     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2222     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5804     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 8997     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5808     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9002     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2225     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5812     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9007     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.155    |\n",
      "|    n_updates        | 2226     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5816     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9011     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2227     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5820     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9016     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 2228     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5824     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9022     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.3      |\n",
      "|    n_updates        | 2230     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5828     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9028     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2231     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5832     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9033     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2233     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5836     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9040     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 2234     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5840     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9045     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 2236     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5844     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9050     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2237     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5848     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9058     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2239     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5852     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9067     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2241     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5856     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9072     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.141    |\n",
      "|    n_updates        | 2242     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5860     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9076     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2243     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5864     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9081     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.237    |\n",
      "|    n_updates        | 2245     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5868     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9088     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.239    |\n",
      "|    n_updates        | 2246     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5872     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9093     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 2248     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5876     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9101     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2250     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5880     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9106     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 2251     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5884     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9112     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.207    |\n",
      "|    n_updates        | 2252     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5888     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9119     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2254     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5892     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2255     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.46     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5896     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9132     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.247    |\n",
      "|    n_updates        | 2257     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.49     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5900     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9139     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5904     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9145     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 2261     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5908     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2262     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.47     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5912     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9154     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2263     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5916     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9164     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2265     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5920     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9169     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 2267     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5924     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9176     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 2268     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5928     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9183     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.238    |\n",
      "|    n_updates        | 2270     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5932     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9187     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2271     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5936     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9191     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2272     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5940     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9198     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2274     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5944     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9204     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2275     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5948     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9214     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.209    |\n",
      "|    n_updates        | 2278     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5952     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9222     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 2280     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5956     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9229     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.107    |\n",
      "|    n_updates        | 2282     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5960     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9234     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.216    |\n",
      "|    n_updates        | 2283     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5964     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9239     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 2284     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5968     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9245     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2286     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5972     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9256     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 2288     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5976     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9264     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5980     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9272     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 2292     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5984     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9276     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 2293     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5988     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9283     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2295     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5992     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9290     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 2297     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5996     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9295     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2298     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6000     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9302     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2300     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6004     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9308     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2301     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6008     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9313     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2303     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6012     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9317     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2304     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6016     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 9326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.251    |\n",
      "|    n_updates        | 2306     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6020     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9333     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6024     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9341     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 2310     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6028     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9348     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2311     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6032     |\n",
      "|    fps              | 333      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9353     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 2313     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6036     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9358     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.213    |\n",
      "|    n_updates        | 2314     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.24    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6040     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9366     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.19     |\n",
      "|    n_updates        | 2316     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6044     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9374     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2318     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6048     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2319     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6052     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9386     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.207    |\n",
      "|    n_updates        | 2321     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6056     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9390     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.198    |\n",
      "|    n_updates        | 2322     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6060     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9398     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2324     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6064     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9402     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2325     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6068     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9407     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 2326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6072     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9412     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.24     |\n",
      "|    n_updates        | 2327     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6076     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9418     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 2329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6080     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9424     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 2330     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6084     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9432     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 2332     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6088     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9439     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2334     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6092     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9444     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 2335     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6096     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9454     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 2338     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6100     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9461     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 2340     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6104     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9468     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 2341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6108     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9473     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 2343     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6112     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9478     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.164    |\n",
      "|    n_updates        | 2344     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6116     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9487     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.178    |\n",
      "|    n_updates        | 2346     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6120     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9494     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 2348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6124     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9501     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 2350     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6128     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9507     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2351     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6132     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9513     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2353     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6136     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9520     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 2354     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6140     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9527     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2356     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6144     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9531     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2357     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6148     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9536     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 2358     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6152     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9549     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.22     |\n",
      "|    n_updates        | 2362     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6156     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9555     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.171    |\n",
      "|    n_updates        | 2363     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6160     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9563     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.231    |\n",
      "|    n_updates        | 2365     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6164     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9568     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0796   |\n",
      "|    n_updates        | 2366     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6168     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9573     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.15     |\n",
      "|    n_updates        | 2368     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6172     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9578     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.253    |\n",
      "|    n_updates        | 2369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.66     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6176     |\n",
      "|    fps              | 332      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9584     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.202    |\n",
      "|    n_updates        | 2370     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6180     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9588     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 2371     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6184     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9594     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 2373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6188     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9599     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6192     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9604     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.21     |\n",
      "|    n_updates        | 2375     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6196     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 9612     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.212    |\n",
      "|    n_updates        | 2377     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6200     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9619     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.251    |\n",
      "|    n_updates        | 2379     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6204     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9624     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 2380     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | 0.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6208     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9631     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 2382     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | 0.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6212     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9641     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2385     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | 0.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6216     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9648     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.152    |\n",
      "|    n_updates        | 2386     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6220     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9654     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 2388     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6224     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9660     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0623   |\n",
      "|    n_updates        | 2389     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59     |\n",
      "|    ep_rew_mean      | 0.02     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6228     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9666     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 2391     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | 0.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6232     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9678     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 2394     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6236     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9685     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.159    |\n",
      "|    n_updates        | 2396     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6240     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9689     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2397     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6244     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9698     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 2399     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6248     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9705     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2401     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.63     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6252     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9712     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2402     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6256     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9717     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.143    |\n",
      "|    n_updates        | 2404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6260     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9723     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.229    |\n",
      "|    n_updates        | 2405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6264     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9728     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.244    |\n",
      "|    n_updates        | 2406     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6268     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9734     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 2408     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6272     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9738     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.175    |\n",
      "|    n_updates        | 2409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6276     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9746     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 2411     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6280     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9753     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 2413     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6284     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9761     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.136    |\n",
      "|    n_updates        | 2415     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6288     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9769     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.182    |\n",
      "|    n_updates        | 2417     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6292     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9775     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 2418     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6296     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9782     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.265    |\n",
      "|    n_updates        | 2420     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6300     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.225    |\n",
      "|    n_updates        | 2422     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6304     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9798     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2424     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6308     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9804     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 2425     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6312     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9809     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.191    |\n",
      "|    n_updates        | 2427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6316     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9819     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2429     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6320     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9825     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.2      |\n",
      "|    n_updates        | 2431     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6324     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9832     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.173    |\n",
      "|    n_updates        | 2432     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6328     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9837     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 2434     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6332     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9845     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.234    |\n",
      "|    n_updates        | 2436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6336     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9852     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.158    |\n",
      "|    n_updates        | 2437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6340     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9861     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2440     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6344     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9868     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 2441     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6348     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9873     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.262    |\n",
      "|    n_updates        | 2443     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69     |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6352     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9881     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.192    |\n",
      "|    n_updates        | 2445     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6356     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9889     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.163    |\n",
      "|    n_updates        | 2447     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6360     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9894     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 2448     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | 0        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6364     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9901     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.187    |\n",
      "|    n_updates        | 2450     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73     |\n",
      "|    ep_rew_mean      | -0.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6368     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9907     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 2451     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6372     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9912     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 2452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74     |\n",
      "|    ep_rew_mean      | -0.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6376     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9920     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 2454     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71     |\n",
      "|    ep_rew_mean      | -0.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6380     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9924     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2455     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6384     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 9933     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 2458     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6388     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9937     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.183    |\n",
      "|    n_updates        | 2459     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7      |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6392     |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9945     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 2461     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68     |\n",
      "|    ep_rew_mean      | -0.09    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6396     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9950     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.217    |\n",
      "|    n_updates        | 2462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6400     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9957     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 2464     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6404     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9963     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2465     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6408     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9969     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.278    |\n",
      "|    n_updates        | 2467     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.64     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6412     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9973     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.215    |\n",
      "|    n_updates        | 2468     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6416     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.17     |\n",
      "|    n_updates        | 2469     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6420     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9981     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 2470     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6424     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9988     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.184    |\n",
      "|    n_updates        | 2471     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6428     |\n",
      "|    fps              | 330      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 9998     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.169    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x789c8260f220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Se utilizará el algoritmo DQN (Deep Q-Network) por su efectividad para resolver\n",
    "problemas con espacios de acción discretos como Blacjack (se utiliza una red neuronal\n",
    "para aproximar la función Q (que estima el valor esperado de cada acción en un\n",
    "estado dado)). Además, es menos complejo que otros algoritmos, pero lo suficientemente\n",
    "avanzado como para mostrar una mejora significativa sobre el baseline.\n",
    "'''\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Se crea el modelo DQN\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=0.001, gamma=0.99, exploration_fraction=0.1, seed=42)\n",
    "\n",
    "# Se entrena el modelo\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "ab8cee3c-c066-415e-c171-d6883acbe4d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.08 ± 0.96\n"
     ]
    }
   ],
   "source": [
    "# Se evalúa el modelo\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sr1DxJhiOIe"
   },
   "source": [
    "El modelo entrenado tiene un promedio de recompensas significativamente menos negativo (-0.08) que el baseline (-0.38). Esto indica que el modelo entrenado pierde menos partidas en promedio y se acerca a un rendimiento neutral. Por su parte, la desviación estándar del modelo entrenado (0.96) es ligeramente mayor que la del baseline (0.90). Esto sugiere que hay una mayor variabilidad en las recompensas obtenidas por el modelo entrenado. Esto podría deberse a la naturaleza estocástica del ambiente o a que el modelo todavía no ha alcanzado una política completamente óptima.\n",
    "\n",
    "De esta forma, el modelo entrenado tiene un desempeño claramente mejor que el baseline, ya que logra reducir significativamente las pérdidas promedio.\n",
    "Aunque no alcanza un rendimiento positivo, está demostrando que puede aprender estrategias más efectivas que las acciones completamente aleatorias. De todas formas, el modelo podría beneficiarse de aumentar su entrenamiento (mayor `total_timesteps`); optimizar los hiperparámetros: `learning_rate` (tasa de aprendizaje para el optimizador), `gamma` (factor de descuento para recompensas futuras), o `exploration_fraction` (fracción de tiempo donde el agente realiza exploración en lugar de explotación) para que el modelo converja mejor; o directamente comparar este resultado con otros algoritmos de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAcXqiw3knfb",
    "outputId": "cf0f51c6-38c8-4921-8733-1862a05b0e6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial: [12  6  1]\n",
      "Tipo de estado: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Se inspecciona el formato de un estado\n",
    "initial_state, _ = env.reset()\n",
    "print(f\"Estado inicial: {initial_state}\")\n",
    "print(f\"Tipo de estado: {type(initial_state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpeC96SjlENy"
   },
   "source": [
    "Los estados pertenecen a la clase `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J57u0bw0k57T",
    "outputId": "226f6b38-d88f-4da4-ad89-8bed860ae0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escenario 1: Estado = (6, 7, 0), Acción = Pedir carta (Hit)\n",
      "Escenario 2: Estado = (19, 3, 1), Acción = Detenerse (Stick)\n"
     ]
    }
   ],
   "source": [
    "def get_agent_action(model, state):\n",
    "    \"\"\"\n",
    "    Obtiene la acción del agente para un estado dado.\n",
    "\n",
    "    Args:\n",
    "    - model: Modelo entrenado (DQN).\n",
    "    - state: Estado del ambiente (tuple o array).\n",
    "\n",
    "    Returns:\n",
    "    - Acción seleccionada por el modelo (int).\n",
    "    \"\"\"\n",
    "    # Se convierte el estado al formato esperado por el modelo (array)\n",
    "    if isinstance(state, tuple):\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "    # Se selecciona la acción usando el modelo\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    return action\n",
    "\n",
    "# Escenarios propuestos\n",
    "scenarios = [\n",
    "    (6, 7, 0),  # Suma de cartas = 6, Dealer muestra 7, sin as\n",
    "    (19, 3, 1)  # Suma de cartas = 19, Dealer muestra 3, con as\n",
    "]\n",
    "\n",
    "# Se evalúan las acciones del agente en cada escenario\n",
    "for i, state in enumerate(scenarios):\n",
    "    action = get_agent_action(model, state)\n",
    "    action_str = \"Pedir carta (Hit)\" if action == 1 else \"Detenerse (Stick)\"\n",
    "    print(f\"Escenario {i+1}: Estado = {state}, Acción = {action_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jOSWgeblOoJ"
   },
   "source": [
    "Dado el estado del escenario 1, lo esperado es que el agente pida una carta (Hit), ya que tiene pocas probabilidades de superar 21. Por otro lado, para el escenario 2, lo esperado es que el agente se detenga (Stick), ya que tiene una suma segura frente a la carta del dealer y pedir una carta sería muy arriesgado. Como se puede ver en \"print\", el agente toma acciones coherentes con las reglas del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS3EILBXZ2pa"
   },
   "source": [
    "En primer lugar, se destaca que especificar el parámetro `continuous = True` implica que el espacio de acciones del ambiente cambia de ser discreto a continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "El ambiente LunarLander consiste en un problema clásico de optimización de trayectoria donde el objetivo es aterrizar una nave espacial en una plataforma situada en el centro de un mapa 2D.\n",
    "\n",
    "1. Estados: \\\\\n",
    "  El espacio de observaciones es un vector continuo de 8 dimensiones (Box), que describe:\n",
    "\n",
    "    - Posición de la nave: coordenadas $x$ e $y$\n",
    "    - Velocidades lineales: $v_x$ y $v_y$\n",
    "    - Ángulo de orientación ($\\theta$)\n",
    "    - Velocidad angular ($\\omega$)\n",
    "    - Contacto de las patas de la nave con el suelo (2 valores booleanos).\n",
    "\n",
    "  Un ejemplo de estado se puede ver a continuación: $[x, y, v_x, v_y, θ, ω, leg1_{contact}, leg2_{contact}]$\n",
    "\n",
    "2. Acciones: \\\\\n",
    "  En modo discreto:\n",
    "    - 0: Hacer nada.\n",
    "    - 1: Encender el motor izquierdo.\n",
    "    - 2: Encender el motor principal.\n",
    "    - 3: Encender el motor derecho.\n",
    "\n",
    "  En modo continuo (el que se tiene al haber definido `continuous = True`):\n",
    "  un vector de dos elementos:\n",
    "    - $a[0]$: Potencia del motor principal (-1 a 1)\n",
    "    - $a[1]$: Potencia de los propulsores laterales (-1 a 1).\n",
    "\n",
    "3. Recompensas: \\\\\n",
    "  Las recompensas están diseñadas para incentivar un aterrizaje seguro:\n",
    "\n",
    "  - Proximidad al objetivo y velocidad reducida: recompensa positiva.\n",
    "  - Contacto de las patas con el suelo: +10 puntos por pata.\n",
    "  - Movimiento en exceso, inclinación o uso de combustible: penalización.\n",
    "  - Aterrizaje seguro: +100 puntos.\n",
    "  - Colisión: -100 puntos.\n",
    "  - El episodio se considera resuelto si el puntaje total es $\\geq 200$.\n",
    "\n",
    "4. Condiciones de finalización \\\\\n",
    "Un episodio termina si:\n",
    "\n",
    "  - La nave colisiona con la superficie lunar.\n",
    "  - La nave sale de los límites del mapa.\n",
    "  - La nave queda inmóvil.\n",
    "\n",
    "Se destaca que en LunarLander con `continuous=True`, las acciones son vectores continuos que permiten mayor control con respecto al caso de Blackjack, donde las acciones son simplemente binarias: \"hit\" (pedir carta) o \"stick\" (detenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "5a6ff05b-95c4-4eb0-f8e0-e4da06ddef60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -243.93\n",
      "Desviación estándar de las recompensas: 108.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lista para almacenar las recompensas de cada episodio\n",
    "episodic_rewards = []\n",
    "\n",
    "# Número de episodios\n",
    "num_episodes = 10\n",
    "\n",
    "# Se simula el ambiente con acciones aleatorias\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Se selecciona una acción aleatoria dentro del espacio de acción continuo\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(random_action)\n",
    "        total_reward += reward\n",
    "\n",
    "    episodic_rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las métricas\n",
    "mean_reward = np.mean(episodic_rewards)\n",
    "std_reward = np.std(episodic_rewards)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lbxtl5he0NN"
   },
   "source": [
    "El performance de esta política es bajo (`recompensa promedio: -243.93`), ya que no sigue ninguna estrategia para optimizar la posición, velocidad o el uso de motores, lo que tiende a resultar en posibles colisiones o aterrizajes incorrectos. Por su parte, por el carácter aleatorio de las acciones, los resultados de cada episodio pueden tener grandes variaciones, lo que se aprecia en el alto valor de desviación estándar obtenido (`108.96`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "8b1dee46-260e-438d-af67-336a1778133e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 653      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -232        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595261 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 509         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.6e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -242        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006132064 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 564         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | -243        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004289039 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 876         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | -235         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 429          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899783 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.00131     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 521          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 954          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Se utilizará el algoritmo PPO (Proximal Policy Optimization) por su efectividad\n",
    "y estabilidad para resolver problemas con espacios de acción continuos como el\n",
    "LunarLander en su configuración continuous=True, al modelar distribuciones\n",
    "gaussianas para las acciones.\n",
    "'''\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Se guarda el modelo entrenado\n",
    "model.save(\"ppo_lunarlander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "87b7653d-9b5f-49ff-a536-36a71c93c295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -142.87\n",
      "Desviación estándar de las recompensas: 96.47\n"
     ]
    }
   ],
   "source": [
    "# Se carga el modelo entrenado\n",
    "model = PPO.load(\"ppo_lunarlander\")\n",
    "\n",
    "# Lista para almacenar las recompensas de cada episodio\n",
    "episodic_rewards = []\n",
    "\n",
    "# Número de episodios para la evaluación\n",
    "num_episodes = 10\n",
    "\n",
    "# Se evalúa el modelo\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Acción del modelo entrenado\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    episodic_rewards.append(total_reward)\n",
    "\n",
    "# Se calculan las métricas\n",
    "mean_reward = np.mean(episodic_rewards)\n",
    "std_reward = np.std(episodic_rewards)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward:.2f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er8RpRGYj5oy"
   },
   "source": [
    "El modelo entrenado tiene un promedio de recompensa significativamente menos negativo (-142.87) que el baseline (-243.93). Esto indica que el modelo entrenado ha aprendido a realizar acciones más orientadas hacia el objetivo, como acercarse al área de aterrizaje y mantener la nave en buenas condiciones. Por su parte, la desviación estándar del modelo entrenado (96.47) es ligeramente menor que la del baseline (108.96), lo que sugiere un comportamiento más consistente en las políticas aprendidas, aunque todavía con cierta variabilidad presente.\n",
    "\n",
    "De esta forma, el modelo entrenado tiene un desempeño claramente mejor que el baseline, ya que aprendió a evitar comportamientos altamente penalizados, es decir, una estrategia más efectiva que efectuar acciones completamente aleatorias. Ahora bien, es importante destacar que todavía no alcanza un desempeño que resuelva completamente el problema. De igual forma que antes, el modelo podría beneficiarse de aumentar su entrenamiento (mayor `total_timesteps`); optimizar los hiperparámetros: `learning_rate` (tasa de aprendizaje para el optimizador) o `gamma` (factor de descuento para recompensas futuras) para que el modelo converja mejor; o directamente comparar este resultado con otros algoritmos de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsVOtLKYnkks"
   },
   "outputs": [],
   "source": [
    "# Stable-Baselines3 espera trabajar con un entorno envuelto en un VecEnv\n",
    "# (un vector de entornos para manejar múltiples episodios en paralelo)\n",
    "\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "def evaluate_model(env, model, n_episodes=10):\n",
    "    rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    print(f\"Recompensa promedio: {avg_reward:.2f}\")\n",
    "    print(f\"Desviación estándar de las recompensas: {std_reward:.2f}\")\n",
    "    return avg_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vsvq177oKS0"
   },
   "source": [
    "##### learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYj2HvXzpIf4"
   },
   "source": [
    "Se disminuye la tasa de aprendizaje para estabilizar el entrenamiento en un ambiente complejo como el de LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SfNSbj8oTYV",
    "outputId": "61de5afb-8e1a-4288-972a-6acc46ed852e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 880      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -223         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027952273 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 669          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.78e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 115          |\n",
      "|    ep_rew_mean          | -213         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027620955 |\n",
      "|    clip_fraction        | 0.00732      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 537          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00413     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.46e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -218        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 551         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001897949 |\n",
      "|    clip_fraction        | 0.000537    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0229     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 500         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -219        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004079805 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.00881    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 635         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.44e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc502f59b70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_learning_rate = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0001, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model_learning_rate.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb7AmmMdqfxp",
    "outputId": "d2a21a7e-fec9-4321-8ab8-48a97f6a9f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -153.18\n",
      "Desviación estándar de las recompensas: 89.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-153.1816, 89.43967)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del modelo optimizado\n",
    "evaluate_model(vec_env, model_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wScMyWZioT7M"
   },
   "source": [
    "##### batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rtodC3XpQ34"
   },
   "source": [
    "Se aumenta el tamaño del batch dado el ambiente altamente complejo y continuo que es LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzDzVnBcoWNP",
    "outputId": "cc742f2e-0309-4d96-d5c9-153b4386b8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 916      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -229         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 763          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038873914 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 885          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.81e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 123         |\n",
      "|    ep_rew_mean          | -230        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 677         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003605734 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00959    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 602         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.5e+03     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 124          |\n",
      "|    ep_rew_mean          | -221         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 688          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014260923 |\n",
      "|    clip_fraction        | 0.00161      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.011       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 694          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.49e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 126          |\n",
      "|    ep_rew_mean          | -218         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 690          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036488809 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0357      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 818          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00468     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.59e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc502f495d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_batch_size = PPO(\"MlpPolicy\", env, verbose=1, batch_size=256, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 10000 timesteps\n",
    "model_batch_size.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcYqiLN8qgTl",
    "outputId": "95e4ac18-ce40-44de-ad86-98da2b9fafb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -149.01\n",
      "Desviación estándar de las recompensas: 91.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-149.0068, 91.97724)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del modelo optimizado\n",
    "evaluate_model(vec_env, model_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYhhMl_GoCtg"
   },
   "source": [
    "##### total_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8MK9biio8BA"
   },
   "source": [
    "Se decide aumentar el número de pasos de entrenamiento para que PPO tenga más tiempo para aprender las políticas complejas de LunarLander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aItYF6sr6F_6",
    "outputId": "b8036702-8bd9-4e52-c8bf-23283cf2eee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -214     |\n",
      "| time/              |          |\n",
      "|    fps             | 749      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -232        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595261 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.00124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 509         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.6e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -242        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006132064 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 564         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | -243        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004289039 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 876         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 138          |\n",
      "|    ep_rew_mean          | -235         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899783 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -0.00131     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 521          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 954          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 132         |\n",
      "|    ep_rew_mean          | -215        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007246358 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.83       |\n",
      "|    explained_variance   | -0.0232     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 219         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 647         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 132         |\n",
      "|    ep_rew_mean          | -199        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008402871 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.000615   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 481         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -176        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008425518 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.000404   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 284         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 140          |\n",
      "|    ep_rew_mean          | -161         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 401          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073442548 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -9.2e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 128          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00477     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 243          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003997383 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.00937     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 263         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 145          |\n",
      "|    ep_rew_mean          | -107         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 383          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045888405 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.0114       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.6         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 233          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004353313 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 130         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | -81.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 388          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030843988 |\n",
      "|    clip_fraction        | 0.00688      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 248          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 159          |\n",
      "|    ep_rew_mean          | -69.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 383          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065037026 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.342        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 171          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 173          |\n",
      "|    ep_rew_mean          | -64.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 364          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059796213 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.5         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00682     |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 138          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 188          |\n",
      "|    ep_rew_mean          | -66          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 358          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034857607 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.1         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 198          |\n",
      "|    ep_rew_mean          | -63.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 359          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043706214 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.74        |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.6         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 139          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 213          |\n",
      "|    ep_rew_mean          | -63.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 353          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067793094 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.2         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00588     |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | -61.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 340          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056233574 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.643        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.3         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0061      |\n",
      "|    std                  | 0.944        |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 248         |\n",
      "|    ep_rew_mean          | -58.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005022563 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    std                  | 0.934       |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 259         |\n",
      "|    ep_rew_mean          | -57.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 319         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008182257 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    std                  | 0.932       |\n",
      "|    value_loss           | 41.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 280         |\n",
      "|    ep_rew_mean          | -54.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005838968 |\n",
      "|    clip_fraction        | 0.0645      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 298          |\n",
      "|    ep_rew_mean          | -52.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 299          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074622035 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.82         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0059      |\n",
      "|    std                  | 0.915        |\n",
      "|    value_loss           | 32.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 307          |\n",
      "|    ep_rew_mean          | -51.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 294          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065849866 |\n",
      "|    clip_fraction        | 0.071        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.32         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00759     |\n",
      "|    std                  | 0.906        |\n",
      "|    value_loss           | 28.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 327          |\n",
      "|    ep_rew_mean          | -48.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 291          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069328765 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.88         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00549     |\n",
      "|    std                  | 0.909        |\n",
      "|    value_loss           | 12.9         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 346       |\n",
      "|    ep_rew_mean          | -45.3     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 289       |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 183       |\n",
      "|    total_timesteps      | 53248     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0070354 |\n",
      "|    clip_fraction        | 0.0679    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.64     |\n",
      "|    explained_variance   | 0.744     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 47.4      |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.00586  |\n",
      "|    std                  | 0.905     |\n",
      "|    value_loss           | 49.6      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 363          |\n",
      "|    ep_rew_mean          | -43.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 286          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067521264 |\n",
      "|    clip_fraction        | 0.0802       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00863     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 36.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 375         |\n",
      "|    ep_rew_mean          | -43         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 201         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005757274 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.62        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    std                  | 0.88        |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 391         |\n",
      "|    ep_rew_mean          | -39.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 209         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009191512 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.7        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 411          |\n",
      "|    ep_rew_mean          | -37.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 278          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075193793 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.54        |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 81.2         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00501     |\n",
      "|    std                  | 0.858        |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 427        |\n",
      "|    ep_rew_mean          | -35        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00861115 |\n",
      "|    clip_fraction        | 0.051      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.52      |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.61       |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0035    |\n",
      "|    std                  | 0.844      |\n",
      "|    value_loss           | 32.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 445         |\n",
      "|    ep_rew_mean          | -32.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 238         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010349799 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.9         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 462          |\n",
      "|    ep_rew_mean          | -31          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 271          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061270446 |\n",
      "|    clip_fraction        | 0.0736       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.39         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    std                  | 0.815        |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 480          |\n",
      "|    ep_rew_mean          | -29          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049967133 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.64         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.805        |\n",
      "|    value_loss           | 7.06         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 489         |\n",
      "|    ep_rew_mean          | -28.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 266         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005673277 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.71        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 8.9         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 507          |\n",
      "|    ep_rew_mean          | -26.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 280          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067978073 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.4         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    std                  | 0.801        |\n",
      "|    value_loss           | 5.15         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 524         |\n",
      "|    ep_rew_mean          | -24.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009767488 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 538         |\n",
      "|    ep_rew_mean          | -19.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007390581 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    std                  | 0.787       |\n",
      "|    value_loss           | 9.57        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 554          |\n",
      "|    ep_rew_mean          | -16.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 258          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045356723 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.36        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.791        |\n",
      "|    value_loss           | 5.73         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 574          |\n",
      "|    ep_rew_mean          | -12.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 258          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 317          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057009123 |\n",
      "|    clip_fraction        | 0.0705       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.89         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    std                  | 0.778        |\n",
      "|    value_loss           | 4.34         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 591          |\n",
      "|    ep_rew_mean          | -9.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 325          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044671623 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.3         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000929    |\n",
      "|    std                  | 0.777        |\n",
      "|    value_loss           | 45.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 608          |\n",
      "|    ep_rew_mean          | -6.41        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072612492 |\n",
      "|    clip_fraction        | 0.0809       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    std                  | 0.775        |\n",
      "|    value_loss           | 4.97         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 626         |\n",
      "|    ep_rew_mean          | -3.93       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009841194 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.32       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 646         |\n",
      "|    ep_rew_mean          | 1.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 353         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005601212 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 5.81        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 663        |\n",
      "|    ep_rew_mean          | 3.06       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 253        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932139 |\n",
      "|    clip_fraction        | 0.0776     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.27      |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.77       |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.00408   |\n",
      "|    std                  | 0.75       |\n",
      "|    value_loss           | 29.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 670         |\n",
      "|    ep_rew_mean          | 8.89        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 373         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009699607 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000668   |\n",
      "|    std                  | 0.742       |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 687         |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005176369 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 58.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 706         |\n",
      "|    ep_rew_mean          | 14          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006862046 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 5.77        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 727          |\n",
      "|    ep_rew_mean          | 19.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 400          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112000285 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.92         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00526     |\n",
      "|    std                  | 0.728        |\n",
      "|    value_loss           | 41.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc50416ef50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Se crea el modelo PPO\n",
    "model_timesteps100000 = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0003, gamma=0.99, seed=42)\n",
    "\n",
    "# Se entrena el modelo con 100000 timesteps\n",
    "model_timesteps100000.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAU5BGBTqaCA",
    "outputId": "6a899529-6a27-48a6-d125-5b612005c0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: 99.12\n",
      "Desviación estándar de las recompensas: 38.09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99.12029, 38.09349)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluación del modelo optimizado\n",
    "evaluate_model(vec_env, model_timesteps100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCuky96rtpCD"
   },
   "outputs": [],
   "source": [
    "export_gif(model_timesteps100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fcB7AShuR22"
   },
   "source": [
    "![Comportamiento del agente entrenado](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwHduRpBu1Zw"
   },
   "source": [
    "Como se puede observar en el gif, la nave tiene, en general, un comportamiento más refinado al ajustar su orientación y acelerar de manera controlada para aterrizar en la plataforma. Esto, además, se aprecia en el nivel de recompensa promedio obtenido, que cumple con ser mayor a 50, mostrando que el agente aprende una política eficiente.\n",
    "\n",
    "Se destaca la relevancia de aumentar el número de pasos de entrenamiento para generar un modelo con mejor desempeño, por sobre realizar variaciones en el resto de hiperparámetros vistos. Sin embargo, también se menciona el aumento en el tiempo de entrenamiento, pasando de 15-20 segundos a 6 minutos, aproximadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ud2Xm_k-hFJn",
    "outputId": "ed6ccbe4-48df-45f3-9278-203c83fa4a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google AI API key: ··········\n",
      "Enter your Tavily API key: ··········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "e9ef4e6a-4438-42d1-9a03-15fcd6fe1b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEd7CKyV2Vhp",
    "outputId": "ac663375-d2f0-44cd-863e-5e8168781562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargado y renombrado: Curso-Estadistica.pdf\n",
      "Descargado y renombrado: Curso-Aprendizaje-de-Maquinas.pdf\n",
      "Total de páginas: 274\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "# Links a los documentos en GitHub\n",
    "github_links = [\n",
    "    \"https://github.com/GAMES-UChile/Curso-Estadistica/raw/master/notas_de_clase.pdf\",\n",
    "    \"https://github.com/GAMES-UChile/Curso-Aprendizaje-de-Maquinas/raw/master/notas_de_clase.pdf\"\n",
    "]\n",
    "\n",
    "# Paths locales como alternativa\n",
    "local_doc_paths = [\n",
    "    \"Notas_de_clase_Aprendizaje_de_Maquinas.pdf\",\n",
    "    \"Notas_de_clase_Estadistica.pdf\"\n",
    "]\n",
    "\n",
    "# Lista donde se almacenarán los paths a los documentos accesibles\n",
    "doc_paths = []\n",
    "\n",
    "# Se intentan descargar los archivos desde GitHub en primer lugar\n",
    "for url in github_links:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Se extrae el identificador único del URL\n",
    "            identifier = url.split(\"/\")[-4]  # Toma el segmento del curso (ejemplo: 'Curso-Estadistica') para diferenciar pdf's\n",
    "            filename = f\"{identifier}.pdf\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            doc_paths.append(filename)\n",
    "            print(f\"Descargado y renombrado: {filename}\")\n",
    "        else:\n",
    "            print(f\"No se pudo descargar desde GitHub: {url} (Status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar {url}: {e}\")\n",
    "\n",
    "# Si no se pudieron descargar desde GitHub, se usan los archivos locales\n",
    "# (se adjuntarán en la tarea entregada por UCursos, mas no en el repositorio).\n",
    "if len(doc_paths) < 2:\n",
    "    print(\"Intentando usar archivos locales...\")\n",
    "    for local_path in local_doc_paths:\n",
    "        if os.path.exists(local_path):\n",
    "            doc_paths.append(local_path)\n",
    "            print(f\"Archivo local encontrado: {local_path}\")\n",
    "        else:\n",
    "            print(f\"Archivo local no encontrado: {local_path}\")\n",
    "\n",
    "# Se verifica que se tienen al menos 2 documentos\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "# Se verifica que el total de páginas es al menos 50\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\"\n",
    "\n",
    "print(f\"Total de páginas: {total_paginas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iI5kYfBSGkzy",
    "outputId": "5526561e-6814-452a-d96d-8d729a61d2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8kZ5h3FBPxL",
    "outputId": "b1d04dda-6f3c-4bce-803b-789e59244c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-yXAdCSn4JM",
    "outputId": "920656a3-a60f-4a3d-8bf4-3109395c1a51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/colab/html/_background_server.py:103: DeprecationWarning: make_current is deprecated; start the event loop first\n",
      "  ioloop.make_current()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrae texto de todas las páginas de un archivo PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def create_documents_from_pdfs(doc_paths):\n",
    "    \"\"\"Convierte PDFs a objetos Document para procesar con LangChain.\"\"\"\n",
    "    documents = []\n",
    "    for doc_path in doc_paths:\n",
    "        text = extract_text_from_pdf(doc_path)\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": doc_path}))\n",
    "    return documents\n",
    "\n",
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorización y almacenamiento\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNDKi098BcqL",
    "outputId": "f2d4439f-5c8c-429e-d302-7fa152fe3f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 1362\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generación de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # Número máximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_UiEn1hoZYR",
    "outputId": "6bd6a7f1-eb72-4382-ba91-8679f09fb9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es un estimador de máxima verosimilitud?\n",
      "Respuesta correcta: Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Según el texto proporcionado, el estimador de máxima verosimilitud (EMV) es una función de los datos que busca entregar un valor cercano a un parámetro desconocido.  Se basa en la función de verosimilitud, que representa la probabilidad de observar los datos dados un valor específico del parámetro.  El EMV se define como el valor del parámetro que maximiza la función de verosimilitud (o una función monótonamente creciente de esta).  En otras palabras, el EMV es el valor del parámetro que hace más probable la observación de los datos.\n",
      "\n",
      "Pregunta: ¿Qué es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) considera datos en forma de pares (dato, etiqueta) y el objetivo es estimar una función f(x) tal que se tiene la siguiente igualdad.\n",
      "\n",
      "Pregunta: ¿Qué es un modelo de regresión lineal?\n",
      "Respuesta correcta: Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: En el contexto de un conjunto de entrenamiento  D que contiene N observaciones de entrada y salida,  fxᵢgᴺᵢ₌₁ y fyᵢgᴺᵢ₌₁,  la regresión lineal busca encontrar un modelo lineal, es decir, una función f definida por f:ℝᵐ→ℝ, x↦f(x) = aᵀx + b; a∈ℝᵐ; b∈ℝ.  En otras palabras, busca una relación lineal entre las variables dependientes e independientes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¿Qué es un estimador de máxima verosimilitud?\", \"Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\"),\n",
    "    (\"¿Qué es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¿Qué es un modelo de regresión lineal?\", \"Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la solución RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPb91-ksN7HT"
   },
   "source": [
    "En general, las respuestas entregadas por la solución RAG son buenas. En todas ellas se explaya más de lo que se aprecia en la tupla correcta, lo que tiene sentido ya que está sacando la información de notas de clase de un curso con alto enfoque matemático.\n",
    "\n",
    "Ahora bien, se destaca que la segunda respuesta pareciera no concluír del todo, lo que puede deberse a varias razones: por un lado, el alto volumen de datos y/o complejidad de la pregunta podría estar generando un corte prematuro debido a limitaciones de tiempo; por otro, el valor de `temperature` definido es 0, lo que significa que el modelo está generando respuestas determinísticas, es decir, más directas y menos variadas, lo que implica que quizás no está explorando de manera apropiada para completar las respuestas de mejor forma; y por último, que el modelo esté tratando de generar una respuesta a partir de fragmentos que no contienen toda la información necesaria, o directamente presentan problemas al hacer la recuperación de los documentos, generando un atasque en el modelo y, así, una respuesta incompleta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Fx_1Y2Z0vV"
   },
   "source": [
    "Para comparar desempeños se obtuvieron los parámetros por defecto (ejecutados en la iteración anterior) del siguiente [link](https://python.langchain.com/api_reference/_modules/langchain_core/vectorstores/base.html#VectorStore.as_retriever).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrDKSEYkTRZM"
   },
   "source": [
    "##### Cantidad de chunks recuperados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li7XOksKTYtU"
   },
   "source": [
    "El número de chunks que recupera el modelo depende de la cantidad de fragmentos que el recuperador de documentos (FAISS) retorna durante la búsqueda (por defecto son 4). El recuperar demasiados chunks puede generar que el modelo se sobrecargue y no logre identificar los fragmentos más relevantes. Por otra parte, si se recuperan pocos chunks, puede que no tenga suficiente información para responder adecuadamente a la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1cDHHvDTToW",
    "outputId": "b54ba808-bdba-48b9-dd0a-dbdedf9f7043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es un estimador de máxima verosimilitud?\n",
      "Respuesta correcta: Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un estimador de máxima verosimilitud (EMV) es una función de los datos que busca encontrar el valor de un parámetro desconocido que maximiza la probabilidad de haber observado los datos.  En otras palabras, se busca el valor del parámetro que hace más probable la muestra obtenida.  Se puede definir con respecto a la función de verosimilitud o a cualquier función no decreciente de esta.  A menudo, se maximiza la log-verosimilitud en lugar de la verosimilitud para facilitar los cálculos.\n",
      "\n",
      "Pregunta: ¿Qué es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) es un tipo de aprendizaje automático que utiliza datos etiquetados, es decir, pares (dato, etiqueta), para entrenar un modelo que pueda predecir la etiqueta de nuevos datos.  El objetivo es estimar una función f(x) tal que etiqueta = f(dato).  Ejemplos incluyen la identificación de spam en correos electrónicos (clasificación) o la estimación del precio de una propiedad (regresión).\n",
      "\n",
      "Pregunta: ¿Qué es un modelo de regresión lineal?\n",
      "Respuesta correcta: Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un modelo de regresión lineal busca encontrar una relación lineal entre una variable dependiente (salida, respuesta o etiqueta; usualmente denotada y) y una o más variables independientes (estímulo o característica; usualmente denotada x).  En esencia, intenta modelar cómo cambia la variable dependiente cuando se modifica la variable(s) independiente(s) mediante una función lineal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generación de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # Número máximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 16}),  # Se recuperan 16 chunks\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¿Qué es un estimador de máxima verosimilitud?\", \"Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\"),\n",
    "    (\"¿Qué es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¿Qué es un modelo de regresión lineal?\", \"Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la solución RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR4jX58UdRTM"
   },
   "source": [
    "Como se puede ver, en general las respuestas son más completas y precisas con respecto a las que se tenían en la sección anterior. Además, ahora la segunda respuesta si logró terminar de generarse, lo que permite comprobar, al menos en parte, una de las razones dadas previamente de porqué no había concluído. Se destaca que el modelo no se sobrecargó en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVIk5t3wTT2x"
   },
   "source": [
    "##### Tipo de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrqcO7_qT_qT"
   },
   "source": [
    "El tipo de búsqueda se refiere al algoritmo utilizado para recuperar los documentos más relevantes. Por un lado, el método `similarity` (el que está definido por defecto) busca y devuelve los k documentos más similares en función de la distancia en el espacio de embeddings (como la distancia coseno o euclidiana). Por otro, el método `mmr` (Maximal Marginal Relevance) equilibra la relevancia y la diversidad de los documentos recuperados, lo que es útil si se quiere evitar que se devuelvan múltiples chunks con contenido muy similar. Por último, el método `similarity_score_threshold` solo devuelve documentos cuya puntuación de similitud supere un umbral definido por el parámetro score_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdSjDdgETWwU",
    "outputId": "3a7ce802-3c11-4285-93d4-2f2525859d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es un estimador de máxima verosimilitud?\n",
      "Respuesta correcta: Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un estimador de máxima verosimilitud (EMV) es un método para estimar los parámetros de una distribución de probabilidad dada una muestra de datos.  En esencia, busca los valores de los parámetros que maximizan la probabilidad de observar los datos que se han obtenido.  Es decir, encuentra los parámetros que hacen que los datos observados sean lo más probables posible.\n",
      "\n",
      "Pregunta: ¿Qué es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado es un tipo de aprendizaje automático donde un algoritmo aprende de un conjunto de datos etiquetados.  Esto significa que cada dato en el conjunto de entrenamiento está asociado con una etiqueta o salida correcta.  El algoritmo aprende a mapear las entradas a las salidas correctas, y luego puede usar este conocimiento para predecir las salidas de nuevas entradas que no ha visto antes.\n",
      "\n",
      "Pregunta: ¿Qué es un modelo de regresión lineal?\n",
      "Respuesta correcta: Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Un modelo de regresión lineal es una herramienta estadística que se utiliza para modelar la relación entre una variable dependiente (la que se quiere predecir) y una o más variables independientes (las que se utilizan para predecir).  El modelo asume que esta relación es lineal, es decir, que se puede representar mediante una línea recta (o un hiperplano en el caso de múltiples variables independientes).  La ecuación general de un modelo de regresión lineal simple (con una sola variable independiente) es:\n",
      "\n",
      "Y = β₀ + β₁X + ε\n",
      "\n",
      "donde:\n",
      "\n",
      "* Y es la variable dependiente.\n",
      "* X es la variable independiente.\n",
      "* β₀ es la intersección (el valor de Y cuando X es 0).\n",
      "* β₁ es la pendiente (el cambio en Y por cada unidad de cambio en X).\n",
      "* ε es el término de error, que representa la variabilidad no explicada por el modelo.\n",
      "\n",
      "En el caso de una regresión lineal múltiple (con múltiples variables independientes), la ecuación se extiende para incluir más términos βᵢXᵢ, uno para cada variable independiente.  El objetivo del modelo es estimar los valores de β₀ y β₁, (y los βᵢ en el caso múltiple) que mejor ajusten la línea a los datos observados, minimizando el error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generación de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # Número máximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8}),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¿Qué es un estimador de máxima verosimilitud?\", \"Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\"),\n",
    "    (\"¿Qué es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¿Qué es un modelo de regresión lineal?\", \"Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la solución RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545U9equb8Ul"
   },
   "source": [
    "De las respuestas generadas se destaca la gran relevancia que tiene el nivel de exigencia con respecto al umbral de similitud permitido, sobre todo para preguntas que no son tan directas de responder al requerir un conocimiento más amplio. Además, se evidencia como ahora la última pregunta tuvo una respuesta mucho más completa (en un sentido matemático) y apropiada/entendible (la respuesta a esta pregunta en la sección anterior tenía una parte matemática difícil de entender en un primer y rápido análisis), lo que se entiende por el alto requisito mínimo de similitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtx17d_jS25Q"
   },
   "source": [
    "##### Tamaño del chunk (chunk_size y chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeYeZgclTAdo"
   },
   "source": [
    "El tamaño del chunk es el número de tokens que contiene cada fragmento de texto extraído de los documentos. Por una parte, el hecho de que los chunks sean más pequeños puede permitir que el modelo enfoque su atención en partes más específicas del texto, pero pueden perder contexto si el fragmento no cubre información clave. Por otro lado, los chunks más grandes pueden permitir una mejor cobertura de contexto, pero también pueden hacer que el modelo se enfoque en demasiada información a la vez, lo que podría resultar en respuestas más difusas o imprecisas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": [
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)  # Se disminuye chunk_size y chunk_overlap\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorización y almacenamiento\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSPjGZ0HjvxH",
    "outputId": "e0e0ab2b-aa5b-40c0-eb6c-dac2993cf948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 4127\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BorltMG8j3kh",
    "outputId": "76ff84ef-027c-4af8-f151-96c7e721d53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es un estimador de máxima verosimilitud?\n",
      "Respuesta correcta: Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Based on the provided text, a maximum likelihood estimator (EMV) is an estimator that finds an estimator based on how probable it is that the estimator generated the data.  The text also mentions that the maximum likelihood estimator is θMV=max{xi}n, but doesn't fully explain what that means in a broader context.\n",
      "\n",
      "Pregunta: ¿Qué es el aprendizaje supervisado?\n",
      "Respuesta correcta: El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: El aprendizaje supervisado (AS) utiliza datos en forma de pares (dato, etiqueta) con el objetivo de estimar una función f(x) tal que se cumple una igualdad.\n",
      "\n",
      "Pregunta: ¿Qué es un modelo de regresión lineal?\n",
      "Respuesta correcta: Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta generada por RAG: Based on the provided text, a simple linear regression model is defined as:  Yi = β0 + β1Xi + εi, where E(εi) = 0, and V(εi) = σ2.  The goal is to estimate β0 and β1 to find the best linear approximation.  A linear regression model seeks a linear function,  f(x) = a<sup>T</sup>x + b, where 'a' is a vector and 'b' is a scalar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generación de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # Número máximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Listado de preguntas y respuestas correctas (tuplas)\n",
    "qa_pairs = [\n",
    "    (\"¿Qué es un estimador de máxima verosimilitud?\", \"Un estimador de máxima verosimilitud es un método para estimar los parámetros de un modelo probabilístico, buscando los valores que hacen que los datos observados sean más probables bajo ese modelo.\"),\n",
    "    (\"¿Qué es el aprendizaje supervisado?\", \"El aprendizaje supervisado es un tipo de aprendizaje automático donde el modelo es entrenado con datos etiquetados para hacer predicciones o clasificaciones.\"),\n",
    "    (\"¿Qué es un modelo de regresión lineal?\", \"Un modelo de regresión lineal es un tipo de modelo estadístico que busca establecer una relación lineal entre una variable dependiente y una o más variables independientes.\")\n",
    "]\n",
    "\n",
    "# Se muestran las respuestas para cada pregunta\n",
    "for question, correct_answer in qa_pairs:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    print(f\"Respuesta correcta: {correct_answer}\")\n",
    "\n",
    "    # Se usa la solución RAG para obtener la respuesta\n",
    "    response = qa_chain.run(question)\n",
    "\n",
    "    print(f\"Respuesta generada por RAG: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj1hHx1zkhzV"
   },
   "source": [
    "Las respuestas generadas coinciden con lo mencionado sobre si se reducía el tamaño del chunk: en general, se pierde contexto cuando el fragmento no cuenta con información clave. El modelo no logra generar respuestas del todo satisfactorias, ya sea porque menciona que el texto no explica de manera completa ciertos aspectos; no entrega la información de manera completa o bien explicada; o directamente experimenta un cambio de idioma, probablemente porque no tenía en su contexto el que la pregunta era en español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Se configura la tool Tavily para devolver un máximo de 3 resultados por búsqueda\n",
    "tavily_tool = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlmreyrVrTwv",
    "outputId": "6546d92c-4259-4217-a47a-ccc6e7774537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=f29e7761509f0fc7b21d0b9656936848b7925a20ff553c7f5804c4200ab30b6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Se inicializa el wrapper de la API de Wikipedia\n",
    "wikipedia_api_wrapper = WikipediaAPIWrapper()\n",
    "\n",
    "# Se crea la tool\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pD1_n0wrsDI5",
    "outputId": "86371a09-fdbe-49e6-9f93-65c1a74c5445"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "# Prompt ReAct predefinido de LangChain\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools = [tavily_tool, wikipedia_tool]\n",
    "\n",
    "# Agente ReAct\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "# Se convierte el agente en un executor para usarlo\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pqo2dsxvywW_",
    "outputId": "f1ff4d31-d3f0-430f-c7c4-149184947318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: This question asks who wrote Don Quixote.  I should consult Wikipedia, as it's a well-known work of literature and Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Don Quixote\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Don Quixote\n",
      "Summary: Don Quixote, the full title being The Ingenious Gentleman Don Quixote of La Mancha, is a Spanish novel by Miguel de Cervantes. It was originally published in two parts, in 1605 and 1615. Considered a founding work of Western literature, it is often said to be the first modern novel. Don Quixote is also one of the most-translated books in the world and one of the best-selling novels of all time. \n",
      "The plot revolves around the adventures of a member of the lowest nobility, an hidalgo from La Mancha named Alonso Quijano, who reads so many chivalric romances that he loses his mind and decides to become a knight-errant (caballero andante) to revive chivalry and serve his nation, under the name Don Quixote de la Mancha. He recruits as his squire a simple farm labourer, Sancho Panza, who brings a unique, earthy wit to Don Quixote's lofty rhetoric. In the first part of the book, Don Quixote does not see the world for what it is and prefers to imagine that he is living out a knightly story meant for the annals of all time. However, as Salvador de Madariaga pointed out in his Guía del lector del Quijote (1972 [1926]), referring to \"the Sanchification of Don Quixote and the Quixotization of Sancho\", as \"Sancho's spirit ascends from reality to illusion, Don Quixote's declines from illusion to reality\".\n",
      "The book had a major influence on the literary community, as evidenced by direct references in Alexandre Dumas's The Three Musketeers (1844), and Edmond Rostand's Cyrano de Bergerac (1897) as well as the word quixotic. Mark Twain referred to the book as having \"swept the world's admiration for the mediaeval chivalry-silliness out of existence\". It has been described by some as the greatest work ever written.\n",
      "\n",
      "Page: Don Quixote (ballet)\n",
      "Summary: Don Quixote is a ballet in three acts, based on episodes taken from the famous novel Don Quixote de la Mancha by Miguel de Cervantes. It was originally choreographed by Marius Petipa to the music of Ludwig Minkus and first presented by Moscow's Bolshoi Ballet on 26 December [O.S. 14 December] 1869. Petipa and Minkus revised the ballet into a more elaborate and expansive version in five acts and eleven scenes for the Mariinsky Ballet, first presented on 21 November [O.S. 9 November] 1871 at the Imperial Bolshoi Kamenny Theatre of St. Petersburg.\n",
      "All modern productions of the Petipa/Minkus ballet are derived from the version staged by Alexander Gorsky for the Bolshoi Theatre of Moscow in 1900, a production the ballet master staged for the Imperial Ballet of St. Petersburg in 1902.\n",
      "\n",
      "Page: List of Don Quixote characters\n",
      "Summary: The following is a partial list of characters in the novel Don Quixote de la Mancha by Miguel de Cervantes Saavedra.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Miguel de Cervantes\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Miguel de Cervantes\n"
     ]
    }
   ],
   "source": [
    "# Pregunta que debería usar Wikipedia\n",
    "response = agent_executor.invoke({\"input\": \"¿Quién escribió Don Quijote de la Mancha?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfgXiQkosbmg",
    "outputId": "57620984-e62a-4a32-825b-f97f2ffb1e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer the question about the latest news on artificial intelligence, I need to use a search engine that provides current and reliable information.  Tavily_search_results_json seems best suited for this.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"latest news artificial intelligence\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://techcrunch.com/category/artificial-intelligence/', 'content': 'AI News & Artificial Intelligence | TechCrunch AI News & Artificial Intelligence | TechCrunch AI Site Search Toggle AI AI News coverage on artificial intelligence and machine learning tech, the companies building them, and the ethical issues AI raises today. AI AI AI AI AI AI AI Juna.ai wants to use AI agents to make factories more energy-efficient Norwegian startup Factiverse wants to fight disinformation with AI AI A popular technique to make AI more efficient has drawbacks AI AI AI AI Here’s the full list of 44 US AI startups that have raised $100M or more in 2024 OpenAI at one point considered acquiring AI chip startup Cerebras AI AI AI AI AI A popular technique to make AI more efficient has drawbacks'}, {'url': 'https://apnews.com/hub/artificial-intelligence', 'content': \"This week’s turmoil with ChatGPT-maker OpenAI has heightened trust concerns in the AI world\\nWhat you need to know about Emmett Shear, OpenAI’s new interim CEO\\nInsider Q&A: Pentagon AI chief on network-centric warfare, generative AI challenges\\n‘Please regulate AI:' Artists push for U.S. copyright reforms but tech industry says not so fast\\nBusiness Highlights: Advertisers flee X as Musk endorses antisemitic conspiracy; Open AI ditches CEO\\nChatGPT-maker OpenAI fires CEO Sam Altman, the face of the AI boom, for lack of candor with company\\nAmazon lays off hundreds in its Alexa division as it plows resources into AI\\nYouTube creators will soon have to disclose use of generative AI in videos or AWS chief Adam Selipsky talks generative AI, Amazon’s investment in Anthropic and cloud cost cutting\\nEurope reaches a deal on the world’s first comprehensive AI rules\\nNvidia CEO suggests Malaysia could be AI ‘manufacturing’ hub as Southeast Asia expands data centers\\nEurope’s talks on world-leading AI rules paused after 22 hours and will start again Friday\\nGoogle launches Gemini, upping the stakes in the global AI race\\nBank of England will review the risks that AI poses to UK financial stability\\nEurope was set to lead the world on AI regulation. Artificial intelligence\\nChatGPT-maker braces for fight with New York Times and authors on ‘fair use’ of copyrighted works\\nCES 2024 updates: The most interesting news and gadgets from tech’s big show\\nMaryland governor signs executive order guiding AI use\\nJudges in England and Wales are given cautious approval to use AI in writing legal opinions\\nMicrosoft’s new AI key is first big change to keyboards in decades\\n Tech giants are divided as they lobby regulators\\nEurope’s world-leading artificial intelligence rules are facing a do-or-die moment\\nSports Illustrated is the latest media company damaged by an AI experiment gone wrong\\nPentagon’s AI initiatives accelerate hard decisions on lethal autonomous weapons\\nPutin to boost AI work in Russia to fight a Western monopoly he says is ‘unacceptable and dangerous’\\n Now he wants a treaty regulating AI\\nCongressional candidate’s voter outreach tool is latest AI experiment ahead of 2024 elections\\nEurope agreed on world-leading AI rules.\"}, {'url': 'https://news.mit.edu/topic/artificial-intelligence2', 'content': 'October 30, 2023\\nRead full story →\\nAccelerating AI tasks while preserving data security\\nThe SecureLoop search tool efficiently identifies secure designs for hardware that can boost the performance of complex AI tasks, while requiring less energy.\\nOctober 30, 2023\\nRead full story →\\nThe brain may learn about the world the same way some computational models do\\nTwo studies find “self-supervised” models, which learn about their environment from unlabeled data, can show activity patterns similar to those of the mammalian brain.\\nOctober 30, 2023\\nRead full story →\\nCelebrating Kendall Square’s past and shaping its future\\nThe 15th Kendall Square Association annual meeting explored new and old aspects of the neighborhood.\\nOctober 23, 2023\\nRead full story →\\nTo excel at engineering design, generative AI must learn to innovate, study finds\\nAI models that prioritize similarity falter when asked to design something completely new.\\n Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nTopic\\nArtificial intelligence\\nDownload RSS feed: News Articles / In the Media / Audio\\nSearch algorithm reveals nearly 200 new kinds of CRISPR systems\\nBy analyzing bacterial data, researchers have discovered thousands of rare new CRISPR systems that have a range of functions and could enable gene editing, diagnostics, and more.\\n November 15, 2023\\nRead full story →\\nExplained: Generative AI\\nHow do powerful generative AI systems like ChatGPT work, and what makes them different from other types of artificial intelligence?\\nNovember 9, 2023\\nRead full story →\\nUsing AI to optimize for rapid neural imaging\\nMIT CSAIL researchers combine AI and electron microscopy to expedite detailed brain network mapping, aiming to enhance connectomics research and clinical pathology.\\n November 2, 2023\\nRead full story →\\nNew techniques efficiently accelerate sparse tensors for massive AI models\\nComplimentary approaches — “HighLight” and “Tailors and Swiftiles” — could boost the performance of demanding machine-learning tasks.\\n November 21, 2023\\nRead full story →\\nSynthetic imagery sets new bar in AI training efficiency\\nMIT CSAIL researchers innovate with synthetic imagery to train AI, paving the way for more efficient and bias-reduced machine learning.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Tavily search results provide a good overview of recent AI news from various sources.  I can synthesize this information to answer the question.\n",
      "\n",
      "Thought:I now know the final answer.\n",
      "\n",
      "Final Answer: Las últimas noticias sobre inteligencia artificial incluyen el despido del CEO de OpenAI, Sam Altman;  el desarrollo de nuevas regulaciones de IA en Europa;  avances en modelos de IA generativa como Gemini de Google;  preocupaciones sobre el uso ético de la IA y su impacto en diversas industrias (incluyendo medios de comunicación y arte); y  el aumento de la inversión y desarrollo en IA por parte de grandes empresas tecnológicas.  También hay noticias sobre el uso de la IA en sectores como la manufactura, la defensa y la atención médica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Las últimas noticias sobre inteligencia artificial incluyen el despido del CEO de OpenAI, Sam Altman;  el desarrollo de nuevas regulaciones de IA en Europa;  avances en modelos de IA generativa como Gemini de Google;  preocupaciones sobre el uso ético de la IA y su impacto en diversas industrias (incluyendo medios de comunicación y arte); y  el aumento de la inversión y desarrollo en IA por parte de grandes empresas tecnológicas.  También hay noticias sobre el uso de la IA en sectores como la manufactura, la defensa y la atención médica.\n"
     ]
    }
   ],
   "source": [
    "# Pregunta que debería usar Tavily\n",
    "response = agent_executor.invoke({\"input\": \"¿Cuáles son las últimas noticias sobre inteligencia artificial?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRZu29jGsyN0"
   },
   "source": [
    "El agente debería ocupar Tavily para preguntas sobre temas actuales o información dinámica que no suele estar en una enciclopedia, como noticias o eventos recientes. Por otra parte, debería ocupar Wikipedia para preguntas históricas, académicas o generales que tienen respuestas bien documentadas en su base de conocimiento.\n",
    "\n",
    "De esta forma, según la verificación de respuestas realizadas se aprecia que el agente está usando correctamente las tools disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm3hMsbMuGRb"
   },
   "source": [
    "Se ejecutará de nuevo la solución RAG, utilizando la versión previa a la prueba de hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCurn5CKuWvU",
    "outputId": "ec7cbfa3-dbc4-4b14-e1f3-9354d433e9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 1362\n",
      "Vectorstore almacenado en vectorstore.faiss\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrae texto de todas las páginas de un archivo PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def create_documents_from_pdfs(doc_paths):\n",
    "    \"\"\"Convierte PDFs a objetos Document para procesar con LangChain.\"\"\"\n",
    "    documents = []\n",
    "    for doc_path in doc_paths:\n",
    "        text = extract_text_from_pdf(doc_path)\n",
    "        documents.append(Document(page_content=text, metadata={\"source\": doc_path}))\n",
    "    return documents\n",
    "\n",
    "def split_and_vectorize_documents(documents):\n",
    "    \"\"\"Divide los documentos en chunks y los vectoriza.\"\"\"\n",
    "    # Se inicializa el splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(documents)  # Se divide en chunks\n",
    "\n",
    "    print(f\"Total de chunks creados: {len(splits)}\")\n",
    "\n",
    "    # Se crean los embeddings\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)  # Vectorización y almacenamiento\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "# Se crean documentos desde PDFs\n",
    "documents = create_documents_from_pdfs(doc_paths)\n",
    "\n",
    "# Se dividen y vectorizan\n",
    "vectorstore = split_and_vectorize_documents(documents)\n",
    "\n",
    "# Se almacena la base de datos FAISS\n",
    "faiss_file = \"vectorstore.faiss\"\n",
    "vectorstore.save_local(faiss_file)\n",
    "print(f\"Vectorstore almacenado en {faiss_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "CkSfbC7EueeL"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Se configura un modelo de generación de texto\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", # Modelo de lenguaje\n",
    "    temperature=0, # Probabilidad de \"respuestas creativas\"\n",
    "    max_tokens=None, # Sin tope de tokens\n",
    "    timeout=None, # Sin timeout\n",
    "    max_retries=2, # Número máximo de intentos\n",
    ")\n",
    "\n",
    "# Se crea la chain RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-5EczyVuTGn"
   },
   "source": [
    "Ahora sí, se generan las tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def rag_tool(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa la solución RAG para responder preguntas basadas en el contenido de los PDFs.\n",
    "    \"\"\"\n",
    "    response = qa_chain.run(question)\n",
    "    return response\n",
    "\n",
    "@tool\n",
    "def react_agent_tool(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Usa el agente ReAct para responder preguntas usando Tavily y Wikipedia.\n",
    "    \"\"\"\n",
    "    response = agent_executor.invoke({\"input\": input_text})\n",
    "    return response[\"output\"]\n",
    "\n",
    "# Lista de herramientas transformadas\n",
    "tools = [rag_tool, react_agent_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "yv2ZY0BAv1RD"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "# Prompt para el agente supervisor\n",
    "supervisor_prompt = \"\"\"\n",
    "Eres un agente supervisor con acceso a herramientas especializadas:\n",
    "1. Usa `rag_tool` para responder preguntas relacionadas con información extraída de PDFs.\n",
    "2. Usa `react_agent_tool` para responder preguntas que requieren búsquedas en la web.\n",
    "\n",
    "Pregunta: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "# Agente supervisor\n",
    "supervisor_agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,  # Lista de herramientas\n",
    "    prompt=PromptTemplate.from_template(supervisor_prompt)\n",
    ")\n",
    "\n",
    "# Se convierte el agente en ejecutor\n",
    "supervisor = AgentExecutor(agent=supervisor_agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_1t0zkgv1qW",
    "outputId": "747bc9ab-1887-40fd-f37a-958e302acbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: ¿Qué es un estimador de máxima verosimilitud?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¿Qué es un estimador de máxima verosimilitud?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to understand what a maximum likelihood estimator is.  A Wikipedia search should provide a good definition and explanation.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Maximum likelihood estimation\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Maximum likelihood estimation\n",
      "Summary: In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.\n",
      "If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when the random errors are assumed to have normal distributions with the same variance.\n",
      "From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with a prior distribution that is uniform in the region of interest. In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.\n",
      "\n",
      "Page: Maximum a posteriori estimation\n",
      "Summary: In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate.  MAP estimation can therefore be seen as a regularization of maximum likelihood estimation.\n",
      "\n",
      "Page: Likelihood function\n",
      "Summary: A likelihood function (often simply called the likelihood) measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. It is constructed from the joint probability distribution of the random variable that (presumably) generated the observations. When evaluated on the actual data points, it becomes a function solely of the model parameters.\n",
      "In maximum likelihood estimation, the argument that maximizes the likelihood function serves as a point estimate for the unknown parameter, while the Fisher information (often approximated by the likelihood's Hessian matrix at the maximum) gives an indication of the estimate's precision.\n",
      "In contrast, in Bayesian statistics, the estimate of interest is the converse of the likelihood, the so-called posterior probability of the parameter given the observed data, which is calculated via Bayes' rule.\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The Wikipedia articles provide a good explanation.\n",
      "\n",
      "Final Answer: Un estimador de máxima verosimilitud (MLE) es un método para estimar los parámetros de una distribución de probabilidad asumida, dados algunos datos observados.  Se logra maximizando una función de verosimilitud para que, bajo el modelo estadístico asumido, los datos observados sean los más probables. El punto en el espacio de parámetros que maximiza la función de verosimilitud se llama estimación de máxima verosimilitud.  La lógica de la máxima verosimilitud es intuitiva y flexible, y como tal, el método se ha convertido en un medio dominante de inferencia estadística.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mUn estimador de máxima verosimilitud (MLE) es un método para estimar los parámetros de una distribución de probabilidad asumida, dados algunos datos observados.  Se logra maximizando una función de verosimilitud para que, bajo el modelo estadístico asumido, los datos observados sean los más probables. El punto en el espacio de parámetros que maximiza la función de verosimilitud se llama estimación de máxima verosimilitud.  La lógica de la máxima verosimilitud es intuitiva y flexible, y como tal, el método se ha convertido en un medio dominante de inferencia estadística.\u001b[0m\u001b[32;1m\u001b[1;3mUn estimador de máxima verosimilitud (MLE) es un método para estimar los parámetros de una distribución de probabilidad asumida, dados algunos datos observados. Se logra maximizando una función de verosimilitud para que, bajo el modelo estadístico asumido, los datos observados sean los más probables. El punto en el espacio de parámetros que maximiza la función de verosimilitud se llama estimación de máxima verosimilitud. La lógica de la máxima verosimilitud es intuitiva y flexible, y como tal, el método se ha convertido en un medio dominante de inferencia estadística.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: Un estimador de máxima verosimilitud (MLE) es un método para estimar los parámetros de una distribución de probabilidad asumida, dados algunos datos observados. Se logra maximizando una función de verosimilitud para que, bajo el modelo estadístico asumido, los datos observados sean los más probables. El punto en el espacio de parámetros que maximiza la función de verosimilitud se llama estimación de máxima verosimilitud. La lógica de la máxima verosimilitud es intuitiva y flexible, y como tal, el método se ha convertido en un medio dominante de inferencia estadística.\n",
      "\n",
      "Pregunta: ¿Qué es el aprendizaje supervisado?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¿Qué es el aprendizaje supervisado?'}`\n",
      "responded: Para responder a la pregunta \"¿Qué es el aprendizaje supervisado?\", usaré la herramienta `react_agent_tool` ya que requiere información general que probablemente se encuentre en la web.  `rag_tool` se enfoca en PDFs específicos.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about supervised learning, I need a definition.  Wikipedia is a good resource for this type of general definition.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Supervised learning\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Supervised learning\n",
      "Summary: Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as a human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data to expected output values. An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n",
      "\n",
      "Page: Self-supervised learning\n",
      "Summary: Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on externally-provided labels. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving them requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples, where one sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects. \n",
      "During SSL, the model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels, which help to initialize the model parameters. Next, the actual task is performed with supervised or unsupervised learning.\n",
      "Self-supervised learning has produced promising results in recent years, and has found practical application in fields such as audio processing, and is being used by Facebook and others for speech recognition. \n",
      "\n",
      "\n",
      "\n",
      "Page: Weak supervision\n",
      "Summary: Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer. Wikipedia provided a good definition of supervised learning.\n",
      "\n",
      "Final Answer: El aprendizaje supervisado (SL) es un paradigma en el aprendizaje automático donde los objetos de entrada (por ejemplo, un vector de variables predictoras) y un valor de salida deseado (también conocido como señal supervisora etiquetada por humanos) entrenan un modelo. Los datos de entrenamiento se procesan, creando una función que asigna nuevos datos a los valores de salida esperados. Un escenario óptimo permitirá que el algoritmo determine correctamente los valores de salida para instancias no vistas. Esto requiere que el algoritmo de aprendizaje generalice a partir de los datos de entrenamiento a situaciones no vistas de una manera \"razonable\" (ver sesgo inductivo). Esta cualidad estadística de un algoritmo se mide a través del llamado error de generalización.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mEl aprendizaje supervisado (SL) es un paradigma en el aprendizaje automático donde los objetos de entrada (por ejemplo, un vector de variables predictoras) y un valor de salida deseado (también conocido como señal supervisora etiquetada por humanos) entrenan un modelo. Los datos de entrenamiento se procesan, creando una función que asigna nuevos datos a los valores de salida esperados. Un escenario óptimo permitirá que el algoritmo determine correctamente los valores de salida para instancias no vistas. Esto requiere que el algoritmo de aprendizaje generalice a partir de los datos de entrenamiento a situaciones no vistas de una manera \"razonable\" (ver sesgo inductivo). Esta cualidad estadística de un algoritmo se mide a través del llamado error de generalización.\u001b[0m\u001b[32;1m\u001b[1;3mEl aprendizaje supervisado es un paradigma de aprendizaje automático donde se utiliza un conjunto de datos etiquetados para entrenar un modelo.  Estos datos consisten en pares de entrada-salida, donde la entrada es el dato que se introduce al modelo y la salida es el resultado esperado. El modelo aprende a mapear las entradas a las salidas correspondientes a través de un proceso de entrenamiento.  Una vez entrenado, el modelo puede predecir la salida para nuevas entradas que no ha visto antes.  Requiere datos etiquetados previamente por humanos, lo que lo diferencia de otros tipos de aprendizaje automático como el no supervisado.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: El aprendizaje supervisado es un paradigma de aprendizaje automático donde se utiliza un conjunto de datos etiquetados para entrenar un modelo.  Estos datos consisten en pares de entrada-salida, donde la entrada es el dato que se introduce al modelo y la salida es el resultado esperado. El modelo aprende a mapear las entradas a las salidas correspondientes a través de un proceso de entrenamiento.  Una vez entrenado, el modelo puede predecir la salida para nuevas entradas que no ha visto antes.  Requiere datos etiquetados previamente por humanos, lo que lo diferencia de otros tipos de aprendizaje automático como el no supervisado.\n",
      "\n",
      "Pregunta: ¿Qué equipo ganó la última Eurocopa?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `react_agent_tool` with `{'input_text': '¿Qué equipo ganó la última Eurocopa?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out which team won the most recent UEFA European Championship.  Wikipedia should have this information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: UEFA European Championship\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: UEFA European Championship\n",
      "Summary: The UEFA European Football Championship, less formally the European Championship and informally the Euro or Euros, is the primary association football tournament organised by the Union of European Football Associations (UEFA). The competition is contested by UEFA members' senior men's national teams, determining the continental champion of Europe. It is the second-most watched football tournament in the world after the FIFA World Cup; the Euro 2016 final was watched by a global audience of around 600 million. The competition has been held every four years since 1960, except for 2020, when it was postponed until 2021 due to the COVID-19 pandemic in Europe, but kept the name Euro 2020. Scheduled to be in the even-numbered year between FIFA World Cup tournaments, it was originally called the European Nations' Cup before changing to its current name in 1968. Since 1996, the individual events have been branded as \"UEFA Euro [year]\".\n",
      "Before entering the tournament, all teams other than the host nations (which qualify automatically) compete in a qualifying process. Until 2016, the championship winners could compete in the following year's FIFA Confederations Cup, but were not obliged to do so. From the 2020 edition onwards, the winner competes in the CONMEBOL–UEFA Cup of Champions.\n",
      "The seventeen European Championship tournaments have been won by ten national teams: Spain have won four titles, Germany have won three titles, Italy and France have won two titles, and the Soviet Union, Czechoslovakia, the Netherlands, Denmark, Greece and Portugal have won one title each. To date, Spain is the only team to have won consecutive titles, doing so in 2008 and 2012.\n",
      "The most recent championship, held in Germany in 2024, was won by Spain, who lifted a record fourth European title after beating England 2–1 in the final at the Olympiastadion in Berlin.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA European Under-21 Championship\n",
      "Summary: The UEFA European Under-21 Championship, the UEFA Under-21 Championship or simply the Euro Under-21, is a biennial football competition contested by the European men's under-21 national teams of the UEFA member associations. Since 1992, the competition also serves as the UEFA qualification tournament for the Summer Olympics.\n",
      "Italy and Spain are the most successful teams in the tournament's history, having won five titles each. England are the current champions, having beaten Spain 1–0 in the 2023 final.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA European Under-19 Championship\n",
      "Summary: The UEFA European Under-19 Championship, or simply the Euro Under-19, is an annual football competition, contested by the European men's under-19 national teams of the UEFA member associations.\n",
      "Spain is the most successful team in this competition, having won twelve titles. Spain are also the current champions.\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia article mentions the most recent Euro (Euro 2020, actually held in 2021) but doesn't state the winner.  It also mentions a future Euro 2024, but that hasn't happened yet. I need to refine my search.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Euro 2020 winner\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: UEFA Euro 2020 final\n",
      "Summary: The UEFA Euro 2020 final was an association football match that took place at Wembley Stadium in London, England, on 11 July 2021, to determine the winners of UEFA Euro 2020. It was the sixteenth final of the UEFA European Championship, a quadrennial tournament contested by the senior men's national teams of the member associations of UEFA to decide the champions of Europe. Originally scheduled for 12 July 2020, the match had been postponed along with the rest of the tournament due to the COVID-19 pandemic in Europe. The match was contested between Italy, in their fourth Euro final, and England, in their first ever Euro final, and just their second final at any major tournament, after the 1966 FIFA World Cup final.\n",
      "In front of a crowd of 67,173, limited by COVID-19 restrictions, with an estimated global audience of 328 million, England's Luke Shaw opened the scoring in the second minute of the match, the fastest goal ever scored in a European Championship final. Leonardo Bonucci – who was later named the man of the match – scored an equaliser midway through the second half. With the score 1–1 after extra time, England gained a 2–1 advantage in the penalty shoot-out after two kicks each, but their last three takers all missed, and Italy won 3–2.\n",
      "This was Italy's first major title since the 2006 FIFA World Cup, and their first European Championship since winning it on home soil in 1968; in terms of European Championship titles, it put Italy level with France on two titles, and one title behind Spain and Germany. England became the third nation in the 21st century to lose the European Championship final on home soil, following Portugal in 2004 and France in 2016. After the match, England's unsuccessful penalty takers (Marcus Rashford, Jadon Sancho and Bukayo Saka) were subjected to racial abuse on social media, which was investigated by the Metropolitan Police. The event was also marred by crowd disorder as roughly six thousand ticketless England supporters fought police and security in attempts to breach the stadium.\n",
      "\n",
      "\n",
      "\n",
      "Page: UEFA Euro 2020\n",
      "Summary: The 2020 UEFA European Football Championship, commonly referred to as UEFA Euro 2020 or simply Euro 2020, was the 16th UEFA European Championship, the quadrennial international men's football championship of Europe organised by the Union of European Football Associations (UEFA). To celebrate the diamond jubilee of the European Championship competition, UEFA president Michel Platini declared that the tournament would be hosted in several nations as a \"romantic\" one-off event, with 11 cities in 11 UEFA countries each providing venues for the tournament, making it the second senior international tournament in history after the 2007 AFC Asian Cup to have more than two nations co-hosting it.\n",
      "Portugal were the defending champions, but were eliminated in the round of 16 by Belgium. Italy won their second European Championship title by beating England on penalties in the final following a 1–1 draw after extra time. The win came exactly on the 39th anniversary of Italy's 1982 FIFA World Cup final victory over West Germany.\n",
      "The tournament was originally intended to be played between 12 June and 12 July 2020. Due to COVID-19 restrictions during that year, the tournament was postponed to June and July 2021, while retaining the name UEFA Euro 2020 and host venues. Alongside special rules regarding COVID-19 restrictions, UEFA also allowed two extra substitutions and implemented video assistant referee (VAR) for the first time. Initially, there were 13 venues chosen for the tournament but two were later dropped. Brussels was dropped in December 2017 after the city's Eurostadium was abandoned, while Dublin was dropped in April 2021 because there was no guarantee that spectators could attend. Spain originally intended to use Bilbao as a host venue but later changed it to Seville to allow for spectators at matches. UEFA chose Stadio Olimpico in Rome to host the open\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Italy won the last Eurocopa (Euro 2020, held in 2021).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mItaly won the last Eurocopa (Euro 2020, held in 2021).\u001b[0m\u001b[32;1m\u001b[1;3mItalia ganó la última Eurocopa (Eurocopa 2020, celebrada en 2021).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta del supervisor: Italia ganó la última Eurocopa (Eurocopa 2020, celebrada en 2021).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preguntas de prueba\n",
    "questions = [\n",
    "    \"¿Qué es un estimador de máxima verosimilitud?\",\n",
    "    \"¿Qué es el aprendizaje supervisado?\",\n",
    "    \"¿Qué equipo ganó la última Eurocopa?\"\n",
    "]\n",
    "\n",
    "# Se obtienen respuestas del supervisor\n",
    "for question in questions:\n",
    "    print(f\"Pregunta: {question}\")\n",
    "    response = supervisor.invoke({\"input\": question})\n",
    "    print(f\"Respuesta del supervisor: {response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Zbntu8xDG5"
   },
   "source": [
    "El agente supervisor decide qué herramienta utilizar en función de la pregunta. Según lo esperado, para preguntas relacionadas con los PDFs entregados, debería usar `rag_tool`, y para preguntas que requieran buscar en la web, usará `react_agent_tool`. Ahora bien, como se puede ver, el agente siempre busca por Wikipedia, inclusive para aquellas preguntas que están muy relacionadas a los archivos PDF. Esto se debe principalmente a la generalidad de las preguntas realizadas, ya que basta con una búsqueda rápida en Wikipedia para responderlas. Ahora bien, si se hicieran preguntas mucho más específicas que su respuesta probablemente no esté en Wikipedia, como por ejemplo algunas relacionadas a un paper específico de la materia de estadística o aprendizaje de máquinas, probablemente sí vaya a buscar la respuesta utilizando `rag_tool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "Un Router es un modelo explícitamente programado para clasificar preguntas y dirigirlas hacia la herramienta adecuada basándose en reglas predefinidas (como un clasificador de texto). En contraste, el enfoque con el agente supervisor utiliza un LLM para razonar dinámicamente sobre qué herramienta utilizar.\n",
    "\n",
    "El enfoque del agente supervisor cuenta con la ventaja de que puede razonar sobre preguntas más complejas, mientras que un Router es más rígido al depender de reglas específicas. Además, se destaca que no requiere entrenamiento adicional, ya que el razonamiento pasa por el LLM. Por otro lado, el agente supervisor cuenta con la desventaja asociada al costo computacional, ya que usar un LLM como razonador es más costoso que utilizar una solución simplemente basada en reglas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8TsvnCPbkIA",
    "outputId": "11d72e1b-7c40-4b0a-b91b-e9e6875575e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "Z3KedQSvg1-n",
    "outputId": "a4157ff5-75d0-43b6-b651-fb82218ba763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://93ee8203a294149d4c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93ee8203a294149d4c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\",\n",
    "    description=\"Hola! Soy un chatbot de aprendizaje de máquinas y estadística c:\",\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qBPet_Mq8dX9",
    "ZJ6J1_-Y9nHO",
    "pmcX6bRC9agQ",
    "LEO_dY4x_SJu",
    "E-bpdb8wZID1",
    "RO-EsAaPAYEm",
    "YChodtNQwzG2",
    "hQrZVQflX_5f",
    "3z-oIUSrlAsY",
    "x6Xw4YHT3P5d",
    "_vsvq177oKS0",
    "wScMyWZioT7M",
    "SYhhMl_GoCtg",
    "mQ4fPRRihGLe",
    "ZrxOQroVnaZ5",
    "Qtx17d_jS25Q",
    "V47l7Mjfrk0N",
    "SonB1A-9rtRq",
    "CvUIMdX6r0ne",
    "dKV0JxK3r-XG",
    "4JWVSuWiZ8Mj",
    "vFc3jBT5g0kT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8dd9b572c805487a9fb430fdc4ab12bb",
        "deepnote_cell_height": 156.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "XUZ1dFPHzAHl"
      },
      "source": [
        "<h1><center>Laboratorio 5: La desperaci√≥n de Mr. Cheems üêº</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2024</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d65413cd8566460dbceffcd13ca236e7",
        "deepnote_cell_type": "markdown",
        "id": "UD8X1uhGzAHq"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicol√°s Ojeda, Melanie Pe√±a, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8e9217d02d124830a9b86046600a1605",
        "deepnote_cell_height": 172.13333129882812,
        "deepnote_cell_type": "markdown",
        "id": "tXflExjqzAHr"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Joaqu√≠n Herrera Su√°rez\n",
        "- Nombre de alumno 2: Hecmar Taucare Reyes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "010402b6d5f743b885a80d2e1c6ae11a",
        "deepnote_cell_height": 62.19999694824219,
        "deepnote_cell_type": "markdown",
        "id": "AD-V0bbZzAHr"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/Joaquin-HS/MDS7202)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ef0224c7a99e4b718b55493b0a1e99c4",
        "deepnote_cell_height": 724.9000244140625,
        "deepnote_cell_type": "markdown",
        "id": "6uBLPj1PzAHs"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 6 d√≠as de plazo con descuento de 1 punto por d√≠a. Entregas Jueves a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar procesos m√°s limpios en Feature Engineering.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59664481c26f4ac4a753765269b1db6a",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "wrG4gYabzAHs"
      },
      "source": [
        "## Descripci√≥n del laboratorio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8c7bf8ea553d44c7a2efd61106a0bac2",
        "deepnote_cell_height": 61.866668701171875,
        "deepnote_cell_type": "markdown",
        "id": "MhISwri4zAHy"
      },
      "source": [
        "### Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-29T00:08:16.884674Z",
          "start_time": "2021-03-29T00:08:16.349846Z"
        },
        "cell_id": "67b4b29f0e6b48719b58d579276f2b19",
        "deepnote_cell_height": 514.13330078125,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 8517,
        "execution_start": 1635469788590,
        "id": "uyc33dKdzAHy",
        "source_hash": "a3741fd5"
      },
      "outputs": [],
      "source": [
        "# Libreria Core del lab.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Libreria para plotear (En colab esta desactualizado plotly)\n",
        "!pip install --upgrade plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Librerias utiles\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ce6a19ec6fc6486e832760ac3740d7ef",
        "deepnote_cell_height": 219.46665954589844,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 7,
        "execution_start": 1635165625274,
        "id": "gQ0-zPV4NNrq",
        "outputId": "a7c33afa-37fe-4965-de1a-53b8994c8c07",
        "source_hash": "c60dc4a7"
      },
      "outputs": [],
      "source": [
        "# Si usted est√° utilizando Colabolatory le puede ser √∫til este c√≥digo para cargar los archivos.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    path = 'Direcci√≥n donde tiene los archivos en el Drive'\n",
        "except:\n",
        "    print('Ignorando conexi√≥n drive-colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "28c7a8b483d84878ac5a4f7ba882b711",
        "deepnote_cell_height": 133.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "QDwIXTh7bK_A",
        "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
      },
      "source": [
        "# Feature engineering en datos de retail üõçÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "160bb2695f6547448bfb0f99420f952c",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "_Eu4qBqnXMff",
        "tags": []
      },
      "source": [
        "### 0. Cargar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6c6799ecc9e74272922d46a3b5a8b79e",
        "deepnote_cell_height": 294.683349609375,
        "deepnote_cell_type": "markdown",
        "id": "4shIzqqwXMfe",
        "tags": []
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://s1.eestatic.com/2018/04/14/social/la_jungla_-_social_299733421_73842361_854x640.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "48d29c89e3b6455083f8fac764f97f3b",
        "deepnote_cell_height": 475.066650390625,
        "deepnote_cell_type": "markdown",
        "id": "cDpKjYRCXMfg",
        "tags": []
      },
      "source": [
        "Mr. Cheems, gerente de una cotizada tienda de retail en Europa, les solicita si pueden analizar los datos de algunas de sus tiendas. En una reuni√≥n, Mr Cheems le comenta que la calidad de sus datos no es muy buena, por lo que le solicita a usted que limpie su base de datos y cree nuevos atributos relevantes para el negocio.\n",
        "\n",
        "Por ello, el √°rea de ventas les entrega archivo llamado `online_retail_data.pickle` el cual usted decide cargar a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se carga el dataset con el archivo \"online_retail_data.pickle\"\n",
        "df_retail = pd.read_pickle(\"online_retail_data.pickle\")\n",
        "df_retail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6nm_0uWvrFv"
      },
      "source": [
        "### 1. Funci√≥n para explorar caracter√≠sticas [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZEZbbLoqfI"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://editor.analyticsvidhya.com/uploads/47389meme.png\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-7ZaNutk2GO"
      },
      "source": [
        "\n",
        "\n",
        "Tras inspeccionar brevemente los datos proporcionados, usted decide crear una funci√≥n que realice lo siguiente:\n",
        "- Plotee un histograma para las variables precios y cantidad. [3 puntos]\n",
        "- Imprima un conteo de datos nulos por variable [2 puntos]\n",
        "\n",
        "NOTA: Para generar los gr√°ficos es **OBLIGATORIO** el uso de plotly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM8FZ_4Yuiwi"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uDqT1Ljpk7vp"
      },
      "outputs": [],
      "source": [
        "def explore_data(dataframe_in):\n",
        "    # Se plotea un histograma para las variables num√©ricas\n",
        "    for col in dataframe_in.select_dtypes(include=np.number).columns:\n",
        "        if col == \"Customer ID\":\n",
        "            continue\n",
        "        fig = px.histogram(dataframe_in, x=col, title=f\"Histograma de {col}\")\n",
        "        fig.show()\n",
        "\n",
        "    # Se podr√≠a aplicar logaritmo a los valores para visualizar mejor la distribuci√≥n,\n",
        "    # pero esto se abordar√° mediante rango intercuartilico en la siguiente secci√≥n.\n",
        "\n",
        "    # Se imprime un conteo de datos nulos por variable\n",
        "    print(\"Conteo de datos nulos por variable\")\n",
        "    print(dataframe_in.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explore_data(df_retail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZY_N0Ad1GP"
      },
      "source": [
        "### 2. Eliminando outliers [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXTpIi1Bo2KG"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.licdn.com/dms/image/C5612AQGdXKCka7HumA/article-cover_image-shrink_600_2000/0/1520056407281?e=2147483647&v=beta&t=VZcfjjzjK4LxXdZkSu1KisWC0Ry8bk4tPCn3R8aYdNM\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqH4t-Jvj05"
      },
      "source": [
        "#### 2.1 Creando la clase IQR [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtCQGHN_mzEp"
      },
      "source": [
        "Entre las falencias de los datos, Mr. Cheems le comenta que a veces los operadores no ingresan el precio correcto de los productos. Mr. Cheems le comenta que se dio cuenta de este fen√≥meno porque hay productos con precios exager√°damente altos o bajos. Por lo cual usted decide eliminar outliers del dataframe a traves del rango intercuartil el cual cuenta con los siguientes pasos:\n",
        "\n",
        "1. Calcular el primer cuartil $Q1$ y el tercer cuartil $Q3$. Hint: utilice el m√©todo `quantile()`\n",
        "\n",
        "2. Calcular el rango intercuartil (RIC): $RIC = Q3 - Q1$\n",
        "\n",
        "3. Calcular los l√≠mites para identificar outliers:\n",
        " - L√≠mite inferior: $~~Q1 - \\lambda \\cdot RIC$\n",
        " - L√≠mite superior: $~~Q3 + \\lambda \\cdot RIC$\n",
        "\n",
        "4. Eliminar outliers: Los outliers son los datos que est√°n por debajo del l√≠mite inferior o por encima del l√≠mite superior.\n",
        "\n",
        "\n",
        "Para realizar dicha tarea, usted decide crear una clase llamada `IQR()` utilizando `BaseEstimator` y `TransformerMixin` para realizar una transformaci√≥n de cada una de las columnas num√©ricas del DataFrame utilizando `ColumnTransformer()` m√°s tarde. Considere que lambda debe ser $\\lambda$ un par√°metro a definir por el usuario.\n",
        "\n",
        "Hint: tome como referencia el siguiente [enlace](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer).\n",
        ">**Nota: No modificar el m√©todo set_output de la clase IQR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqK6AZnuhmL"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "70CGFkRScKKP"
      },
      "outputs": [],
      "source": [
        "class IQR(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, lambd=1.5):\n",
        "    self.lambd = lambd  # Factor de multiplicaci√≥n para el rango intercuartil (umbral de outliers)\n",
        "    self.q1 = None  # Primer cuartil\n",
        "    self.q3 = None  # Tercer cuartil\n",
        "    self.iqr = None  # Rango intercuartil\n",
        "    self.lower_bound = None  # L√≠mite inferior\n",
        "    self.upper_bound = None  # L√≠mite superior\n",
        "\n",
        "  def fit(self, X):\n",
        "    # Se calcula el primer y tercer cuartil\n",
        "    self.q1 = X.quantile(0.25)\n",
        "    self.q3 = X.quantile(0.75)\n",
        "\n",
        "    # Se calcula el rango intercuartil\n",
        "    self.iqr = self.q3 - self.q1\n",
        "\n",
        "    # Se calculan los l√≠mites inferior y superior\n",
        "    self.lower_bound = self.q1 - self.iqr * self.lambd\n",
        "    self.upper_bound = self.q3 + self.iqr * self.lambd\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    # Se eliminan los valores at√≠picos del dataframe para cualquier columna\n",
        "    X = X[(X >= self.lower_bound) & (X <= self.upper_bound)]\n",
        "    \n",
        "    return X\n",
        "\n",
        "  def set_output(self,transform='default'):\n",
        "    # No se modifica esta funci√≥n\n",
        "    return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pse94ohOm1um"
      },
      "source": [
        "#### 2.2 Creaci√≥n del Pipeline [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWWiGA5m_Hj"
      },
      "source": [
        "Para comenzar introduci√©ndose en el uso de pipeline, usted decide definir un pipeline con el Transformer previamente definido. Adem√°s, usted decide visualizar c√≥mo cambia la distribuci√≥n de las variables Precio y Cantidad antes y despues de aplicar IQR. Para ello, usted aplica los siguientes pasos:\n",
        "\n",
        "- Definir un pipeline llamado `numeric_transformations` para las variables precio y cantidad con la transformaci√≥n IQR. [1 punto]\n",
        "- Defina un column transformer que aplique `numeric_transformations` para las variables num√©ricas y `passthrough` para las variables categ√≥ricas. Adicionalmente, fije el par√°metro `verbose_feature_names_out` en `False`. Ver hint al final [1 puntos]\n",
        "- Defina el dataframe `df_iqr` aplicando el column transformer a los datos proporcionados por Mr. Cheems considerando un valor de $\\lambda$ que tenga un desempe√±o aceptable para ambas variables. [1 punto]\n",
        "- Usar `explore_data` en `df_retail` y en `df_iqr`.  [1 punto]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables. ¬øQu√© sucede al aumentar el valor de lambda? [1 punto]\n",
        "\n",
        "\n",
        "HINT: El transformador `passthrough` est√° predefinido y es una opci√≥n que puedes usar para las columnas que no deseas transformar. Al especificar 'passthrough' para una parte de tu ColumnTransformer, las columnas correspondientes pasar√°n a trav√©s del ColumnTransformer sin ninguna modificaci√≥n. El siguiente [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) le puede ser √∫til.\n",
        ">**Nota: Mantenga el m√©todo set_output del column transformer con la transformaci√≥n `pandas` para obtener un dataframe una vez aplicado el column transformer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeizZcLuabD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF24vWb4GwLo"
      },
      "source": [
        "Ap√≥yese de la siguiente estructura para su respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaSuz2NSn7g6"
      },
      "outputs": [],
      "source": [
        "# Definici√≥n las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = ['Quantity', 'Price']\n",
        "categorical_columns = ['Invoice', 'StockCode', 'Description', 'InvoiceDate', 'Customer ID', 'Country']\n",
        "\n",
        "# Definici√≥n del pipeline\n",
        "numeric_transformations = Pipeline(steps=[('iqr', IQR(lambd=1.5))])\n",
        "\n",
        "# ColumnTransformer\n",
        "column_transformer = ColumnTransformer(transformers=[\n",
        "                                        ('numerical', numeric_transformations, numerical_columns),\n",
        "                                        ('categorical', 'passthrough', categorical_columns)\n",
        "                                        ],\n",
        "                                        verbose_feature_names_out=False)\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "# Se aplica ColumnTransformer a los datos\n",
        "df_iqr = column_transformer.fit_transform(df_retail)\n",
        "\n",
        "# Gr√°ficos\n",
        "explore_data(df_retail)\n",
        "explore_data(df_iqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKnc6UcsDkm"
      },
      "source": [
        "Como se puede observar en los gr√°ficos, aplicar IQR permite eliminar datos anormales, generando as√≠ una mejor visualizaci√≥n de la distribuci√≥n de cierta variable, que es lo que se busca al realizar un estudio inicial, dejando los outliers para ser analizados aparte. N√≥tese que estos outliers pasan a ser valores nulos seg√∫n la implementaci√≥n utilizada, por lo que habr√≠a que abordar dicho aspecto seg√∫n lo que se quiera.\n",
        "\n",
        "Las distribuciones de las variables num√©ricas analizadas exhiben que, en el caso de los precios, se tienen distintos valores marcados, con una tendencia a estar m√°s centrados entorno a 1 o 2 unidades. Por otro lado, las cantidades est√°n mucho m√°s agrupadas, acerc√°ndose a valores peque√±os como 5 o 10 unidades.\n",
        "\n",
        "Por √∫ltimo, se destaca que un aumento en el valor de lambda genera un mayor recorte de datos, haciendo m√°s estrictos los l√≠mites superior e inferior, los cuales indican los registros permitidos. Esto puede generar que se eliminen datos que son parte de la distribuci√≥n normal de la variable, perdiendo m√°s informaci√≥n de la debida. Se estableci√≥ un valor de 1.5 por defecto al ser un valor intermedio. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5s4dqMYCbJ"
      },
      "source": [
        "### 3. Agregando un imputer al pipeline [10 puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bc9fFeXp-At"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.makeameme.org/created/hmm-there-is.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugEdc26vJ5N"
      },
      "source": [
        "Para continuar con la limpieza del dataframe usted decide imputar los datos nulos de las variables num√©ricas, para lo cual decide realizar las siguientes tareas:\n",
        "\n",
        "1. Crear un pipeline para variables categ√≥ricas llamado `categoric_transformations` con un paso llamado `mode_imputer`, en el cual se imputen los datos faltantes por la categor√≠a m√°s frecuente.\n",
        "2. Agregar al pipeline `numeric_transformations` un paso llamado `mean_imputer`, en el cual se imputen los datos por la media usando [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) [1 punto]\n",
        "3. Crear y aplicar un `ColumnTransformer` actualizado con los pipelines `categoric_transformations` y `numeric_transformations` a `df_retail`, creando un dataframe llamado `df_mean_imputer`. [1 punto]\n",
        "4. Comparar los resultados de `explore_data` en `df_mean_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [2 puntos]\n",
        "5. Cambiar el imputer de `numeric_transformations` por [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) y definir un nuevo dataframe llamado `df_knn_imputer`, aplicando el nuevo ColumnTransformer a `df_retail`. En caso de los tiempos de ejecuci√≥n sean altos puede probar a reducir el par√°metro `n_neighbors`. [1 punto]\n",
        "6. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [2 puntos]\n",
        "7. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_mean_imputer`. ¬øCu√°l m√©todo de imputaci√≥n es mejor? Deje el m√©todo escogido en el ColumnTransformer. [2 puntos]\n",
        "\n",
        ">**Nota: Fije el par√°metro verbose_feature_names_out en `False` y utilice el m√©todo set_output con transformaci√≥n `pandas` en cada ColumnTransformer para obtener como salida un dataframe.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVUdZZxuo4o"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8jgag-EYFai"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "categoric_transformations = Pipeline(steps=[\n",
        "    ('mode_imputer', SimpleImputer(strategy='most_frequent')),\n",
        "])\n",
        "\n",
        "# 2\n",
        "numeric_transformations = Pipeline(steps=[\n",
        "    ('iqr', IQR(lambd=1.5)),\n",
        "    ('mean_imputer', SimpleImputer(strategy='mean'))])\n",
        "\n",
        "# 3\n",
        "column_transformer = ColumnTransformer(transformers=[\n",
        "    ('numerical', numeric_transformations, numerical_columns),\n",
        "    ('categorical', categoric_transformations, categorical_columns)\n",
        "], verbose_feature_names_out=False)\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "df_mean_imputer = column_transformer.fit_transform(df_retail)\n",
        "\n",
        "# 4\n",
        "explore_data(df_mean_imputer)\n",
        "explore_data(df_iqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De los gr√°ficos se observa que la distribuci√≥n no var√≠a de manera destacable, salvo por la agregaci√≥n de un punto nuevo en cada histograma, correspondiente al valor promedio de cada variable con una frecuencia igual al n√∫mero de nulos que se tiene para esa variable (2.28 - 2.29 para precio, y 6.8188 para cantidad). Se menciona, por un lado, lo inapropiado que puede ser realizar este tipo de imputaci√≥n para variables que solo tienen valores posibles enteros (como la variable cantidad); por otro, lo poco representativo que es simplemente a√±adir un peak con gran frecuencia si se est√°n analizando distribuciones; y por √∫ltimo, el notorio efecto que tiene el aumento de nulos debido a la aplicaci√≥n de IQR sobre procedimientos de imputaci√≥n, ya que gener√≥ que los peaks respectivos sean a√∫n mayores que los que se hubieran generado si se eliminaban directamente los outliers (esto no necesariamente es algo inapropiado, ya que depende de lo que desee el analista)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5\n",
        "numeric_transformations = Pipeline(steps=[\n",
        "    ('iqr', IQR(lambd=1.5)),\n",
        "    ('knn_imputer', KNNImputer(n_neighbors=3))])  # Se establece el n√∫mero de vecinos m√°s cercanos en 3 para evitar tiempos de ejecuci√≥n demasiado largos\n",
        "\n",
        "column_transformer = ColumnTransformer(transformers=[\n",
        "    ('numerical', numeric_transformations, numerical_columns),\n",
        "    ('categorical', categoric_transformations, categorical_columns)\n",
        "], verbose_feature_names_out=False)\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "df_knn_imputer = column_transformer.fit_transform(df_retail)\n",
        "\n",
        "# 6\n",
        "explore_data(df_knn_imputer)\n",
        "explore_data(df_iqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las distribuciones de los datos al imputar con KNN tienden a ser m√°s suaves que las distribuciones sin imputar, rellenando espacios intermedios sin perturbar demasiado la distribuci√≥n. Sin embargo, estos valores imputados no dejan de ser inventados, generando por ejemplo cantidades con valores decimales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7\n",
        "explore_data(df_knn_imputer)\n",
        "explore_data(df_mean_imputer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El m√©todo escogido es KNN, principalmente por su mejor capacidad de mantener la forma de la distribuci√≥n de las variables, con respecto a c√≥mo lo hace la imputaci√≥n por media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuUiW-9YYZ3"
      },
      "source": [
        "### 4. Creaci√≥n de nuevas features [20 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQSuoL5mubnA"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=250 src=\"https://miro.medium.com/max/1000/1*JtTWgAcfVTWV8OTjT47Atg.jpeg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-yHP5oIvzFS"
      },
      "source": [
        "#### 4.1 Definicion de LRMFP [10 puntos]\n",
        "(2 puntos por cada custom feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe0V2CnZY8Bc"
      },
      "source": [
        "Dado que Mr. Lepin est√° interesado en obtener nuevos atributos relevantes para su negocio, su equipo de expertos sugiere la construcci√≥n de variables **LRMFP**, las que se construyen en base a las siguientes definiciones:\n",
        "\n",
        "- **Length (L)**: Intervalo de tiempo, en d√≠as, entre la primera y la √∫ltima visita del cliente. Mientras mas grande sea el valor, mas fiel es el cliente.\n",
        "\n",
        "- **Recency (R)**: Indica hace cuanto tiempo el cliente realizo su ultima compra. Notar que para este caso, mientras mas grande es el valor, menos interes posee el usuario para repetir una compra en uno de los locales. **Considere \"hoy\" como la fecha mas reciente del dataset**.\n",
        "\n",
        "- **Monetary (M)**: El t√©rmino \"monetario\" se refiere a la cantidad media de dinero gastada por cada visita del cliente durante el per√≠odo de observaci√≥n y refleja la contribuci√≥n del cliente a los ingresos de la empresa.\n",
        "\n",
        "- **Frequency (F)**: Se refiere al n√∫mero total de visitas del cliente durante el periodo de observaci√≥n. Cuanto mayor sea la frecuencia, mayor ser√° la fidelidad del cliente.\n",
        "\n",
        "- **Periodicity (P)**: Representa si los clientes visitan las tiendas con regularidad.\n",
        "\n",
        "$$Periodicity(n)=std(IVT_1, ..., IVT_n)$$\n",
        "\n",
        "Donde $IVT$ denota el tiempo entre visitas y n representa el n√∫mero de valores de tiempo entre visitas de un cliente.\n",
        "\n",
        "\n",
        "$$IVT_i=date\\_diff(t_{i+1},t)$$\n",
        "\n",
        "En base a las definiciones se√±aladas, dise√±e una funci√≥n que permita obtener las caracter√≠sticas **LRMFP** recibiendo un DataFrame como entrada. Para esto, no estar√° permitido el uso de iteradores, utilice todas las herramientas que les ofrece `pandas` para realizar esto.\n",
        "\n",
        "Una referencia que le puede ser √∫til es el [documento original](https://www.researchgate.net/publication/315979555_LRFMP_model_for_customer_segmentation_in_the_grocery_retail_industry_a_case_study) en donde se propone este m√©todo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bee8d549c7c043a5b0cafae0543afadf",
        "deepnote_cell_height": 212.6666717529297,
        "deepnote_cell_type": "markdown",
        "id": "L7ZwWJxhXMfk",
        "tags": []
      },
      "source": [
        "**<u>Formato</u> del Resultado Esperado:**\n",
        "\n",
        "| Customer ID | Length | Recency | Frequency | Monetary | Periodicity |\n",
        "|------------:|-------:|--------:|----------:|---------:|------------:|\n",
        "|   12346.0   |    294 |      67 |        46 |   -64.68 |        37.0 |\n",
        "|   12347.0   |     37 |       3 |        71 |  1323.32 |         0.0 |\n",
        "|   12349.0   |    327 |      43 |       107 |  2646.99 |        78.0 |\n",
        "|   12352.0   |     16 |      11 |        18 |   343.80 |         0.0 |\n",
        "|   12356.0   |     44 |      16 |        84 |  3562.25 |        12.0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3c7f8a4a06a44cbd8d50e8a4decf4c71",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "6GaQZaMXXMfk",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "39a8b98eacdc43a4bdfeaa138b746198",
        "deepnote_cell_height": 83.86666870117188,
        "deepnote_cell_type": "code",
        "id": "VsgqgqsjXMfl",
        "owner_user_id": "8c58f50a-7a08-41a2-952e-38bdb7507048",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def custom_features(dataframe_in):\n",
        "    # Se trabaja con la columna InvoiceDate en formato datetime\n",
        "    dataframe_in['InvoiceDate'] = pd.to_datetime(dataframe_in['InvoiceDate'])\n",
        "    \n",
        "    # Gasto total por registro\n",
        "    dataframe_in['TotalSpent'] = dataframe_in['Quantity'] * dataframe_in['Price']\n",
        "    \n",
        "    # Antes de agrupar se define la fecha actual\n",
        "    today = dataframe_in['InvoiceDate'].max()\n",
        "\n",
        "    # Se agrupa por Customer ID\n",
        "    df_grouped = dataframe_in.groupby('Customer ID')\n",
        "\n",
        "    # Length (L): Intervalo de tiempo, en d√≠as, entre la primera y la √∫ltima visita del cliente\n",
        "    L = (df_grouped['InvoiceDate'].max() - df_grouped['InvoiceDate'].min()).dt.days\n",
        "\n",
        "    # Recency (R): Indica hace cuanto tiempo el cliente realiz√≥ su √∫ltima compra\n",
        "    R = (today - df_grouped['InvoiceDate'].max()).dt.days\n",
        "\n",
        "    # Monetary (M): Cantidad media de dinero gastada por cada visita del cliente\n",
        "    M = df_grouped['TotalSpent'].mean()\n",
        "\n",
        "    # Frequency (F): N√∫mero total de visitas del cliente\n",
        "    F = df_grouped['InvoiceDate'].nunique()\n",
        "\n",
        "    # Periodicity (P): Desviaci√≥n est√°ndar del tiempo entre visitas\n",
        "    def calc_periodicity(df_grouped):\n",
        "        # Se ordenan las fechas y se calcula la diferencia en d√≠as entre cada visita\n",
        "        sorted_dates = df_grouped['InvoiceDate'].sort_values()\n",
        "        visit_intervals = sorted_dates.diff().dt.days.dropna()  # Se eliminan los valores nulos, de existir\n",
        "        \n",
        "        # Desviaci√≥n est√°ndar del tiempo entre visitas\n",
        "        return visit_intervals.std() if len(visit_intervals) > 1 else np.nan\n",
        "    \n",
        "    P = df_grouped.apply(calc_periodicity)\n",
        "\n",
        "    # Se crea un nuevo dataframe con las variables calculadas\n",
        "    lrmfp_df = pd.DataFrame({'Length': L, 'Recency': R, 'Monetary': M, 'Frequency': F, 'Periodicity': P}).reset_index()\n",
        "\n",
        "    return lrmfp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lrmfp_df = custom_features(df_knn_imputer)\n",
        "lrmfp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddL8wThv36t"
      },
      "source": [
        "#### 4.2 Agregando las custom features [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehLWiQzjwDm-"
      },
      "source": [
        "Ahora, usted decide agregar al pipeline las nuevas variables creadas, para lo cual realiza las siguientes tareas:\n",
        "\n",
        "1. Cree un nuevo pipeline llamado `retail_pipeline` que encapsule el ColumnTransformer y calcule las LRMFP. El primer paso del pipeline ll√°melo  `col_tranformer` y el segundo paso ll√°melo `custom_features`, incorpora las nuevas variables al dataframe. Hint: les puede ser √∫til investigar [este](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) m√©todo. [1 punto]\n",
        "2. Aplicar el pipeline actualizado a los datos proporcionados por Mr. Cheems, creando un nuevo dataframe llamado `df_custom`. [1 punto]\n",
        "3. Explorar la distribuci√≥n de las nuevas variables con `explore_data` y comentar brevemente (2-3 l√≠neas) caracter√≠sticas de cada custom feature. [5 puntos]\n",
        "5. Entregar un insight para el negocio en base a las nuevas variables. [3 puntos]\n",
        "\n",
        ">Nota: Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVCGxPgtwFsk"
      },
      "source": [
        "**Respuesta**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxILi3w0wE9Q"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "retail_pipeline = Pipeline(steps=[\n",
        "    ('col_transformer', column_transformer),\n",
        "    ('custom_features', FunctionTransformer(custom_features, validate=False))\n",
        "])\n",
        "\n",
        "# 2\n",
        "df_custom = retail_pipeline.fit_transform(df_retail)\n",
        "\n",
        "# 3\n",
        "explore_data(df_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Length**: Se destaca que para la mayor√≠a de personas ha pasado entre 0 y 9 d√≠as entre la primera y la √∫ltima visita, lo que indica que la mayor√≠a son clientes nuevos. Por otro lado, existe un peak de frecuencia entre 200 y 250 d√≠as, y entre 300 y 350 d√≠as, lo que podr√≠a indicar un sector fiel de clientes (esto no es necesariamente as√≠, ya que podr√≠a darse el caso de que un cliente compra entre periodos largos de tiempo, pero no entre medio. Para un an√°lisis en detalle habr√≠a que disolver la agrupaci√≥n y revisar esos clientes espec√≠ficos).\n",
        "\n",
        "- **Recency**: Que el gr√°fico este enfocado principalmente en un n√∫mero de d√≠as bajo implica que la mayor√≠a de clientes est√° activo recientemente. N√≥tese que esto se condice con el hecho de que la mayor√≠a de los clientes son nuevos. Por otro lado, a√∫n que en menor cantidad, existen clientes que hace mucho tiempo hicieron su √∫ltima compra. Esto quiere decir que, o bien existen clientes que decidieron dejar de comprar en alguna de las tiendas, o bien realizan compras distanciadas en el tiempo.\n",
        "\n",
        "- **Monetary**: La distribuci√≥n tiende a ser una mezcla de normales, exhibiendo que existe un grupo de compras de menor valor (entre 5 y 6 unidades), y otro de compras de mayor valor (entre 14 y 15 unidades). Se destaca que el grupo de compras de mayor valor es m√°s frecuente que el grupo de compras de menor valor. Por √∫ltimo, se aprecia que existen clientes outliers que en promedio realizan compras muy superiores en gasto con respecto a las normales mencionadas.\n",
        "\n",
        "- **Frequency**: Este gr√°fico se√±ala que la mayor√≠a de clientes no ha visitado m√°s de 5 veces, es decir, no visita mucho las tiendas. Adem√°s, esto se condice con lo que se ha mencionado previamente con respecto a que la mayor√≠a son clientes nuevos. Por otro lado, se destaca la existencia, aunque baja, de clientes con un gran n√∫mero de visitas, yendo desde 50 a incluso m√°s de 200 apariciones.\n",
        "\n",
        "- **Periodicity**: El gr√°fico muestra que, en general, la mayor√≠a de los clientes realiza visitas de manera regular (baja variabilidad en los intervalos entre visitas). Por otro lado, existe un grupo importante de clientes que tiene una variabilidad mucho mayor (valor de periodicity entorno a 9 y 30). Esto implica la existencia de dos grupos marcados de clientes: los que compran a menudo, y los que compran entre periodos largos.\n",
        "\n",
        "**Insight**:  Se puede se√±alar que existen 4 tipos de clientes: aquellos que compran con cierta regularidad, ya sea gastando m√°s o menos, y los que compran en intervalos m√°s largos de tiempo, de nuevo ya sea gastando m√°s o menos. De esta forma, llevar a cabo pol√≠ticas comerciales que toman en cuenta distintas propuestas de valor permitir√≠a maximizar beneficios para la empresa, ya sea mediante promociones u ofertas que hagan que los clientes tiendan a gastar m√°s, o que aumenten su regularidad, entre muchas otras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOV0y-e_lS39"
      },
      "source": [
        "### 5. MinMax Scaler [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T55ZgReXvjGe"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://i.imgflip.com/1fsprn.jpg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk2R1kvuu-e"
      },
      "source": [
        "#### 5.1 Definici√≥n del Column Transformer [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "94c48775ecb4496d970fbd920f65c126",
        "deepnote_cell_height": 268.70001220703125,
        "deepnote_cell_type": "markdown",
        "id": "iWsfp1dKXMfo",
        "tags": []
      },
      "source": [
        "Construya una clase llamada `MinMax()` para realizar una transformaci√≥n de cada una de las columnas de un DataFrame utilizando `ColumnTransformer()`. Recuerde  usar `BaseEstimator` y `TransformerMixin`.\n",
        "\n",
        "\n",
        " Para esto considere que Min-Max escaler queda dada por la ecuaci√≥n:\n",
        "\n",
        "$$MinMax = \\dfrac{x-min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "\n",
        "Consulte el siguiente [link](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer) si tiene dudas sobre la creaci√≥n de custom transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c087d1fa8aa94d7485fe1292bf628660",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "MUOLTWPDXMfo",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "07cb4dcf097c4c6baabb9ae2bda25caf",
        "deepnote_cell_height": 83.86666870117188,
        "deepnote_cell_type": "code",
        "id": "g15ZMCs-XMfo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MinMax(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Valores m√≠nimos y m√°ximos por columna\n",
        "        self.min_values = X.min(axis=0)\n",
        "        self.max_values = X.max(axis=0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transformaci√≥n Min-Max        \n",
        "        return (X - self.min_values) / (self.max_values - self.min_values)\n",
        "\n",
        "    def set_output(self,transform='default'):\n",
        "        # No se modifica este m√©todo\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySqWq1Muzp8"
      },
      "source": [
        "#### 5.2 Incorporando MinMax al pipeline [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIqjkgDwRsV"
      },
      "source": [
        "Ahora, usted decide agregar el escalamiento al pipeline, para lo que decide seguir los siguientes pasos:\n",
        "\n",
        "- Agregar el paso `minmax` al pipeline `numeric_transformations`, haciendo uso de la clase creada. [1 punto]\n",
        "- Defina el dataframe `df_minmax` aplicando el ColumnTransformer actualizado a los datos proporcionados por Mr. Cheems. [1 punto]\n",
        "- Usar `explore_data` en `df_retail` y en `df_minmax`. [1 punto]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables.  [2 puntos]\n",
        "\n",
        ">Nota: Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a480355952a34b6cb7e72afa764091d6",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "lL2_CyAGXMfp",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "1889976b7a4c40c7825752979b577567",
        "deepnote_cell_height": 65.86666870117188,
        "deepnote_cell_type": "code",
        "id": "NmApXgB8XMfp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "numeric_transformations = Pipeline(steps=[\n",
        "    ('iqr', IQR(lambd=1.5)),\n",
        "    ('knn_imputer', KNNImputer(n_neighbors=3)),  # Se establece el n√∫mero de vecinos m√°s cercanos en 3 para evitar tiempos de ejecuci√≥n demasiado largos\n",
        "    ('minmax', MinMax())\n",
        "])  \n",
        "\n",
        "column_transformer = ColumnTransformer(transformers=[\n",
        "    ('numerical', numeric_transformations, numerical_columns),\n",
        "    ('categorical', categoric_transformations, categorical_columns)\n",
        "], verbose_feature_names_out=False)\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "df_minmax = column_transformer.fit_transform(df_retail)\n",
        "\n",
        "explore_data(df_minmax)\n",
        "explore_data(df_retail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los cambios que se exhiben en la distribuci√≥n corresponden a los mismos vistos en los puntos anteriores con respecto a la eliminaci√≥n de outliers y la imputaci√≥n de nulos mediante KNN, pero ahora agregando el hecho de que los valores de las variables se mueven entre 0 y 1 (escalamiento no altera proporciones). Esto es de gran utilidad cuando se quieren realizar comparaciones entre variables que se mueven en distintas escalas, para no darle un mayor peso a alguna sobre la otra, por ejemplo en algoritmos que requieren caracter√≠sticas normalizadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXlAO8-wfNt"
      },
      "source": [
        "### 6. Pregunta te√≥rica [5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvsFRwpVtMh_"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://file.coinexstatic.com/2023-09-19/166BAC031F222E5910954E7D7D0BC844.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7lQIAHwiZv"
      },
      "source": [
        "Finalmente, expl√≠quele a Mr. Cheems porqu√© es √∫til la creaci√≥n de pipelines al momento de hacer Feature Engineering en Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29QJyzOCwjdD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMDYYL1stUVO"
      },
      "source": [
        "Los pipelines ayudan en un gran n√∫mero de aspectos al hacer Feature Engineering. Por un lado, el simplificar y combinar los distintos procesos que se deben aplicar en machine learning en una √∫nica entidad, facilitando el flujo de trabajo, reduciendo fuentes de errores y asegurando consistencia entre los datos. Por otro, permite que ajustar hiperpar√°metros sea una tarea sencilla cuando se quieran realizar pruebas con distintas combinaciones de hiperpar√°metros para mejorar el rendimiento de cierto modelo. Por √∫ltimo, la simplificaci√≥n y compactaci√≥n en la manipulaci√≥n de datos permite no solo que un c√≥digo sea mucho m√°s legible y reutilizable con respecto a un proyecto que no utilice pipelines, sino que tambi√©n asegura una mejora de rendimiento para modelos complejos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "94721075d5ff44bd83601c871797ae2a",
        "deepnote_cell_height": 514.4666748046875,
        "deepnote_cell_type": "markdown",
        "id": "Rg4ZMq8ezAH6"
      },
      "source": [
        "# Conclusi√≥n\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por el foro de U-cursos o por correo.\n",
        "\n",
        "![Gracias Totales!](https://i.pinimg.com/originals/65/ae/27/65ae270df87c3c4adcea997e48f60852.gif \"bruno\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e31a91f8cb744cabd0ed0287ac5257e",
        "deepnote_cell_height": 171.28334045410156,
        "deepnote_cell_type": "markdown",
        "id": "wCL1lACBzAH7"
      },
      "source": [
        "<br>\n",
        "<center>\n",
        "<img src=\"https://i.kym-cdn.com/photos/images/original/001/194/195/b18.png\" width=100 height=50 />\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "ALHqwrAFXMgD"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q6nm_0uWvrFv",
        "buuUiW-9YYZ3",
        "1ddL8wThv36t",
        "qOV0y-e_lS39",
        "iWXlAO8-wfNt"
      ],
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "33c253a4f84d40a091bd5023e95abb64",
    "kernelspec": {
      "display_name": "Lab_MDS_Primavera",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
